{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IY0MoRkdARrO"
   },
   "source": [
    "## Work\n",
    "1. 請比較使用不同層數以及不同 Dropout rate 對訓練的效果\n",
    "2. 將 optimizer 改成使用 Adam 並加上適當的 dropout rate 檢視結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4728,
     "status": "ok",
     "timestamp": 1575381389284,
     "user": {
      "displayName": "林建宏",
      "photoUrl": "",
      "userId": "12416564836120847438"
     },
     "user_tz": -480
    },
    "id": "wB7sIt0xARrQ",
    "outputId": "619ee181-bb5f-49f9-e585-a76c3e0ce6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  3 13:56:25 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Enp2QOSlARrT"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2695,
     "status": "ok",
     "timestamp": 1575381402187,
     "user": {
      "displayName": "林建宏",
      "photoUrl": "",
      "userId": "12416564836120847438"
     },
     "user_tz": -480
    },
    "id": "hH7OpuoWARrV",
    "outputId": "e7077900-70b0-4209-96c1-7c1289340996"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import itertools\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGHGqLh4ARrX"
   },
   "outputs": [],
   "source": [
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BiDGAnFARrZ"
   },
   "outputs": [],
   "source": [
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv_DL30_ARrb"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# Preproc the inputs\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# Preprc the outputs\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYsUOi_CARrc"
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 256, 128, 64], l1_ratio=1e-4, l2_ratio=1e-4, drp_ratio=0.2):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "#             x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1), kernel_regularizer=l2(l2_ratio))(input_layer)\n",
    "            x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1), \n",
    "            kernel_regularizer=l1_l2(l1=l1_ratio, l2=l2_ratio))(input_layer)\n",
    "    \n",
    "            x = Dropout(drp_ratio)(x)\n",
    "        else:\n",
    "#             x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1), kernel_regularizer=l2(l2_ratio))(x)\n",
    "            x = keras.layers.Dense(units=n_units, activation=\"relu\", name=\"hidden_layer\"+str(i+1), \n",
    "            kernel_regularizer=l1_l2(l1=l1_ratio, l2=l2_ratio))(x)\n",
    "    \n",
    "            x = Dropout(drp_ratio)(x)\n",
    "    \n",
    "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vij5tLwrARrf"
   },
   "outputs": [],
   "source": [
    "\"\"\"Code Here\n",
    "設定超參數\n",
    "\"\"\"\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "MOMENTUM = 0.95\n",
    "Dropout_EXP = [0.15, 0.20, 0.25, 0.30, 0.40]\n",
    "LAYER_NEURONS = [[128, 128, 128], [128, 256, 256], [128, 256, 512], [128, 256, 256], [128, 256, 256]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2798533,
     "status": "ok",
     "timestamp": 1575384229800,
     "user": {
      "displayName": "林建宏",
      "photoUrl": "",
      "userId": "12416564836120847438"
     },
     "user_tz": -480
    },
    "id": "YeDpy-_9ARrg",
    "outputId": "e74fbc62-82a7-4720-e3a4-9540f110d802",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Numbers of exp: 0, layer: [128, 128, 128], dropout_rate: 0.15\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 427,658\n",
      "Trainable params: 427,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 3.1959 - acc: 0.2169 - val_loss: 2.9262 - val_acc: 0.3229\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.8680 - acc: 0.2979 - val_loss: 2.6980 - val_acc: 0.3562\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.6929 - acc: 0.3311 - val_loss: 2.5528 - val_acc: 0.3767\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5707 - acc: 0.3519 - val_loss: 2.4457 - val_acc: 0.3935\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.4782 - acc: 0.3664 - val_loss: 2.3694 - val_acc: 0.4065\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.4102 - acc: 0.3783 - val_loss: 2.3112 - val_acc: 0.4147\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3553 - acc: 0.3889 - val_loss: 2.2503 - val_acc: 0.4277\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3083 - acc: 0.4002 - val_loss: 2.2166 - val_acc: 0.4342\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2700 - acc: 0.4049 - val_loss: 2.1862 - val_acc: 0.4365\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2370 - acc: 0.4119 - val_loss: 2.1535 - val_acc: 0.4415\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2040 - acc: 0.4196 - val_loss: 2.1183 - val_acc: 0.4545\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1781 - acc: 0.4227 - val_loss: 2.1018 - val_acc: 0.4562\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1495 - acc: 0.4306 - val_loss: 2.0741 - val_acc: 0.4544\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1302 - acc: 0.4324 - val_loss: 2.0537 - val_acc: 0.4575\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1061 - acc: 0.4409 - val_loss: 2.0356 - val_acc: 0.4631\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0823 - acc: 0.4420 - val_loss: 2.0199 - val_acc: 0.4649\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.0654 - acc: 0.4465 - val_loss: 2.0039 - val_acc: 0.4689\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0446 - acc: 0.4497 - val_loss: 2.0000 - val_acc: 0.4615\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0307 - acc: 0.4517 - val_loss: 1.9849 - val_acc: 0.4717\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0172 - acc: 0.4552 - val_loss: 1.9656 - val_acc: 0.4671\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9961 - acc: 0.4551 - val_loss: 1.9410 - val_acc: 0.4785\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 1.9828 - acc: 0.4585 - val_loss: 1.9163 - val_acc: 0.4824\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9659 - acc: 0.4624 - val_loss: 1.9064 - val_acc: 0.4818\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.9559 - acc: 0.4643 - val_loss: 1.9106 - val_acc: 0.4747\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.9435 - acc: 0.4656 - val_loss: 1.8809 - val_acc: 0.4884\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9278 - acc: 0.4709 - val_loss: 1.8834 - val_acc: 0.4821\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9142 - acc: 0.4708 - val_loss: 1.8682 - val_acc: 0.4877\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9030 - acc: 0.4712 - val_loss: 1.8453 - val_acc: 0.4928\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9011 - acc: 0.4728 - val_loss: 1.8646 - val_acc: 0.4845\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8822 - acc: 0.4769 - val_loss: 1.8293 - val_acc: 0.4956\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8754 - acc: 0.4797 - val_loss: 1.8235 - val_acc: 0.4938\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8612 - acc: 0.4813 - val_loss: 1.8310 - val_acc: 0.4855\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8528 - acc: 0.4819 - val_loss: 1.8163 - val_acc: 0.4928\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8438 - acc: 0.4833 - val_loss: 1.8086 - val_acc: 0.4962\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 1.8335 - acc: 0.4853 - val_loss: 1.7900 - val_acc: 0.4956\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8242 - acc: 0.4862 - val_loss: 1.7869 - val_acc: 0.5025\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8154 - acc: 0.4908 - val_loss: 1.7724 - val_acc: 0.5057\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8097 - acc: 0.4897 - val_loss: 1.7731 - val_acc: 0.5021\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7958 - acc: 0.4924 - val_loss: 1.7558 - val_acc: 0.5017\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 1.7928 - acc: 0.4934 - val_loss: 1.7562 - val_acc: 0.5071\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7834 - acc: 0.4956 - val_loss: 1.7530 - val_acc: 0.5081\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7805 - acc: 0.4954 - val_loss: 1.7628 - val_acc: 0.5045\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7736 - acc: 0.4960 - val_loss: 1.7424 - val_acc: 0.5065\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7671 - acc: 0.4961 - val_loss: 1.7423 - val_acc: 0.5042\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7567 - acc: 0.5009 - val_loss: 1.7374 - val_acc: 0.5106\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 1.7465 - acc: 0.5018 - val_loss: 1.7275 - val_acc: 0.5043\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7386 - acc: 0.5039 - val_loss: 1.7479 - val_acc: 0.4980\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7361 - acc: 0.5016 - val_loss: 1.7317 - val_acc: 0.5004\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7371 - acc: 0.5006 - val_loss: 1.7155 - val_acc: 0.5064\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7305 - acc: 0.5015 - val_loss: 1.7030 - val_acc: 0.5111\n",
      "Numbers of exp: 1, layer: [128, 128, 128], dropout_rate: 0.20\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 427,658\n",
      "Trainable params: 427,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 3.2194 - acc: 0.1976 - val_loss: 2.9648 - val_acc: 0.2935\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.9035 - acc: 0.2730 - val_loss: 2.7325 - val_acc: 0.3376\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.7263 - acc: 0.3095 - val_loss: 2.5832 - val_acc: 0.3643\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5952 - acc: 0.3391 - val_loss: 2.4771 - val_acc: 0.3867\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5066 - acc: 0.3511 - val_loss: 2.3776 - val_acc: 0.3940\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4301 - acc: 0.3675 - val_loss: 2.3102 - val_acc: 0.4094\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.3716 - acc: 0.3775 - val_loss: 2.2723 - val_acc: 0.4193\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3231 - acc: 0.3857 - val_loss: 2.2133 - val_acc: 0.4342\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2820 - acc: 0.3942 - val_loss: 2.1859 - val_acc: 0.4354\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2483 - acc: 0.3999 - val_loss: 2.1385 - val_acc: 0.4395\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2183 - acc: 0.4059 - val_loss: 2.1115 - val_acc: 0.4467\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1930 - acc: 0.4090 - val_loss: 2.0899 - val_acc: 0.4476\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.1626 - acc: 0.4179 - val_loss: 2.0720 - val_acc: 0.4504\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.1387 - acc: 0.4200 - val_loss: 2.0304 - val_acc: 0.4629\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1126 - acc: 0.4273 - val_loss: 2.0232 - val_acc: 0.4590\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.1020 - acc: 0.4269 - val_loss: 2.0047 - val_acc: 0.4644\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0809 - acc: 0.4317 - val_loss: 2.0024 - val_acc: 0.4556\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0628 - acc: 0.4348 - val_loss: 1.9873 - val_acc: 0.4618\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0470 - acc: 0.4393 - val_loss: 1.9683 - val_acc: 0.4687\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0280 - acc: 0.4430 - val_loss: 1.9441 - val_acc: 0.4737\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0155 - acc: 0.4466 - val_loss: 1.9545 - val_acc: 0.4675\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0040 - acc: 0.4465 - val_loss: 1.9343 - val_acc: 0.4693\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.9876 - acc: 0.4474 - val_loss: 1.9067 - val_acc: 0.4837\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9775 - acc: 0.4488 - val_loss: 1.9083 - val_acc: 0.4784\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9617 - acc: 0.4557 - val_loss: 1.9021 - val_acc: 0.4749\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9539 - acc: 0.4536 - val_loss: 1.8840 - val_acc: 0.4814\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9397 - acc: 0.4562 - val_loss: 1.8575 - val_acc: 0.4937\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9254 - acc: 0.4606 - val_loss: 1.8631 - val_acc: 0.4879\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9149 - acc: 0.4617 - val_loss: 1.8529 - val_acc: 0.4895\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9083 - acc: 0.4651 - val_loss: 1.8392 - val_acc: 0.4877\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8979 - acc: 0.4613 - val_loss: 1.8404 - val_acc: 0.4850\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8883 - acc: 0.4652 - val_loss: 1.8309 - val_acc: 0.4886\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8754 - acc: 0.4677 - val_loss: 1.8180 - val_acc: 0.4903\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8714 - acc: 0.4690 - val_loss: 1.8075 - val_acc: 0.4926\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8594 - acc: 0.4700 - val_loss: 1.8205 - val_acc: 0.4806\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8504 - acc: 0.4719 - val_loss: 1.7941 - val_acc: 0.4932\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8420 - acc: 0.4743 - val_loss: 1.7911 - val_acc: 0.4940\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8376 - acc: 0.4733 - val_loss: 1.7730 - val_acc: 0.4990\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8247 - acc: 0.4751 - val_loss: 1.7886 - val_acc: 0.4897\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8185 - acc: 0.4796 - val_loss: 1.7759 - val_acc: 0.4923\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8151 - acc: 0.4769 - val_loss: 1.7749 - val_acc: 0.4908\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8060 - acc: 0.4810 - val_loss: 1.7579 - val_acc: 0.4984\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8009 - acc: 0.4791 - val_loss: 1.7533 - val_acc: 0.5020\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7903 - acc: 0.4870 - val_loss: 1.7390 - val_acc: 0.4988\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7887 - acc: 0.4822 - val_loss: 1.7422 - val_acc: 0.5004\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7747 - acc: 0.4869 - val_loss: 1.7409 - val_acc: 0.5003\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7725 - acc: 0.4861 - val_loss: 1.7266 - val_acc: 0.5006\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.7658 - acc: 0.4868 - val_loss: 1.7304 - val_acc: 0.4992\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7627 - acc: 0.4874 - val_loss: 1.7110 - val_acc: 0.5056\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7551 - acc: 0.4876 - val_loss: 1.7163 - val_acc: 0.5029\n",
      "Numbers of exp: 2, layer: [128, 128, 128], dropout_rate: 0.25\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 427,658\n",
      "Trainable params: 427,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 3.2507 - acc: 0.1829 - val_loss: 2.9731 - val_acc: 0.2830\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.9101 - acc: 0.2629 - val_loss: 2.7156 - val_acc: 0.3236\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.7137 - acc: 0.2958 - val_loss: 2.5530 - val_acc: 0.3526\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5832 - acc: 0.3147 - val_loss: 2.4396 - val_acc: 0.3671\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.4948 - acc: 0.3295 - val_loss: 2.3556 - val_acc: 0.3795\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4138 - acc: 0.3419 - val_loss: 2.2917 - val_acc: 0.3885\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3601 - acc: 0.3534 - val_loss: 2.2374 - val_acc: 0.3965\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3094 - acc: 0.3604 - val_loss: 2.1859 - val_acc: 0.4073\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2671 - acc: 0.3716 - val_loss: 2.1509 - val_acc: 0.4179\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2335 - acc: 0.3736 - val_loss: 2.1244 - val_acc: 0.4252\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2079 - acc: 0.3791 - val_loss: 2.0901 - val_acc: 0.4264\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1800 - acc: 0.3893 - val_loss: 2.0669 - val_acc: 0.4297\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1535 - acc: 0.3923 - val_loss: 2.0434 - val_acc: 0.4355\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1334 - acc: 0.3948 - val_loss: 2.0212 - val_acc: 0.4434\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 2.1100 - acc: 0.4037 - val_loss: 2.0005 - val_acc: 0.4433\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0870 - acc: 0.4081 - val_loss: 1.9818 - val_acc: 0.4519\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0708 - acc: 0.4106 - val_loss: 1.9662 - val_acc: 0.4532\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0538 - acc: 0.4112 - val_loss: 1.9562 - val_acc: 0.4514\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0442 - acc: 0.4162 - val_loss: 1.9384 - val_acc: 0.4522\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0275 - acc: 0.4178 - val_loss: 1.9298 - val_acc: 0.4584\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0133 - acc: 0.4240 - val_loss: 1.9127 - val_acc: 0.4612\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9965 - acc: 0.4243 - val_loss: 1.9040 - val_acc: 0.4591\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9853 - acc: 0.4284 - val_loss: 1.8866 - val_acc: 0.4676\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9707 - acc: 0.4323 - val_loss: 1.8854 - val_acc: 0.4589\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9599 - acc: 0.4299 - val_loss: 1.8615 - val_acc: 0.4703\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9468 - acc: 0.4357 - val_loss: 1.8528 - val_acc: 0.4705\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.9382 - acc: 0.4376 - val_loss: 1.8525 - val_acc: 0.4703\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9269 - acc: 0.4386 - val_loss: 1.8507 - val_acc: 0.4693\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9215 - acc: 0.4423 - val_loss: 1.8316 - val_acc: 0.4671\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9073 - acc: 0.4420 - val_loss: 1.8123 - val_acc: 0.4803\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9009 - acc: 0.4441 - val_loss: 1.8143 - val_acc: 0.4815\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8914 - acc: 0.4471 - val_loss: 1.8111 - val_acc: 0.4787\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 1.8833 - acc: 0.4436 - val_loss: 1.8032 - val_acc: 0.4712\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8745 - acc: 0.4484 - val_loss: 1.7980 - val_acc: 0.4749\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8660 - acc: 0.4467 - val_loss: 1.7859 - val_acc: 0.4855\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8592 - acc: 0.4509 - val_loss: 1.7765 - val_acc: 0.4858\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8549 - acc: 0.4538 - val_loss: 1.7696 - val_acc: 0.4820\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8469 - acc: 0.4533 - val_loss: 1.7637 - val_acc: 0.4871\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8420 - acc: 0.4539 - val_loss: 1.7528 - val_acc: 0.4880\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8290 - acc: 0.4557 - val_loss: 1.7602 - val_acc: 0.4861\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8267 - acc: 0.4571 - val_loss: 1.7465 - val_acc: 0.4874\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8145 - acc: 0.4588 - val_loss: 1.7507 - val_acc: 0.4852\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8093 - acc: 0.4610 - val_loss: 1.7413 - val_acc: 0.4886\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8071 - acc: 0.4612 - val_loss: 1.7360 - val_acc: 0.4849\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8014 - acc: 0.4641 - val_loss: 1.7452 - val_acc: 0.4779\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7962 - acc: 0.4622 - val_loss: 1.7158 - val_acc: 0.4915\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7925 - acc: 0.4600 - val_loss: 1.7146 - val_acc: 0.4888\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7863 - acc: 0.4640 - val_loss: 1.7020 - val_acc: 0.4981\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7805 - acc: 0.4649 - val_loss: 1.7156 - val_acc: 0.4905\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7780 - acc: 0.4645 - val_loss: 1.7005 - val_acc: 0.4929\n",
      "Numbers of exp: 3, layer: [128, 128, 128], dropout_rate: 0.30\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 427,658\n",
      "Trainable params: 427,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 3.2995 - acc: 0.1628 - val_loss: 3.0577 - val_acc: 0.2745\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9997 - acc: 0.2408 - val_loss: 2.8163 - val_acc: 0.3274\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.8100 - acc: 0.2788 - val_loss: 2.6487 - val_acc: 0.3488\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6732 - acc: 0.3042 - val_loss: 2.5210 - val_acc: 0.3611\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5680 - acc: 0.3166 - val_loss: 2.4259 - val_acc: 0.3698\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4761 - acc: 0.3334 - val_loss: 2.3443 - val_acc: 0.3863\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4116 - acc: 0.3414 - val_loss: 2.2750 - val_acc: 0.3975\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3528 - acc: 0.3483 - val_loss: 2.2213 - val_acc: 0.4005\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3038 - acc: 0.3541 - val_loss: 2.1870 - val_acc: 0.4034\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2656 - acc: 0.3623 - val_loss: 2.1414 - val_acc: 0.4127\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2328 - acc: 0.3702 - val_loss: 2.1102 - val_acc: 0.4198\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2025 - acc: 0.3757 - val_loss: 2.0884 - val_acc: 0.4205\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1791 - acc: 0.3790 - val_loss: 2.0631 - val_acc: 0.4306\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1560 - acc: 0.3853 - val_loss: 2.0525 - val_acc: 0.4330\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1347 - acc: 0.3896 - val_loss: 2.0286 - val_acc: 0.4245\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.1135 - acc: 0.3953 - val_loss: 1.9879 - val_acc: 0.4398\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.1005 - acc: 0.3976 - val_loss: 1.9881 - val_acc: 0.4377\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0845 - acc: 0.3991 - val_loss: 1.9571 - val_acc: 0.4447\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0681 - acc: 0.4028 - val_loss: 1.9528 - val_acc: 0.4475\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0561 - acc: 0.4032 - val_loss: 1.9381 - val_acc: 0.4523\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0367 - acc: 0.4088 - val_loss: 1.9332 - val_acc: 0.4460\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0233 - acc: 0.4112 - val_loss: 1.9068 - val_acc: 0.4569\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0106 - acc: 0.4142 - val_loss: 1.8921 - val_acc: 0.4578\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9937 - acc: 0.4170 - val_loss: 1.8844 - val_acc: 0.4593\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9867 - acc: 0.4189 - val_loss: 1.8772 - val_acc: 0.4582\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9788 - acc: 0.4203 - val_loss: 1.8875 - val_acc: 0.4494\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9723 - acc: 0.4190 - val_loss: 1.8594 - val_acc: 0.4577\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9599 - acc: 0.4251 - val_loss: 1.8799 - val_acc: 0.4553\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9574 - acc: 0.4226 - val_loss: 1.8487 - val_acc: 0.4660\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9378 - acc: 0.4275 - val_loss: 1.8376 - val_acc: 0.4681\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9287 - acc: 0.4288 - val_loss: 1.8252 - val_acc: 0.4669\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9201 - acc: 0.4284 - val_loss: 1.8252 - val_acc: 0.4679\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9124 - acc: 0.4328 - val_loss: 1.8161 - val_acc: 0.4662\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9048 - acc: 0.4322 - val_loss: 1.8160 - val_acc: 0.4597\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8989 - acc: 0.4316 - val_loss: 1.8005 - val_acc: 0.4712\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8898 - acc: 0.4359 - val_loss: 1.7888 - val_acc: 0.4756\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8825 - acc: 0.4374 - val_loss: 1.7895 - val_acc: 0.4726\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8738 - acc: 0.4403 - val_loss: 1.7747 - val_acc: 0.4741\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8723 - acc: 0.4377 - val_loss: 1.7642 - val_acc: 0.4745\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8637 - acc: 0.4406 - val_loss: 1.7750 - val_acc: 0.4706\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8593 - acc: 0.4411 - val_loss: 1.7653 - val_acc: 0.4751\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8554 - acc: 0.4406 - val_loss: 1.7609 - val_acc: 0.4736\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8471 - acc: 0.4435 - val_loss: 1.7472 - val_acc: 0.4815\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8407 - acc: 0.4432 - val_loss: 1.7510 - val_acc: 0.4770\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8341 - acc: 0.4488 - val_loss: 1.7428 - val_acc: 0.4796\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8267 - acc: 0.4472 - val_loss: 1.7497 - val_acc: 0.4791\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8227 - acc: 0.4485 - val_loss: 1.7314 - val_acc: 0.4844\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8214 - acc: 0.4472 - val_loss: 1.7249 - val_acc: 0.4848\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8100 - acc: 0.4518 - val_loss: 1.7264 - val_acc: 0.4833\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8108 - acc: 0.4490 - val_loss: 1.7273 - val_acc: 0.4781\n",
      "Numbers of exp: 4, layer: [128, 128, 128], dropout_rate: 0.40\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 427,658\n",
      "Trainable params: 427,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 3.3510 - acc: 0.1476 - val_loss: 3.1517 - val_acc: 0.2640\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.0812 - acc: 0.2122 - val_loss: 2.8803 - val_acc: 0.2999\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.8981 - acc: 0.2450 - val_loss: 2.7303 - val_acc: 0.3135\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7561 - acc: 0.2658 - val_loss: 2.5979 - val_acc: 0.3291\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.6484 - acc: 0.2767 - val_loss: 2.4916 - val_acc: 0.3437\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5546 - acc: 0.2876 - val_loss: 2.4083 - val_acc: 0.3527\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4716 - acc: 0.3024 - val_loss: 2.3400 - val_acc: 0.3598\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4123 - acc: 0.3080 - val_loss: 2.2599 - val_acc: 0.3742\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.3531 - acc: 0.3184 - val_loss: 2.2126 - val_acc: 0.3820\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3092 - acc: 0.3219 - val_loss: 2.1806 - val_acc: 0.3852\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2758 - acc: 0.3305 - val_loss: 2.1319 - val_acc: 0.3977\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2404 - acc: 0.3326 - val_loss: 2.1011 - val_acc: 0.4046\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2168 - acc: 0.3402 - val_loss: 2.0854 - val_acc: 0.4050\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1906 - acc: 0.3471 - val_loss: 2.0424 - val_acc: 0.4103\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1726 - acc: 0.3472 - val_loss: 2.0382 - val_acc: 0.4083\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1490 - acc: 0.3518 - val_loss: 2.0191 - val_acc: 0.4133\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1361 - acc: 0.3545 - val_loss: 2.0035 - val_acc: 0.4108\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1219 - acc: 0.3599 - val_loss: 1.9942 - val_acc: 0.4092\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1080 - acc: 0.3613 - val_loss: 1.9840 - val_acc: 0.4220\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0906 - acc: 0.3659 - val_loss: 1.9633 - val_acc: 0.4319\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0812 - acc: 0.3671 - val_loss: 1.9554 - val_acc: 0.4222\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0737 - acc: 0.3701 - val_loss: 1.9508 - val_acc: 0.4253\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0629 - acc: 0.3712 - val_loss: 1.9236 - val_acc: 0.4316\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0542 - acc: 0.3747 - val_loss: 1.9061 - val_acc: 0.4398\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0404 - acc: 0.3749 - val_loss: 1.9122 - val_acc: 0.4285\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0315 - acc: 0.3767 - val_loss: 1.8941 - val_acc: 0.4401\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0230 - acc: 0.3804 - val_loss: 1.8953 - val_acc: 0.4411\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0131 - acc: 0.3849 - val_loss: 1.8839 - val_acc: 0.4404\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0045 - acc: 0.3856 - val_loss: 1.8718 - val_acc: 0.4426\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9976 - acc: 0.3843 - val_loss: 1.8652 - val_acc: 0.4456\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9902 - acc: 0.3881 - val_loss: 1.8655 - val_acc: 0.4460\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9806 - acc: 0.3910 - val_loss: 1.8511 - val_acc: 0.4453\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9716 - acc: 0.3916 - val_loss: 1.8384 - val_acc: 0.4462\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9663 - acc: 0.3916 - val_loss: 1.8440 - val_acc: 0.4501\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9640 - acc: 0.3948 - val_loss: 1.8281 - val_acc: 0.4499\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9572 - acc: 0.3936 - val_loss: 1.8207 - val_acc: 0.4516\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9452 - acc: 0.3957 - val_loss: 1.8219 - val_acc: 0.4498\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9423 - acc: 0.3977 - val_loss: 1.8159 - val_acc: 0.4493\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9302 - acc: 0.4002 - val_loss: 1.7998 - val_acc: 0.4581\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9296 - acc: 0.3988 - val_loss: 1.8027 - val_acc: 0.4517\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9226 - acc: 0.4042 - val_loss: 1.7889 - val_acc: 0.4597\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9197 - acc: 0.4005 - val_loss: 1.7909 - val_acc: 0.4574\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9137 - acc: 0.4027 - val_loss: 1.7894 - val_acc: 0.4506\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9126 - acc: 0.4042 - val_loss: 1.7736 - val_acc: 0.4616\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9013 - acc: 0.4065 - val_loss: 1.7804 - val_acc: 0.4575\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9018 - acc: 0.4086 - val_loss: 1.7855 - val_acc: 0.4528\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8985 - acc: 0.4046 - val_loss: 1.7669 - val_acc: 0.4642\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8902 - acc: 0.4067 - val_loss: 1.7680 - val_acc: 0.4573\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8838 - acc: 0.4088 - val_loss: 1.7638 - val_acc: 0.4626\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8795 - acc: 0.4106 - val_loss: 1.7468 - val_acc: 0.4604\n",
      "Numbers of exp: 5, layer: [128, 256, 256], dropout_rate: 0.15\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 56us/step - loss: 3.4303 - acc: 0.2450 - val_loss: 3.1566 - val_acc: 0.3300\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0853 - acc: 0.3255 - val_loss: 2.9295 - val_acc: 0.3702\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9054 - acc: 0.3577 - val_loss: 2.7743 - val_acc: 0.3963\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7832 - acc: 0.3770 - val_loss: 2.6694 - val_acc: 0.4156\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6909 - acc: 0.3904 - val_loss: 2.5849 - val_acc: 0.4233\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.6176 - acc: 0.4039 - val_loss: 2.5173 - val_acc: 0.4382\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5555 - acc: 0.4117 - val_loss: 2.4725 - val_acc: 0.4392\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5013 - acc: 0.4225 - val_loss: 2.4258 - val_acc: 0.4487\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4500 - acc: 0.4335 - val_loss: 2.3707 - val_acc: 0.4556\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4092 - acc: 0.4368 - val_loss: 2.3356 - val_acc: 0.4539\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3725 - acc: 0.4423 - val_loss: 2.3408 - val_acc: 0.4530\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.3394 - acc: 0.4421 - val_loss: 2.2691 - val_acc: 0.4664\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3055 - acc: 0.4505 - val_loss: 2.2695 - val_acc: 0.4624\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2741 - acc: 0.4543 - val_loss: 2.2028 - val_acc: 0.4812\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2455 - acc: 0.4570 - val_loss: 2.2266 - val_acc: 0.4605\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.2190 - acc: 0.4614 - val_loss: 2.1672 - val_acc: 0.4789\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.1913 - acc: 0.4656 - val_loss: 2.1475 - val_acc: 0.4754\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1719 - acc: 0.4668 - val_loss: 2.1150 - val_acc: 0.4838\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1471 - acc: 0.4722 - val_loss: 2.0954 - val_acc: 0.4906\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1230 - acc: 0.4732 - val_loss: 2.0863 - val_acc: 0.4866\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1094 - acc: 0.4763 - val_loss: 2.0548 - val_acc: 0.4917\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0828 - acc: 0.4793 - val_loss: 2.0429 - val_acc: 0.4931\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0645 - acc: 0.4807 - val_loss: 2.0219 - val_acc: 0.4952\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0462 - acc: 0.4827 - val_loss: 2.0001 - val_acc: 0.5011\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0254 - acc: 0.4867 - val_loss: 1.9954 - val_acc: 0.4980\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0093 - acc: 0.4887 - val_loss: 1.9701 - val_acc: 0.4985\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9942 - acc: 0.4905 - val_loss: 1.9528 - val_acc: 0.5060\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9799 - acc: 0.4923 - val_loss: 1.9455 - val_acc: 0.4993\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9591 - acc: 0.4953 - val_loss: 1.9385 - val_acc: 0.5073\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9496 - acc: 0.4982 - val_loss: 1.9248 - val_acc: 0.5068\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9363 - acc: 0.4977 - val_loss: 1.9307 - val_acc: 0.5038\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9196 - acc: 0.5011 - val_loss: 1.9187 - val_acc: 0.5062\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9073 - acc: 0.5018 - val_loss: 1.8995 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8927 - acc: 0.5029 - val_loss: 1.8729 - val_acc: 0.5165\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8804 - acc: 0.5053 - val_loss: 1.8571 - val_acc: 0.5172\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8738 - acc: 0.5048 - val_loss: 1.8557 - val_acc: 0.5164\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8600 - acc: 0.5066 - val_loss: 1.8446 - val_acc: 0.5155\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8505 - acc: 0.5086 - val_loss: 1.8299 - val_acc: 0.5106\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8397 - acc: 0.5070 - val_loss: 1.8434 - val_acc: 0.5034\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8311 - acc: 0.5097 - val_loss: 1.8084 - val_acc: 0.5245\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8135 - acc: 0.5143 - val_loss: 1.8196 - val_acc: 0.5157\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8092 - acc: 0.5139 - val_loss: 1.8047 - val_acc: 0.5166\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8003 - acc: 0.5149 - val_loss: 1.7901 - val_acc: 0.5190\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7887 - acc: 0.5183 - val_loss: 1.7856 - val_acc: 0.5196\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7838 - acc: 0.5172 - val_loss: 1.7821 - val_acc: 0.5164\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7764 - acc: 0.5158 - val_loss: 1.7828 - val_acc: 0.5148\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7644 - acc: 0.5191 - val_loss: 1.7698 - val_acc: 0.5163\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7610 - acc: 0.5181 - val_loss: 1.7543 - val_acc: 0.5245\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7509 - acc: 0.5213 - val_loss: 1.7571 - val_acc: 0.5215\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7432 - acc: 0.5258 - val_loss: 1.7474 - val_acc: 0.5209\n",
      "Numbers of exp: 6, layer: [128, 256, 256], dropout_rate: 0.20\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.4683 - acc: 0.2180 - val_loss: 3.1647 - val_acc: 0.3267\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.0897 - acc: 0.3091 - val_loss: 2.9014 - val_acc: 0.3615\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.8834 - acc: 0.3405 - val_loss: 2.7434 - val_acc: 0.3723\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7374 - acc: 0.3617 - val_loss: 2.6011 - val_acc: 0.4009\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.6322 - acc: 0.3772 - val_loss: 2.5236 - val_acc: 0.4178\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5486 - acc: 0.3865 - val_loss: 2.4357 - val_acc: 0.4262\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4840 - acc: 0.3977 - val_loss: 2.3915 - val_acc: 0.4289\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.4286 - acc: 0.4073 - val_loss: 2.3390 - val_acc: 0.4463\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3866 - acc: 0.4134 - val_loss: 2.2917 - val_acc: 0.4415\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3436 - acc: 0.4183 - val_loss: 2.2615 - val_acc: 0.4449\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3065 - acc: 0.4233 - val_loss: 2.2114 - val_acc: 0.4596\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2764 - acc: 0.4279 - val_loss: 2.1792 - val_acc: 0.4669\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2438 - acc: 0.4355 - val_loss: 2.1653 - val_acc: 0.4555\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2208 - acc: 0.4373 - val_loss: 2.1501 - val_acc: 0.4599\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1943 - acc: 0.4394 - val_loss: 2.1089 - val_acc: 0.4737\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1693 - acc: 0.4433 - val_loss: 2.0976 - val_acc: 0.4719\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1454 - acc: 0.4497 - val_loss: 2.0677 - val_acc: 0.4701\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1239 - acc: 0.4493 - val_loss: 2.0433 - val_acc: 0.4820\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1027 - acc: 0.4530 - val_loss: 2.0416 - val_acc: 0.4701\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0844 - acc: 0.4575 - val_loss: 2.0267 - val_acc: 0.4727\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0635 - acc: 0.4586 - val_loss: 1.9930 - val_acc: 0.4777\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0480 - acc: 0.4638 - val_loss: 2.0073 - val_acc: 0.4727\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0298 - acc: 0.4622 - val_loss: 1.9761 - val_acc: 0.4821\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0181 - acc: 0.4653 - val_loss: 1.9403 - val_acc: 0.4881\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9993 - acc: 0.4660 - val_loss: 1.9388 - val_acc: 0.4849\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9838 - acc: 0.4676 - val_loss: 1.9218 - val_acc: 0.4860\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9675 - acc: 0.4702 - val_loss: 1.9104 - val_acc: 0.4899\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9583 - acc: 0.4708 - val_loss: 1.9065 - val_acc: 0.4835\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9427 - acc: 0.4743 - val_loss: 1.8826 - val_acc: 0.4955\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9329 - acc: 0.4735 - val_loss: 1.8905 - val_acc: 0.4904\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9131 - acc: 0.4772 - val_loss: 1.8583 - val_acc: 0.4988\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9070 - acc: 0.4775 - val_loss: 1.8707 - val_acc: 0.4906\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8941 - acc: 0.4802 - val_loss: 1.8620 - val_acc: 0.4872\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8870 - acc: 0.4798 - val_loss: 1.8382 - val_acc: 0.4979\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8768 - acc: 0.4815 - val_loss: 1.8275 - val_acc: 0.4987\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8665 - acc: 0.4822 - val_loss: 1.8420 - val_acc: 0.4917\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8544 - acc: 0.4853 - val_loss: 1.8286 - val_acc: 0.4858\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8443 - acc: 0.4861 - val_loss: 1.7965 - val_acc: 0.5033\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8363 - acc: 0.4876 - val_loss: 1.8043 - val_acc: 0.4950\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8274 - acc: 0.4863 - val_loss: 1.7928 - val_acc: 0.4999\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8175 - acc: 0.4899 - val_loss: 1.7918 - val_acc: 0.4967\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8137 - acc: 0.4885 - val_loss: 1.7716 - val_acc: 0.5021\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8086 - acc: 0.4873 - val_loss: 1.7694 - val_acc: 0.5046\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7971 - acc: 0.4933 - val_loss: 1.7507 - val_acc: 0.5104\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7871 - acc: 0.4966 - val_loss: 1.7776 - val_acc: 0.4929\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7823 - acc: 0.4957 - val_loss: 1.7569 - val_acc: 0.4982\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7753 - acc: 0.4948 - val_loss: 1.7523 - val_acc: 0.4996\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.7681 - acc: 0.4954 - val_loss: 1.7369 - val_acc: 0.5111\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7621 - acc: 0.4982 - val_loss: 1.7252 - val_acc: 0.5108\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7532 - acc: 0.5001 - val_loss: 1.7169 - val_acc: 0.5146\n",
      "Numbers of exp: 7, layer: [128, 256, 256], dropout_rate: 0.25\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.5329 - acc: 0.1987 - val_loss: 3.2435 - val_acc: 0.3207\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.1693 - acc: 0.2894 - val_loss: 2.9851 - val_acc: 0.3473\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.9633 - acc: 0.3245 - val_loss: 2.8039 - val_acc: 0.3741\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.8188 - acc: 0.3439 - val_loss: 2.6841 - val_acc: 0.3893\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.7074 - acc: 0.3632 - val_loss: 2.5888 - val_acc: 0.3987\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6216 - acc: 0.3755 - val_loss: 2.5034 - val_acc: 0.4121\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5492 - acc: 0.3816 - val_loss: 2.4274 - val_acc: 0.4362\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4860 - acc: 0.3953 - val_loss: 2.3935 - val_acc: 0.4245\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4337 - acc: 0.4034 - val_loss: 2.3180 - val_acc: 0.4471\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3948 - acc: 0.4083 - val_loss: 2.2944 - val_acc: 0.4426\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3523 - acc: 0.4176 - val_loss: 2.2680 - val_acc: 0.4426\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3178 - acc: 0.4167 - val_loss: 2.2324 - val_acc: 0.4492\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.2876 - acc: 0.4247 - val_loss: 2.1951 - val_acc: 0.4554\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2545 - acc: 0.4286 - val_loss: 2.1569 - val_acc: 0.4640\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2274 - acc: 0.4321 - val_loss: 2.1301 - val_acc: 0.4675\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2031 - acc: 0.4348 - val_loss: 2.1101 - val_acc: 0.4660\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1766 - acc: 0.4367 - val_loss: 2.0833 - val_acc: 0.4706\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1542 - acc: 0.4399 - val_loss: 2.0632 - val_acc: 0.4695\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1348 - acc: 0.4450 - val_loss: 2.0436 - val_acc: 0.4772\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1127 - acc: 0.4450 - val_loss: 2.0231 - val_acc: 0.4749\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0899 - acc: 0.4489 - val_loss: 2.0200 - val_acc: 0.4725\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0720 - acc: 0.4533 - val_loss: 1.9927 - val_acc: 0.4747\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0568 - acc: 0.4557 - val_loss: 1.9645 - val_acc: 0.4848\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0363 - acc: 0.4571 - val_loss: 1.9620 - val_acc: 0.4803\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0179 - acc: 0.4575 - val_loss: 1.9402 - val_acc: 0.4896\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0067 - acc: 0.4584 - val_loss: 1.9434 - val_acc: 0.4773\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9925 - acc: 0.4624 - val_loss: 1.9129 - val_acc: 0.4891\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9788 - acc: 0.4599 - val_loss: 1.9092 - val_acc: 0.4823\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9631 - acc: 0.4651 - val_loss: 1.8922 - val_acc: 0.4880\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9556 - acc: 0.4642 - val_loss: 1.8799 - val_acc: 0.4863\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9320 - acc: 0.4686 - val_loss: 1.8770 - val_acc: 0.4854\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9273 - acc: 0.4676 - val_loss: 1.8713 - val_acc: 0.4873\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9136 - acc: 0.4694 - val_loss: 1.8608 - val_acc: 0.4924\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9044 - acc: 0.4716 - val_loss: 1.8446 - val_acc: 0.4958\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8959 - acc: 0.4719 - val_loss: 1.8369 - val_acc: 0.4905\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8837 - acc: 0.4726 - val_loss: 1.8217 - val_acc: 0.4926\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8778 - acc: 0.4724 - val_loss: 1.8112 - val_acc: 0.4972\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8642 - acc: 0.4759 - val_loss: 1.8020 - val_acc: 0.5008\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8524 - acc: 0.4783 - val_loss: 1.8029 - val_acc: 0.4894\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8466 - acc: 0.4790 - val_loss: 1.7938 - val_acc: 0.5017\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8410 - acc: 0.4773 - val_loss: 1.7966 - val_acc: 0.4905\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8303 - acc: 0.4808 - val_loss: 1.7960 - val_acc: 0.4931\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8231 - acc: 0.4800 - val_loss: 1.7695 - val_acc: 0.4987\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8199 - acc: 0.4785 - val_loss: 1.7793 - val_acc: 0.4966\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8101 - acc: 0.4806 - val_loss: 1.7574 - val_acc: 0.5013\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8070 - acc: 0.4804 - val_loss: 1.7637 - val_acc: 0.4985\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7943 - acc: 0.4835 - val_loss: 1.7434 - val_acc: 0.5067\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7876 - acc: 0.4840 - val_loss: 1.7489 - val_acc: 0.5019\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.7810 - acc: 0.4838 - val_loss: 1.7409 - val_acc: 0.4994\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7776 - acc: 0.4865 - val_loss: 1.7321 - val_acc: 0.5008\n",
      "Numbers of exp: 8, layer: [128, 256, 256], dropout_rate: 0.30\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.5230 - acc: 0.1944 - val_loss: 3.2045 - val_acc: 0.2904\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 3.1308 - acc: 0.2784 - val_loss: 2.9273 - val_acc: 0.3491\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.9218 - acc: 0.3122 - val_loss: 2.7604 - val_acc: 0.3623\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.7690 - acc: 0.3328 - val_loss: 2.6233 - val_acc: 0.3853\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.6581 - acc: 0.3454 - val_loss: 2.5182 - val_acc: 0.4015\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.5675 - acc: 0.3593 - val_loss: 2.4303 - val_acc: 0.4102\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4992 - acc: 0.3699 - val_loss: 2.3821 - val_acc: 0.4128\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.4436 - acc: 0.3798 - val_loss: 2.3158 - val_acc: 0.4248\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3922 - acc: 0.3886 - val_loss: 2.2817 - val_acc: 0.4280\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3529 - acc: 0.3940 - val_loss: 2.2361 - val_acc: 0.4404\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3259 - acc: 0.3929 - val_loss: 2.2108 - val_acc: 0.4355\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2872 - acc: 0.4051 - val_loss: 2.1662 - val_acc: 0.4501\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2537 - acc: 0.4104 - val_loss: 2.1530 - val_acc: 0.4484\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2316 - acc: 0.4102 - val_loss: 2.1255 - val_acc: 0.4487\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2060 - acc: 0.4149 - val_loss: 2.1045 - val_acc: 0.4538\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1804 - acc: 0.4188 - val_loss: 2.0894 - val_acc: 0.4559\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1594 - acc: 0.4224 - val_loss: 2.0540 - val_acc: 0.4625\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1416 - acc: 0.4234 - val_loss: 2.0358 - val_acc: 0.4597\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1183 - acc: 0.4240 - val_loss: 2.0241 - val_acc: 0.4560\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.0995 - acc: 0.4285 - val_loss: 2.0013 - val_acc: 0.4624\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0816 - acc: 0.4318 - val_loss: 1.9755 - val_acc: 0.4706\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0698 - acc: 0.4331 - val_loss: 1.9823 - val_acc: 0.4567\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0550 - acc: 0.4359 - val_loss: 1.9472 - val_acc: 0.4720\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0304 - acc: 0.4382 - val_loss: 1.9415 - val_acc: 0.4757\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0217 - acc: 0.4368 - val_loss: 1.9276 - val_acc: 0.4721\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0103 - acc: 0.4380 - val_loss: 1.9085 - val_acc: 0.4767\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9970 - acc: 0.4424 - val_loss: 1.9008 - val_acc: 0.4718\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9792 - acc: 0.4415 - val_loss: 1.8860 - val_acc: 0.4775\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9669 - acc: 0.4432 - val_loss: 1.8767 - val_acc: 0.4840\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9584 - acc: 0.4462 - val_loss: 1.8677 - val_acc: 0.4761\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9446 - acc: 0.4483 - val_loss: 1.8676 - val_acc: 0.4752\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9353 - acc: 0.4515 - val_loss: 1.8455 - val_acc: 0.4840\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9269 - acc: 0.4502 - val_loss: 1.8544 - val_acc: 0.4753\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9184 - acc: 0.4510 - val_loss: 1.8313 - val_acc: 0.4890\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9037 - acc: 0.4565 - val_loss: 1.8182 - val_acc: 0.4843\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8962 - acc: 0.4505 - val_loss: 1.8099 - val_acc: 0.4884\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8911 - acc: 0.4534 - val_loss: 1.7991 - val_acc: 0.4828\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8812 - acc: 0.4555 - val_loss: 1.7993 - val_acc: 0.4846\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8744 - acc: 0.4561 - val_loss: 1.7865 - val_acc: 0.4895\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8661 - acc: 0.4536 - val_loss: 1.7900 - val_acc: 0.4834\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8589 - acc: 0.4556 - val_loss: 1.7794 - val_acc: 0.4915\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8548 - acc: 0.4554 - val_loss: 1.7792 - val_acc: 0.4835\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8447 - acc: 0.4595 - val_loss: 1.7602 - val_acc: 0.4909\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8425 - acc: 0.4594 - val_loss: 1.7592 - val_acc: 0.4884\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8399 - acc: 0.4578 - val_loss: 1.7460 - val_acc: 0.4938\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8259 - acc: 0.4621 - val_loss: 1.7403 - val_acc: 0.4995\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8204 - acc: 0.4644 - val_loss: 1.7374 - val_acc: 0.4959\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8172 - acc: 0.4609 - val_loss: 1.7330 - val_acc: 0.4949\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8130 - acc: 0.4602 - val_loss: 1.7436 - val_acc: 0.4921\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8049 - acc: 0.4633 - val_loss: 1.7188 - val_acc: 0.4985\n",
      "Numbers of exp: 9, layer: [128, 256, 256], dropout_rate: 0.40\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 56us/step - loss: 3.6145 - acc: 0.1614 - val_loss: 3.3584 - val_acc: 0.2745\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.2706 - acc: 0.2372 - val_loss: 3.0589 - val_acc: 0.3157\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0501 - acc: 0.2738 - val_loss: 2.8739 - val_acc: 0.3322\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.8894 - acc: 0.2966 - val_loss: 2.7320 - val_acc: 0.3525\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.7650 - acc: 0.3101 - val_loss: 2.5944 - val_acc: 0.3623\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6563 - acc: 0.3202 - val_loss: 2.4928 - val_acc: 0.3782\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5640 - acc: 0.3327 - val_loss: 2.4118 - val_acc: 0.3853\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4889 - acc: 0.3421 - val_loss: 2.3322 - val_acc: 0.4018\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.4268 - acc: 0.3503 - val_loss: 2.2958 - val_acc: 0.3999\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3785 - acc: 0.3565 - val_loss: 2.2444 - val_acc: 0.4044\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3384 - acc: 0.3603 - val_loss: 2.1992 - val_acc: 0.4154\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3070 - acc: 0.3627 - val_loss: 2.1660 - val_acc: 0.4265\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2715 - acc: 0.3717 - val_loss: 2.1305 - val_acc: 0.4299\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2487 - acc: 0.3725 - val_loss: 2.1060 - val_acc: 0.4261\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.2192 - acc: 0.3782 - val_loss: 2.0977 - val_acc: 0.4307\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2024 - acc: 0.3771 - val_loss: 2.0718 - val_acc: 0.4383\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1794 - acc: 0.3828 - val_loss: 2.0359 - val_acc: 0.4447\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1589 - acc: 0.3864 - val_loss: 2.0306 - val_acc: 0.4421\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1416 - acc: 0.3846 - val_loss: 2.0087 - val_acc: 0.4429\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1235 - acc: 0.3888 - val_loss: 1.9889 - val_acc: 0.4459\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1067 - acc: 0.3924 - val_loss: 1.9826 - val_acc: 0.4403\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0941 - acc: 0.3936 - val_loss: 1.9713 - val_acc: 0.4437\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0810 - acc: 0.3917 - val_loss: 1.9534 - val_acc: 0.4411\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0653 - acc: 0.3981 - val_loss: 1.9556 - val_acc: 0.4388\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0505 - acc: 0.3963 - val_loss: 1.9342 - val_acc: 0.4525\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0406 - acc: 0.4018 - val_loss: 1.9235 - val_acc: 0.4462\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0275 - acc: 0.4022 - val_loss: 1.9051 - val_acc: 0.4515\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0205 - acc: 0.4037 - val_loss: 1.9125 - val_acc: 0.4396\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0126 - acc: 0.4018 - val_loss: 1.8940 - val_acc: 0.4565\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9996 - acc: 0.4040 - val_loss: 1.8721 - val_acc: 0.4498\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9867 - acc: 0.4068 - val_loss: 1.8892 - val_acc: 0.4522\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9825 - acc: 0.4060 - val_loss: 1.8648 - val_acc: 0.4536\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9729 - acc: 0.4048 - val_loss: 1.8545 - val_acc: 0.4619\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9627 - acc: 0.4097 - val_loss: 1.8495 - val_acc: 0.4560\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9556 - acc: 0.4084 - val_loss: 1.8430 - val_acc: 0.4588\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9459 - acc: 0.4106 - val_loss: 1.8368 - val_acc: 0.4622\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9415 - acc: 0.4132 - val_loss: 1.8269 - val_acc: 0.4661\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9349 - acc: 0.4101 - val_loss: 1.8185 - val_acc: 0.4665\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9291 - acc: 0.4127 - val_loss: 1.8129 - val_acc: 0.4680\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9170 - acc: 0.4164 - val_loss: 1.8060 - val_acc: 0.4627\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9138 - acc: 0.4148 - val_loss: 1.8234 - val_acc: 0.4539\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9087 - acc: 0.4143 - val_loss: 1.7980 - val_acc: 0.4617\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9046 - acc: 0.4153 - val_loss: 1.7888 - val_acc: 0.4680\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8971 - acc: 0.4145 - val_loss: 1.7787 - val_acc: 0.4673\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8916 - acc: 0.4195 - val_loss: 1.7706 - val_acc: 0.4709\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8843 - acc: 0.4195 - val_loss: 1.7699 - val_acc: 0.4714\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8840 - acc: 0.4201 - val_loss: 1.7744 - val_acc: 0.4638\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8838 - acc: 0.4173 - val_loss: 1.7671 - val_acc: 0.4696\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8713 - acc: 0.4204 - val_loss: 1.7629 - val_acc: 0.4689\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8721 - acc: 0.4234 - val_loss: 1.7602 - val_acc: 0.4690\n",
      "Numbers of exp: 10, layer: [128, 256, 512], dropout_rate: 0.15\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 563,082\n",
      "Trainable params: 563,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 3.6108 - acc: 0.2454 - val_loss: 3.2665 - val_acc: 0.3336\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 3.1553 - acc: 0.3326 - val_loss: 2.9614 - val_acc: 0.3792\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9237 - acc: 0.3586 - val_loss: 2.7784 - val_acc: 0.4037\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.7738 - acc: 0.3803 - val_loss: 2.6576 - val_acc: 0.4059\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.6648 - acc: 0.3973 - val_loss: 2.5560 - val_acc: 0.4301\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5857 - acc: 0.4062 - val_loss: 2.4984 - val_acc: 0.4411\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5206 - acc: 0.4159 - val_loss: 2.4361 - val_acc: 0.4428\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4634 - acc: 0.4269 - val_loss: 2.3779 - val_acc: 0.4545\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4113 - acc: 0.4326 - val_loss: 2.3277 - val_acc: 0.4625\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3699 - acc: 0.4379 - val_loss: 2.2886 - val_acc: 0.4663\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3317 - acc: 0.4417 - val_loss: 2.2538 - val_acc: 0.4676\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2921 - acc: 0.4483 - val_loss: 2.2290 - val_acc: 0.4663\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2574 - acc: 0.4540 - val_loss: 2.1845 - val_acc: 0.4788\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2260 - acc: 0.4567 - val_loss: 2.1617 - val_acc: 0.4784\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1955 - acc: 0.4605 - val_loss: 2.1312 - val_acc: 0.4805\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1651 - acc: 0.4617 - val_loss: 2.1054 - val_acc: 0.4803\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1405 - acc: 0.4662 - val_loss: 2.0785 - val_acc: 0.4878\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1150 - acc: 0.4679 - val_loss: 2.0722 - val_acc: 0.4852\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0938 - acc: 0.4723 - val_loss: 2.0389 - val_acc: 0.4927\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0705 - acc: 0.4759 - val_loss: 2.0217 - val_acc: 0.4908\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0501 - acc: 0.4752 - val_loss: 2.0149 - val_acc: 0.4863\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0320 - acc: 0.4800 - val_loss: 1.9782 - val_acc: 0.4997\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0056 - acc: 0.4816 - val_loss: 1.9609 - val_acc: 0.4990\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9931 - acc: 0.4807 - val_loss: 1.9557 - val_acc: 0.4935\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9781 - acc: 0.4826 - val_loss: 1.9269 - val_acc: 0.5018\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9574 - acc: 0.4885 - val_loss: 1.9233 - val_acc: 0.5012\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9435 - acc: 0.4868 - val_loss: 1.9238 - val_acc: 0.4933\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9281 - acc: 0.4886 - val_loss: 1.8871 - val_acc: 0.5038\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9122 - acc: 0.4922 - val_loss: 1.8877 - val_acc: 0.5004\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8984 - acc: 0.4921 - val_loss: 1.8653 - val_acc: 0.5056\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8854 - acc: 0.4945 - val_loss: 1.8651 - val_acc: 0.5008\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8708 - acc: 0.4975 - val_loss: 1.8511 - val_acc: 0.5008\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8626 - acc: 0.5007 - val_loss: 1.8284 - val_acc: 0.5152\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8512 - acc: 0.5006 - val_loss: 1.8240 - val_acc: 0.5125\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8417 - acc: 0.5014 - val_loss: 1.8245 - val_acc: 0.5053\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8294 - acc: 0.5008 - val_loss: 1.8155 - val_acc: 0.5097\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8233 - acc: 0.5031 - val_loss: 1.7977 - val_acc: 0.5151\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8131 - acc: 0.5018 - val_loss: 1.8236 - val_acc: 0.4934\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8008 - acc: 0.5074 - val_loss: 1.7834 - val_acc: 0.5154\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7910 - acc: 0.5063 - val_loss: 1.7812 - val_acc: 0.5106\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7876 - acc: 0.5046 - val_loss: 1.7729 - val_acc: 0.5141\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7747 - acc: 0.5112 - val_loss: 1.7835 - val_acc: 0.5022\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7696 - acc: 0.5098 - val_loss: 1.7641 - val_acc: 0.5102\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7603 - acc: 0.5116 - val_loss: 1.7529 - val_acc: 0.5079\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7527 - acc: 0.5117 - val_loss: 1.7401 - val_acc: 0.5234\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7465 - acc: 0.5122 - val_loss: 1.7426 - val_acc: 0.5135\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7364 - acc: 0.5129 - val_loss: 1.7349 - val_acc: 0.5171\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7346 - acc: 0.5133 - val_loss: 1.7505 - val_acc: 0.5166\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7242 - acc: 0.5162 - val_loss: 1.7174 - val_acc: 0.5243\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7226 - acc: 0.5135 - val_loss: 1.7222 - val_acc: 0.5150\n",
      "Numbers of exp: 11, layer: [128, 256, 512], dropout_rate: 0.20\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 563,082\n",
      "Trainable params: 563,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.6396 - acc: 0.2357 - val_loss: 3.3269 - val_acc: 0.3252\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 3.2221 - acc: 0.3211 - val_loss: 3.0344 - val_acc: 0.3748\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.9960 - acc: 0.3542 - val_loss: 2.8364 - val_acc: 0.4000\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.8367 - acc: 0.3733 - val_loss: 2.7162 - val_acc: 0.4006\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.7246 - acc: 0.3837 - val_loss: 2.5978 - val_acc: 0.4289\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6320 - acc: 0.3998 - val_loss: 2.5288 - val_acc: 0.4242\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5594 - acc: 0.4106 - val_loss: 2.4466 - val_acc: 0.4387\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5000 - acc: 0.4170 - val_loss: 2.4084 - val_acc: 0.4482\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4510 - acc: 0.4220 - val_loss: 2.3535 - val_acc: 0.4497\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4022 - acc: 0.4287 - val_loss: 2.3113 - val_acc: 0.4594\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3597 - acc: 0.4348 - val_loss: 2.2921 - val_acc: 0.4617\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3232 - acc: 0.4376 - val_loss: 2.2290 - val_acc: 0.4678\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2854 - acc: 0.4426 - val_loss: 2.2172 - val_acc: 0.4669\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2562 - acc: 0.4446 - val_loss: 2.1639 - val_acc: 0.4746\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2182 - acc: 0.4500 - val_loss: 2.1429 - val_acc: 0.4731\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1913 - acc: 0.4523 - val_loss: 2.1171 - val_acc: 0.4778\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1660 - acc: 0.4567 - val_loss: 2.1012 - val_acc: 0.4697\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1397 - acc: 0.4588 - val_loss: 2.0689 - val_acc: 0.4851\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1176 - acc: 0.4596 - val_loss: 2.0587 - val_acc: 0.4790\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0930 - acc: 0.4633 - val_loss: 2.0268 - val_acc: 0.4872\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0726 - acc: 0.4648 - val_loss: 2.0012 - val_acc: 0.4876\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0512 - acc: 0.4650 - val_loss: 1.9888 - val_acc: 0.4872\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0304 - acc: 0.4712 - val_loss: 1.9638 - val_acc: 0.4905\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0085 - acc: 0.4713 - val_loss: 1.9522 - val_acc: 0.4934\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9931 - acc: 0.4733 - val_loss: 1.9457 - val_acc: 0.4863\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9819 - acc: 0.4741 - val_loss: 1.9360 - val_acc: 0.4850\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9630 - acc: 0.4765 - val_loss: 1.9516 - val_acc: 0.4716\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9481 - acc: 0.4802 - val_loss: 1.8906 - val_acc: 0.4977\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9336 - acc: 0.4792 - val_loss: 1.8829 - val_acc: 0.4977\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9246 - acc: 0.4804 - val_loss: 1.8673 - val_acc: 0.4994\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9093 - acc: 0.4841 - val_loss: 1.8679 - val_acc: 0.4971\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8954 - acc: 0.4844 - val_loss: 1.8553 - val_acc: 0.4963\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8848 - acc: 0.4865 - val_loss: 1.8401 - val_acc: 0.4996\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8758 - acc: 0.4874 - val_loss: 1.8573 - val_acc: 0.4904\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8606 - acc: 0.4887 - val_loss: 1.8212 - val_acc: 0.5018\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8564 - acc: 0.4901 - val_loss: 1.8155 - val_acc: 0.5006\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8453 - acc: 0.4914 - val_loss: 1.8149 - val_acc: 0.4981\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8341 - acc: 0.4913 - val_loss: 1.8023 - val_acc: 0.5033\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8239 - acc: 0.4918 - val_loss: 1.7889 - val_acc: 0.5037\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8167 - acc: 0.4938 - val_loss: 1.7812 - val_acc: 0.5067\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8104 - acc: 0.4952 - val_loss: 1.7781 - val_acc: 0.5074\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8064 - acc: 0.4933 - val_loss: 1.7803 - val_acc: 0.5011\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7929 - acc: 0.4966 - val_loss: 1.7663 - val_acc: 0.5018\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7871 - acc: 0.4958 - val_loss: 1.7729 - val_acc: 0.5014\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.7840 - acc: 0.4961 - val_loss: 1.7575 - val_acc: 0.5042\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7734 - acc: 0.4980 - val_loss: 1.7538 - val_acc: 0.5069\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7705 - acc: 0.4969 - val_loss: 1.7421 - val_acc: 0.5105\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7638 - acc: 0.4998 - val_loss: 1.7256 - val_acc: 0.5132\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7556 - acc: 0.5035 - val_loss: 1.7266 - val_acc: 0.5093\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7492 - acc: 0.5023 - val_loss: 1.7214 - val_acc: 0.5137\n",
      "Numbers of exp: 12, layer: [128, 256, 512], dropout_rate: 0.25\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 563,082\n",
      "Trainable params: 563,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 58us/step - loss: 3.6901 - acc: 0.2135 - val_loss: 3.3618 - val_acc: 0.3195\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 3.2709 - acc: 0.3082 - val_loss: 3.0760 - val_acc: 0.3553\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0459 - acc: 0.3387 - val_loss: 2.8876 - val_acc: 0.3748\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.8834 - acc: 0.3593 - val_loss: 2.7618 - val_acc: 0.3858\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7689 - acc: 0.3695 - val_loss: 2.6459 - val_acc: 0.4045\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.6733 - acc: 0.3823 - val_loss: 2.5560 - val_acc: 0.4210\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5989 - acc: 0.3915 - val_loss: 2.4817 - val_acc: 0.4287\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5347 - acc: 0.3990 - val_loss: 2.4174 - val_acc: 0.4343\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4802 - acc: 0.4040 - val_loss: 2.3743 - val_acc: 0.4391\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.4303 - acc: 0.4124 - val_loss: 2.3264 - val_acc: 0.4505\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3911 - acc: 0.4167 - val_loss: 2.2905 - val_acc: 0.4448\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3454 - acc: 0.4217 - val_loss: 2.2450 - val_acc: 0.4591\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3107 - acc: 0.4241 - val_loss: 2.2156 - val_acc: 0.4608\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2724 - acc: 0.4300 - val_loss: 2.1797 - val_acc: 0.4628\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2450 - acc: 0.4336 - val_loss: 2.1515 - val_acc: 0.4650\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2059 - acc: 0.4387 - val_loss: 2.1159 - val_acc: 0.4681\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1861 - acc: 0.4362 - val_loss: 2.1047 - val_acc: 0.4665\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1545 - acc: 0.4445 - val_loss: 2.0857 - val_acc: 0.4673\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1322 - acc: 0.4444 - val_loss: 2.0441 - val_acc: 0.4755\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1075 - acc: 0.4475 - val_loss: 2.0303 - val_acc: 0.4712\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0884 - acc: 0.4467 - val_loss: 2.0182 - val_acc: 0.4665\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0720 - acc: 0.4509 - val_loss: 1.9891 - val_acc: 0.4778\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0543 - acc: 0.4523 - val_loss: 1.9742 - val_acc: 0.4794\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0318 - acc: 0.4533 - val_loss: 1.9665 - val_acc: 0.4737\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0143 - acc: 0.4560 - val_loss: 1.9701 - val_acc: 0.4691\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0015 - acc: 0.4550 - val_loss: 1.9129 - val_acc: 0.4919\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9823 - acc: 0.4598 - val_loss: 1.9081 - val_acc: 0.4869\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9703 - acc: 0.4593 - val_loss: 1.9018 - val_acc: 0.4860\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9610 - acc: 0.4614 - val_loss: 1.8980 - val_acc: 0.4823\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9486 - acc: 0.4613 - val_loss: 1.8741 - val_acc: 0.4885\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9335 - acc: 0.4641 - val_loss: 1.8688 - val_acc: 0.4805\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9218 - acc: 0.4643 - val_loss: 1.8649 - val_acc: 0.4832\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9103 - acc: 0.4686 - val_loss: 1.8444 - val_acc: 0.4976\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8939 - acc: 0.4688 - val_loss: 1.8281 - val_acc: 0.4907\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8900 - acc: 0.4686 - val_loss: 1.8317 - val_acc: 0.4926\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8793 - acc: 0.4705 - val_loss: 1.8207 - val_acc: 0.4938\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8682 - acc: 0.4712 - val_loss: 1.8012 - val_acc: 0.4958\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8556 - acc: 0.4768 - val_loss: 1.8061 - val_acc: 0.4996\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8525 - acc: 0.4757 - val_loss: 1.7851 - val_acc: 0.5005\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8414 - acc: 0.4768 - val_loss: 1.7906 - val_acc: 0.5005\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8330 - acc: 0.4771 - val_loss: 1.7899 - val_acc: 0.4970\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8306 - acc: 0.4758 - val_loss: 1.7880 - val_acc: 0.4942\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8232 - acc: 0.4792 - val_loss: 1.7664 - val_acc: 0.5015\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8118 - acc: 0.4806 - val_loss: 1.7812 - val_acc: 0.4897\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8103 - acc: 0.4786 - val_loss: 1.7691 - val_acc: 0.4975\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8010 - acc: 0.4829 - val_loss: 1.7491 - val_acc: 0.5007\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7922 - acc: 0.4849 - val_loss: 1.7677 - val_acc: 0.4873\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7847 - acc: 0.4844 - val_loss: 1.7294 - val_acc: 0.5070\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7818 - acc: 0.4850 - val_loss: 1.7386 - val_acc: 0.5011\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7757 - acc: 0.4849 - val_loss: 1.7339 - val_acc: 0.5010\n",
      "Numbers of exp: 13, layer: [128, 256, 512], dropout_rate: 0.30\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 563,082\n",
      "Trainable params: 563,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.7088 - acc: 0.2055 - val_loss: 3.3977 - val_acc: 0.3114\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 3.3158 - acc: 0.2940 - val_loss: 3.1149 - val_acc: 0.3438\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 3.0951 - acc: 0.3265 - val_loss: 2.9264 - val_acc: 0.3751\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9398 - acc: 0.3428 - val_loss: 2.7864 - val_acc: 0.3890\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.8216 - acc: 0.3560 - val_loss: 2.6671 - val_acc: 0.4080\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.7227 - acc: 0.3677 - val_loss: 2.5984 - val_acc: 0.4065\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.6453 - acc: 0.3776 - val_loss: 2.5150 - val_acc: 0.4196\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5726 - acc: 0.3875 - val_loss: 2.4558 - val_acc: 0.4240\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5178 - acc: 0.3956 - val_loss: 2.4067 - val_acc: 0.4373\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4653 - acc: 0.4015 - val_loss: 2.3402 - val_acc: 0.4422\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4204 - acc: 0.4045 - val_loss: 2.3044 - val_acc: 0.4435\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3753 - acc: 0.4124 - val_loss: 2.2676 - val_acc: 0.4487\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3415 - acc: 0.4156 - val_loss: 2.2304 - val_acc: 0.4534\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3027 - acc: 0.4218 - val_loss: 2.1925 - val_acc: 0.4616\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2745 - acc: 0.4202 - val_loss: 2.1636 - val_acc: 0.4615\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2369 - acc: 0.4271 - val_loss: 2.1514 - val_acc: 0.4513\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2087 - acc: 0.4307 - val_loss: 2.1096 - val_acc: 0.4626\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1847 - acc: 0.4320 - val_loss: 2.0962 - val_acc: 0.4613\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1563 - acc: 0.4366 - val_loss: 2.0673 - val_acc: 0.4668\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1375 - acc: 0.4351 - val_loss: 2.0539 - val_acc: 0.4715\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1111 - acc: 0.4390 - val_loss: 2.0169 - val_acc: 0.4743\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0921 - acc: 0.4413 - val_loss: 1.9887 - val_acc: 0.4741\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0722 - acc: 0.4399 - val_loss: 1.9802 - val_acc: 0.4810\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0550 - acc: 0.4442 - val_loss: 1.9631 - val_acc: 0.4775\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0385 - acc: 0.4459 - val_loss: 1.9603 - val_acc: 0.4738\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0212 - acc: 0.4445 - val_loss: 1.9470 - val_acc: 0.4812\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0073 - acc: 0.4494 - val_loss: 1.9183 - val_acc: 0.4784\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.9914 - acc: 0.4502 - val_loss: 1.8978 - val_acc: 0.4848\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9762 - acc: 0.4523 - val_loss: 1.9230 - val_acc: 0.4662\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9677 - acc: 0.4512 - val_loss: 1.9140 - val_acc: 0.4702\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9527 - acc: 0.4554 - val_loss: 1.8725 - val_acc: 0.4807\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9401 - acc: 0.4555 - val_loss: 1.8714 - val_acc: 0.4792\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9297 - acc: 0.4552 - val_loss: 1.8556 - val_acc: 0.4800\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9162 - acc: 0.4599 - val_loss: 1.8476 - val_acc: 0.4864\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9112 - acc: 0.4577 - val_loss: 1.8349 - val_acc: 0.4899\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8978 - acc: 0.4606 - val_loss: 1.8288 - val_acc: 0.4878\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8896 - acc: 0.4621 - val_loss: 1.8330 - val_acc: 0.4866\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8860 - acc: 0.4620 - val_loss: 1.8168 - val_acc: 0.4914\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8745 - acc: 0.4619 - val_loss: 1.7983 - val_acc: 0.4921\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8666 - acc: 0.4618 - val_loss: 1.7984 - val_acc: 0.4940\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8613 - acc: 0.4636 - val_loss: 1.7940 - val_acc: 0.4904\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8474 - acc: 0.4679 - val_loss: 1.7756 - val_acc: 0.4958\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8437 - acc: 0.4672 - val_loss: 1.7765 - val_acc: 0.4944\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8369 - acc: 0.4668 - val_loss: 1.7807 - val_acc: 0.4885\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8346 - acc: 0.4648 - val_loss: 1.7755 - val_acc: 0.4932\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8233 - acc: 0.4668 - val_loss: 1.7655 - val_acc: 0.4941\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8189 - acc: 0.4699 - val_loss: 1.7453 - val_acc: 0.4990\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8119 - acc: 0.4691 - val_loss: 1.7582 - val_acc: 0.4918\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8087 - acc: 0.4695 - val_loss: 1.7391 - val_acc: 0.4952\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8063 - acc: 0.4726 - val_loss: 1.7308 - val_acc: 0.4997\n",
      "Numbers of exp: 14, layer: [128, 256, 512], dropout_rate: 0.40\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 563,082\n",
      "Trainable params: 563,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.7753 - acc: 0.1738 - val_loss: 3.4689 - val_acc: 0.2733\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.3716 - acc: 0.2474 - val_loss: 3.1455 - val_acc: 0.3207\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 3.1186 - acc: 0.2902 - val_loss: 2.9226 - val_acc: 0.3482\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9323 - acc: 0.3104 - val_loss: 2.7555 - val_acc: 0.3708\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.7938 - acc: 0.3220 - val_loss: 2.6249 - val_acc: 0.3827\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.6816 - acc: 0.3352 - val_loss: 2.5211 - val_acc: 0.3953\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.6000 - acc: 0.3436 - val_loss: 2.4467 - val_acc: 0.3967\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5247 - acc: 0.3585 - val_loss: 2.3819 - val_acc: 0.4079\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4713 - acc: 0.3591 - val_loss: 2.3344 - val_acc: 0.4117\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4178 - acc: 0.3653 - val_loss: 2.2850 - val_acc: 0.4201\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3770 - acc: 0.3681 - val_loss: 2.2457 - val_acc: 0.4278\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3427 - acc: 0.3715 - val_loss: 2.2257 - val_acc: 0.4157\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3034 - acc: 0.3800 - val_loss: 2.1777 - val_acc: 0.4280\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2738 - acc: 0.3815 - val_loss: 2.1289 - val_acc: 0.4376\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2447 - acc: 0.3837 - val_loss: 2.1135 - val_acc: 0.4347\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2164 - acc: 0.3887 - val_loss: 2.1005 - val_acc: 0.4341\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1921 - acc: 0.3870 - val_loss: 2.0629 - val_acc: 0.4456\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1694 - acc: 0.3887 - val_loss: 2.0501 - val_acc: 0.4419\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1416 - acc: 0.3955 - val_loss: 2.0265 - val_acc: 0.4437\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1224 - acc: 0.3967 - val_loss: 2.0060 - val_acc: 0.4441\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1082 - acc: 0.3987 - val_loss: 1.9853 - val_acc: 0.4515\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0899 - acc: 0.3985 - val_loss: 1.9793 - val_acc: 0.4463\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0715 - acc: 0.4017 - val_loss: 1.9627 - val_acc: 0.4452\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0599 - acc: 0.4042 - val_loss: 1.9400 - val_acc: 0.4503\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0449 - acc: 0.4034 - val_loss: 1.9224 - val_acc: 0.4541\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0313 - acc: 0.4040 - val_loss: 1.9132 - val_acc: 0.4580\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0154 - acc: 0.4060 - val_loss: 1.9181 - val_acc: 0.4500\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0031 - acc: 0.4104 - val_loss: 1.8859 - val_acc: 0.4604\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9947 - acc: 0.4081 - val_loss: 1.8911 - val_acc: 0.4529\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9864 - acc: 0.4092 - val_loss: 1.8788 - val_acc: 0.4602\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9736 - acc: 0.4134 - val_loss: 1.8625 - val_acc: 0.4593\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9629 - acc: 0.4106 - val_loss: 1.8583 - val_acc: 0.4571\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9594 - acc: 0.4123 - val_loss: 1.8516 - val_acc: 0.4581\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9452 - acc: 0.4157 - val_loss: 1.8351 - val_acc: 0.4618\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9445 - acc: 0.4155 - val_loss: 1.8332 - val_acc: 0.4634\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9346 - acc: 0.4163 - val_loss: 1.8281 - val_acc: 0.4583\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9222 - acc: 0.4178 - val_loss: 1.8396 - val_acc: 0.4504\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9166 - acc: 0.4193 - val_loss: 1.8137 - val_acc: 0.4587\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9135 - acc: 0.4196 - val_loss: 1.7965 - val_acc: 0.4628\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9028 - acc: 0.4197 - val_loss: 1.8056 - val_acc: 0.4605\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9026 - acc: 0.4204 - val_loss: 1.7892 - val_acc: 0.4683\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8929 - acc: 0.4221 - val_loss: 1.7974 - val_acc: 0.4630\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8870 - acc: 0.4220 - val_loss: 1.7855 - val_acc: 0.4648\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8816 - acc: 0.4228 - val_loss: 1.7699 - val_acc: 0.4708\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8768 - acc: 0.4240 - val_loss: 1.7679 - val_acc: 0.4736\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8778 - acc: 0.4264 - val_loss: 1.7692 - val_acc: 0.4669\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8681 - acc: 0.4263 - val_loss: 1.7615 - val_acc: 0.4712\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8626 - acc: 0.4260 - val_loss: 1.7549 - val_acc: 0.4698\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8574 - acc: 0.4276 - val_loss: 1.7436 - val_acc: 0.4721\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8511 - acc: 0.4276 - val_loss: 1.7546 - val_acc: 0.4648\n",
      "Numbers of exp: 15, layer: [128, 256, 256], dropout_rate: 0.15\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.4417 - acc: 0.2432 - val_loss: 3.1474 - val_acc: 0.3362\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0743 - acc: 0.3298 - val_loss: 2.9079 - val_acc: 0.3729\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.8789 - acc: 0.3615 - val_loss: 2.7369 - val_acc: 0.4078\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.7498 - acc: 0.3802 - val_loss: 2.6224 - val_acc: 0.4202\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.6619 - acc: 0.3934 - val_loss: 2.5465 - val_acc: 0.4350\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5804 - acc: 0.4071 - val_loss: 2.4809 - val_acc: 0.4433\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5207 - acc: 0.4174 - val_loss: 2.4283 - val_acc: 0.4500\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4695 - acc: 0.4272 - val_loss: 2.3827 - val_acc: 0.4618\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4322 - acc: 0.4308 - val_loss: 2.3498 - val_acc: 0.4592\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3878 - acc: 0.4386 - val_loss: 2.3107 - val_acc: 0.4644\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3535 - acc: 0.4413 - val_loss: 2.2888 - val_acc: 0.4638\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3185 - acc: 0.4517 - val_loss: 2.2520 - val_acc: 0.4714\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2911 - acc: 0.4515 - val_loss: 2.2203 - val_acc: 0.4754\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2612 - acc: 0.4550 - val_loss: 2.2076 - val_acc: 0.4785\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.2347 - acc: 0.4580 - val_loss: 2.1865 - val_acc: 0.4785\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2088 - acc: 0.4632 - val_loss: 2.1471 - val_acc: 0.4879\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1817 - acc: 0.4675 - val_loss: 2.1276 - val_acc: 0.4880\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1599 - acc: 0.4678 - val_loss: 2.1195 - val_acc: 0.4795\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1369 - acc: 0.4701 - val_loss: 2.0864 - val_acc: 0.4882\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1202 - acc: 0.4755 - val_loss: 2.0758 - val_acc: 0.4916\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0978 - acc: 0.4758 - val_loss: 2.0479 - val_acc: 0.4966\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0757 - acc: 0.4814 - val_loss: 2.0478 - val_acc: 0.4900\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0575 - acc: 0.4821 - val_loss: 2.0222 - val_acc: 0.4891\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0425 - acc: 0.4816 - val_loss: 1.9993 - val_acc: 0.5014\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0243 - acc: 0.4839 - val_loss: 1.9869 - val_acc: 0.4974\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0086 - acc: 0.4888 - val_loss: 1.9702 - val_acc: 0.5046\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9929 - acc: 0.4902 - val_loss: 1.9693 - val_acc: 0.5009\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9769 - acc: 0.4915 - val_loss: 1.9489 - val_acc: 0.5055\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9640 - acc: 0.4923 - val_loss: 1.9387 - val_acc: 0.5029\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.9501 - acc: 0.4941 - val_loss: 1.9342 - val_acc: 0.5017\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9309 - acc: 0.4979 - val_loss: 1.9168 - val_acc: 0.5005\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9196 - acc: 0.4974 - val_loss: 1.9059 - val_acc: 0.5020\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9051 - acc: 0.5013 - val_loss: 1.9048 - val_acc: 0.5077\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8999 - acc: 0.4992 - val_loss: 1.8777 - val_acc: 0.5038\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8857 - acc: 0.5003 - val_loss: 1.8765 - val_acc: 0.5036\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8728 - acc: 0.5024 - val_loss: 1.8593 - val_acc: 0.5140\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8632 - acc: 0.5034 - val_loss: 1.8636 - val_acc: 0.5011\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8522 - acc: 0.5077 - val_loss: 1.8514 - val_acc: 0.5088\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8443 - acc: 0.5050 - val_loss: 1.8244 - val_acc: 0.5155\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8324 - acc: 0.5112 - val_loss: 1.8227 - val_acc: 0.5142\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8187 - acc: 0.5111 - val_loss: 1.8066 - val_acc: 0.5192\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8127 - acc: 0.5110 - val_loss: 1.8076 - val_acc: 0.5122\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7987 - acc: 0.5139 - val_loss: 1.8151 - val_acc: 0.5134\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7946 - acc: 0.5139 - val_loss: 1.7929 - val_acc: 0.5118\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7867 - acc: 0.5172 - val_loss: 1.7835 - val_acc: 0.5163\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7754 - acc: 0.5185 - val_loss: 1.7923 - val_acc: 0.5125\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7690 - acc: 0.5176 - val_loss: 1.7705 - val_acc: 0.5194\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7666 - acc: 0.5175 - val_loss: 1.7704 - val_acc: 0.5112\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7549 - acc: 0.5186 - val_loss: 1.7629 - val_acc: 0.5179\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7475 - acc: 0.5221 - val_loss: 1.8144 - val_acc: 0.5023\n",
      "Numbers of exp: 16, layer: [128, 256, 256], dropout_rate: 0.20\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.4763 - acc: 0.2173 - val_loss: 3.1742 - val_acc: 0.3237\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.0948 - acc: 0.3102 - val_loss: 2.9155 - val_acc: 0.3622\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.8796 - acc: 0.3422 - val_loss: 2.7282 - val_acc: 0.3822\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7310 - acc: 0.3639 - val_loss: 2.5972 - val_acc: 0.4016\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.6291 - acc: 0.3745 - val_loss: 2.5058 - val_acc: 0.4150\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5447 - acc: 0.3888 - val_loss: 2.4299 - val_acc: 0.4320\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4850 - acc: 0.4010 - val_loss: 2.3912 - val_acc: 0.4312\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4347 - acc: 0.4108 - val_loss: 2.3540 - val_acc: 0.4311\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3885 - acc: 0.4155 - val_loss: 2.2923 - val_acc: 0.4468\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3523 - acc: 0.4190 - val_loss: 2.2700 - val_acc: 0.4536\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3168 - acc: 0.4229 - val_loss: 2.2189 - val_acc: 0.4575\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2812 - acc: 0.4298 - val_loss: 2.1989 - val_acc: 0.4610\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2531 - acc: 0.4365 - val_loss: 2.1747 - val_acc: 0.4576\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2243 - acc: 0.4388 - val_loss: 2.1399 - val_acc: 0.4717\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1986 - acc: 0.4437 - val_loss: 2.1166 - val_acc: 0.4699\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1737 - acc: 0.4467 - val_loss: 2.1004 - val_acc: 0.4715\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.1512 - acc: 0.4495 - val_loss: 2.0740 - val_acc: 0.4772\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1307 - acc: 0.4515 - val_loss: 2.0599 - val_acc: 0.4755\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1105 - acc: 0.4558 - val_loss: 2.0415 - val_acc: 0.4769\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.0911 - acc: 0.4584 - val_loss: 2.0291 - val_acc: 0.4697\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0710 - acc: 0.4592 - val_loss: 2.0007 - val_acc: 0.4846\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0545 - acc: 0.4626 - val_loss: 1.9935 - val_acc: 0.4873\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0408 - acc: 0.4603 - val_loss: 1.9715 - val_acc: 0.4849\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0185 - acc: 0.4678 - val_loss: 1.9755 - val_acc: 0.4769\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0071 - acc: 0.4666 - val_loss: 1.9646 - val_acc: 0.4793\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9953 - acc: 0.4654 - val_loss: 1.9351 - val_acc: 0.4882\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9708 - acc: 0.4719 - val_loss: 1.9091 - val_acc: 0.4945\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9629 - acc: 0.4723 - val_loss: 1.9079 - val_acc: 0.4925\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9552 - acc: 0.4717 - val_loss: 1.8948 - val_acc: 0.4950\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9353 - acc: 0.4735 - val_loss: 1.8894 - val_acc: 0.4899\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9266 - acc: 0.4773 - val_loss: 1.8746 - val_acc: 0.4917\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9125 - acc: 0.4769 - val_loss: 1.8576 - val_acc: 0.4976\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9037 - acc: 0.4765 - val_loss: 1.8559 - val_acc: 0.4928\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8907 - acc: 0.4810 - val_loss: 1.8610 - val_acc: 0.4836\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8782 - acc: 0.4842 - val_loss: 1.8372 - val_acc: 0.4986\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8659 - acc: 0.4845 - val_loss: 1.8372 - val_acc: 0.4927\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8622 - acc: 0.4838 - val_loss: 1.8276 - val_acc: 0.4934\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8503 - acc: 0.4859 - val_loss: 1.8072 - val_acc: 0.4978\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8407 - acc: 0.4872 - val_loss: 1.8093 - val_acc: 0.5006\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8310 - acc: 0.4898 - val_loss: 1.8008 - val_acc: 0.5009\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8208 - acc: 0.4909 - val_loss: 1.7874 - val_acc: 0.4968\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8167 - acc: 0.4902 - val_loss: 1.7841 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8129 - acc: 0.4896 - val_loss: 1.7887 - val_acc: 0.4960\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8027 - acc: 0.4930 - val_loss: 1.7675 - val_acc: 0.5014\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7972 - acc: 0.4907 - val_loss: 1.7679 - val_acc: 0.5006\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7863 - acc: 0.4947 - val_loss: 1.7585 - val_acc: 0.5047\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7775 - acc: 0.4971 - val_loss: 1.7481 - val_acc: 0.5090\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.7727 - acc: 0.4969 - val_loss: 1.7423 - val_acc: 0.5031\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7697 - acc: 0.4957 - val_loss: 1.7280 - val_acc: 0.5110\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7623 - acc: 0.4969 - val_loss: 1.7351 - val_acc: 0.5048\n",
      "Numbers of exp: 17, layer: [128, 256, 256], dropout_rate: 0.25\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 60us/step - loss: 3.4883 - acc: 0.2119 - val_loss: 3.1880 - val_acc: 0.3130\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 3.1153 - acc: 0.2976 - val_loss: 2.9210 - val_acc: 0.3549\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.9086 - acc: 0.3288 - val_loss: 2.7648 - val_acc: 0.3677\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7659 - acc: 0.3517 - val_loss: 2.6319 - val_acc: 0.3913\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6734 - acc: 0.3611 - val_loss: 2.5570 - val_acc: 0.4003\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5916 - acc: 0.3728 - val_loss: 2.4789 - val_acc: 0.4147\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5304 - acc: 0.3829 - val_loss: 2.4147 - val_acc: 0.4228\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4744 - acc: 0.3906 - val_loss: 2.3710 - val_acc: 0.4276\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4290 - acc: 0.3982 - val_loss: 2.3291 - val_acc: 0.4351\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3877 - acc: 0.4059 - val_loss: 2.2856 - val_acc: 0.4476\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3468 - acc: 0.4130 - val_loss: 2.2460 - val_acc: 0.4505\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3146 - acc: 0.4158 - val_loss: 2.2216 - val_acc: 0.4524\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2849 - acc: 0.4200 - val_loss: 2.1884 - val_acc: 0.4502\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2538 - acc: 0.4245 - val_loss: 2.1571 - val_acc: 0.4666\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2303 - acc: 0.4297 - val_loss: 2.1315 - val_acc: 0.4645\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2023 - acc: 0.4334 - val_loss: 2.1134 - val_acc: 0.4610\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1798 - acc: 0.4360 - val_loss: 2.0964 - val_acc: 0.4649\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1592 - acc: 0.4389 - val_loss: 2.0729 - val_acc: 0.4636\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1391 - acc: 0.4432 - val_loss: 2.0689 - val_acc: 0.4624\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1175 - acc: 0.4447 - val_loss: 2.0448 - val_acc: 0.4683\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0994 - acc: 0.4466 - val_loss: 2.0211 - val_acc: 0.4735\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0864 - acc: 0.4475 - val_loss: 1.9917 - val_acc: 0.4806\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0677 - acc: 0.4487 - val_loss: 2.0060 - val_acc: 0.4615\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0466 - acc: 0.4539 - val_loss: 1.9719 - val_acc: 0.4783\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0295 - acc: 0.4529 - val_loss: 1.9545 - val_acc: 0.4800\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0118 - acc: 0.4580 - val_loss: 1.9447 - val_acc: 0.4803\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0013 - acc: 0.4591 - val_loss: 1.9320 - val_acc: 0.4823\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9900 - acc: 0.4585 - val_loss: 1.9277 - val_acc: 0.4818\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9751 - acc: 0.4627 - val_loss: 1.9031 - val_acc: 0.4878\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9700 - acc: 0.4598 - val_loss: 1.8928 - val_acc: 0.4875\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9474 - acc: 0.4645 - val_loss: 1.8804 - val_acc: 0.4884\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9321 - acc: 0.4656 - val_loss: 1.8654 - val_acc: 0.4907\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9231 - acc: 0.4697 - val_loss: 1.8614 - val_acc: 0.4890\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9163 - acc: 0.4702 - val_loss: 1.8480 - val_acc: 0.4911\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9026 - acc: 0.4696 - val_loss: 1.8432 - val_acc: 0.4925\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8915 - acc: 0.4731 - val_loss: 1.8393 - val_acc: 0.4938\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8830 - acc: 0.4725 - val_loss: 1.8179 - val_acc: 0.5016\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8722 - acc: 0.4754 - val_loss: 1.8065 - val_acc: 0.4974\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8659 - acc: 0.4752 - val_loss: 1.8022 - val_acc: 0.4958\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8567 - acc: 0.4794 - val_loss: 1.7973 - val_acc: 0.4994\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8495 - acc: 0.4783 - val_loss: 1.7887 - val_acc: 0.4969\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8394 - acc: 0.4797 - val_loss: 1.7833 - val_acc: 0.4997\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8306 - acc: 0.4800 - val_loss: 1.7703 - val_acc: 0.5052\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8292 - acc: 0.4798 - val_loss: 1.7645 - val_acc: 0.5031\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8160 - acc: 0.4827 - val_loss: 1.7652 - val_acc: 0.5010\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8146 - acc: 0.4794 - val_loss: 1.7578 - val_acc: 0.5050\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8047 - acc: 0.4814 - val_loss: 1.7479 - val_acc: 0.5052\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7995 - acc: 0.4848 - val_loss: 1.7488 - val_acc: 0.5051\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7976 - acc: 0.4810 - val_loss: 1.7512 - val_acc: 0.4996\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7875 - acc: 0.4838 - val_loss: 1.7302 - val_acc: 0.5055\n",
      "Numbers of exp: 18, layer: [128, 256, 256], dropout_rate: 0.30\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 57us/step - loss: 3.5448 - acc: 0.1920 - val_loss: 3.2564 - val_acc: 0.2950\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 3.1876 - acc: 0.2797 - val_loss: 3.0037 - val_acc: 0.3434\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9927 - acc: 0.3126 - val_loss: 2.8322 - val_acc: 0.3637\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.8440 - acc: 0.3364 - val_loss: 2.6897 - val_acc: 0.3892\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.7288 - acc: 0.3514 - val_loss: 2.5826 - val_acc: 0.3974\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.6364 - acc: 0.3637 - val_loss: 2.5003 - val_acc: 0.4082\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5647 - acc: 0.3735 - val_loss: 2.4367 - val_acc: 0.4169\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5033 - acc: 0.3809 - val_loss: 2.3937 - val_acc: 0.4228\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4549 - acc: 0.3904 - val_loss: 2.3381 - val_acc: 0.4327\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4114 - acc: 0.3927 - val_loss: 2.3025 - val_acc: 0.4276\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3720 - acc: 0.4016 - val_loss: 2.2666 - val_acc: 0.4427\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3362 - acc: 0.4056 - val_loss: 2.2205 - val_acc: 0.4491\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3086 - acc: 0.4125 - val_loss: 2.2002 - val_acc: 0.4475\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2769 - acc: 0.4142 - val_loss: 2.1613 - val_acc: 0.4569\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2496 - acc: 0.4202 - val_loss: 2.1498 - val_acc: 0.4568\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2270 - acc: 0.4214 - val_loss: 2.1173 - val_acc: 0.4570\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2082 - acc: 0.4223 - val_loss: 2.0959 - val_acc: 0.4637\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1813 - acc: 0.4263 - val_loss: 2.0832 - val_acc: 0.4595\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1540 - acc: 0.4289 - val_loss: 2.0778 - val_acc: 0.4541\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1389 - acc: 0.4299 - val_loss: 2.0400 - val_acc: 0.4712\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1195 - acc: 0.4328 - val_loss: 2.0159 - val_acc: 0.4729\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1010 - acc: 0.4373 - val_loss: 2.0286 - val_acc: 0.4621\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0826 - acc: 0.4376 - val_loss: 1.9882 - val_acc: 0.4710\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0646 - acc: 0.4396 - val_loss: 1.9763 - val_acc: 0.4688\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0503 - acc: 0.4426 - val_loss: 1.9659 - val_acc: 0.4716\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0346 - acc: 0.4436 - val_loss: 1.9684 - val_acc: 0.4651\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.0261 - acc: 0.4457 - val_loss: 1.9338 - val_acc: 0.4754\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0065 - acc: 0.4453 - val_loss: 1.9236 - val_acc: 0.4761\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9963 - acc: 0.4454 - val_loss: 1.8955 - val_acc: 0.4854\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9830 - acc: 0.4467 - val_loss: 1.8943 - val_acc: 0.4845\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9691 - acc: 0.4512 - val_loss: 1.8733 - val_acc: 0.4860\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9584 - acc: 0.4529 - val_loss: 1.8781 - val_acc: 0.4797\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9510 - acc: 0.4532 - val_loss: 1.8634 - val_acc: 0.4812\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9372 - acc: 0.4546 - val_loss: 1.8669 - val_acc: 0.4842\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9286 - acc: 0.4550 - val_loss: 1.8737 - val_acc: 0.4716\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9230 - acc: 0.4535 - val_loss: 1.8362 - val_acc: 0.4861\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9074 - acc: 0.4603 - val_loss: 1.8256 - val_acc: 0.4893\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8997 - acc: 0.4609 - val_loss: 1.8211 - val_acc: 0.4884\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8920 - acc: 0.4564 - val_loss: 1.8139 - val_acc: 0.4895\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8840 - acc: 0.4595 - val_loss: 1.8003 - val_acc: 0.4932\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8692 - acc: 0.4612 - val_loss: 1.7947 - val_acc: 0.4948\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8723 - acc: 0.4601 - val_loss: 1.7886 - val_acc: 0.4891\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8609 - acc: 0.4623 - val_loss: 1.7897 - val_acc: 0.4932\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8561 - acc: 0.4612 - val_loss: 1.7830 - val_acc: 0.4917\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8477 - acc: 0.4653 - val_loss: 1.7785 - val_acc: 0.4908\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8358 - acc: 0.4650 - val_loss: 1.7655 - val_acc: 0.4969\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8313 - acc: 0.4675 - val_loss: 1.7686 - val_acc: 0.4883\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8230 - acc: 0.4657 - val_loss: 1.7547 - val_acc: 0.4930\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8175 - acc: 0.4686 - val_loss: 1.7483 - val_acc: 0.4939\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8111 - acc: 0.4683 - val_loss: 1.7563 - val_acc: 0.4899\n",
      "Numbers of exp: 19, layer: [128, 256, 256], dropout_rate: 0.40\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 3.5912 - acc: 0.1620 - val_loss: 3.3190 - val_acc: 0.2750\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.2481 - acc: 0.2335 - val_loss: 3.0392 - val_acc: 0.3291\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 3.0254 - acc: 0.2746 - val_loss: 2.8522 - val_acc: 0.3415\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.8617 - acc: 0.2960 - val_loss: 2.6931 - val_acc: 0.3584\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.7276 - acc: 0.3114 - val_loss: 2.5736 - val_acc: 0.3693\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.6189 - acc: 0.3229 - val_loss: 2.4646 - val_acc: 0.3750\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5350 - acc: 0.3320 - val_loss: 2.3877 - val_acc: 0.3855\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4650 - acc: 0.3402 - val_loss: 2.3439 - val_acc: 0.3838\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4130 - acc: 0.3430 - val_loss: 2.2691 - val_acc: 0.4016\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3677 - acc: 0.3528 - val_loss: 2.2355 - val_acc: 0.4046\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3354 - acc: 0.3565 - val_loss: 2.2025 - val_acc: 0.4099\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2988 - acc: 0.3621 - val_loss: 2.1644 - val_acc: 0.4136\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.2683 - acc: 0.3689 - val_loss: 2.1377 - val_acc: 0.4235\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2416 - acc: 0.3700 - val_loss: 2.1075 - val_acc: 0.4221\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2184 - acc: 0.3722 - val_loss: 2.1038 - val_acc: 0.4234\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2028 - acc: 0.3776 - val_loss: 2.0617 - val_acc: 0.4345\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1768 - acc: 0.3785 - val_loss: 2.0427 - val_acc: 0.4356\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1603 - acc: 0.3815 - val_loss: 2.0205 - val_acc: 0.4362\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.1378 - acc: 0.3855 - val_loss: 2.0173 - val_acc: 0.4403\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1299 - acc: 0.3862 - val_loss: 1.9892 - val_acc: 0.4407\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1095 - acc: 0.3868 - val_loss: 1.9821 - val_acc: 0.4372\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0971 - acc: 0.3915 - val_loss: 1.9605 - val_acc: 0.4475\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0762 - acc: 0.3941 - val_loss: 1.9546 - val_acc: 0.4472\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0689 - acc: 0.3942 - val_loss: 1.9325 - val_acc: 0.4515\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0535 - acc: 0.3976 - val_loss: 1.9325 - val_acc: 0.4508\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0396 - acc: 0.3988 - val_loss: 1.9206 - val_acc: 0.4523\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0281 - acc: 0.3997 - val_loss: 1.9139 - val_acc: 0.4522\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0148 - acc: 0.4016 - val_loss: 1.9143 - val_acc: 0.4443\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0067 - acc: 0.4017 - val_loss: 1.8802 - val_acc: 0.4556\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9991 - acc: 0.4035 - val_loss: 1.8634 - val_acc: 0.4641\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9894 - acc: 0.4020 - val_loss: 1.8639 - val_acc: 0.4548\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9841 - acc: 0.4060 - val_loss: 1.8520 - val_acc: 0.4590\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9728 - acc: 0.4058 - val_loss: 1.8549 - val_acc: 0.4565\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9587 - acc: 0.4100 - val_loss: 1.8392 - val_acc: 0.4622\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9560 - acc: 0.4095 - val_loss: 1.8381 - val_acc: 0.4641\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9500 - acc: 0.4094 - val_loss: 1.8313 - val_acc: 0.4605\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9444 - acc: 0.4076 - val_loss: 1.8250 - val_acc: 0.4562\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9321 - acc: 0.4116 - val_loss: 1.8143 - val_acc: 0.4615\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9269 - acc: 0.4110 - val_loss: 1.8220 - val_acc: 0.4565\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9212 - acc: 0.4113 - val_loss: 1.8024 - val_acc: 0.4662\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9118 - acc: 0.4142 - val_loss: 1.8032 - val_acc: 0.4572\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9141 - acc: 0.4171 - val_loss: 1.7876 - val_acc: 0.4702\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9012 - acc: 0.4155 - val_loss: 1.7840 - val_acc: 0.4693\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8943 - acc: 0.4162 - val_loss: 1.7809 - val_acc: 0.4719\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8960 - acc: 0.4194 - val_loss: 1.7823 - val_acc: 0.4651\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8879 - acc: 0.4161 - val_loss: 1.7979 - val_acc: 0.4543\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8839 - acc: 0.4149 - val_loss: 1.7671 - val_acc: 0.4736\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8740 - acc: 0.4173 - val_loss: 1.7633 - val_acc: 0.4718\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8722 - acc: 0.4201 - val_loss: 1.7577 - val_acc: 0.4699\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8700 - acc: 0.4187 - val_loss: 1.7508 - val_acc: 0.4747\n",
      "Numbers of exp: 20, layer: [128, 256, 256], dropout_rate: 0.15\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 3.4527 - acc: 0.2305 - val_loss: 3.1589 - val_acc: 0.3342\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.0843 - acc: 0.3242 - val_loss: 2.9147 - val_acc: 0.3730\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.8964 - acc: 0.3583 - val_loss: 2.7650 - val_acc: 0.4017\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.7639 - acc: 0.3782 - val_loss: 2.6602 - val_acc: 0.4092\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6655 - acc: 0.3956 - val_loss: 2.5655 - val_acc: 0.4238\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5878 - acc: 0.4067 - val_loss: 2.4962 - val_acc: 0.4333\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.5215 - acc: 0.4155 - val_loss: 2.4347 - val_acc: 0.4463\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4699 - acc: 0.4230 - val_loss: 2.3830 - val_acc: 0.4516\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4213 - acc: 0.4331 - val_loss: 2.3509 - val_acc: 0.4547\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.3837 - acc: 0.4376 - val_loss: 2.3113 - val_acc: 0.4598\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3446 - acc: 0.4431 - val_loss: 2.2794 - val_acc: 0.4656\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3150 - acc: 0.4479 - val_loss: 2.2533 - val_acc: 0.4713\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2812 - acc: 0.4526 - val_loss: 2.2318 - val_acc: 0.4677\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2515 - acc: 0.4589 - val_loss: 2.1965 - val_acc: 0.4791\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2255 - acc: 0.4600 - val_loss: 2.1687 - val_acc: 0.4828\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2005 - acc: 0.4638 - val_loss: 2.1631 - val_acc: 0.4779\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1790 - acc: 0.4638 - val_loss: 2.1231 - val_acc: 0.4857\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1472 - acc: 0.4729 - val_loss: 2.1164 - val_acc: 0.4818\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1331 - acc: 0.4710 - val_loss: 2.0802 - val_acc: 0.4892\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.1095 - acc: 0.4757 - val_loss: 2.0653 - val_acc: 0.4893\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0874 - acc: 0.4781 - val_loss: 2.0476 - val_acc: 0.4911\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0729 - acc: 0.4772 - val_loss: 2.0238 - val_acc: 0.4949\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0529 - acc: 0.4821 - val_loss: 2.0100 - val_acc: 0.4950\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0368 - acc: 0.4833 - val_loss: 1.9973 - val_acc: 0.4941\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0181 - acc: 0.4863 - val_loss: 1.9798 - val_acc: 0.5018\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0028 - acc: 0.4881 - val_loss: 1.9632 - val_acc: 0.5003\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9872 - acc: 0.4891 - val_loss: 1.9556 - val_acc: 0.5045\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9733 - acc: 0.4928 - val_loss: 1.9504 - val_acc: 0.4972\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9573 - acc: 0.4941 - val_loss: 1.9261 - val_acc: 0.5061\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9443 - acc: 0.4972 - val_loss: 1.9212 - val_acc: 0.5003\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9271 - acc: 0.4990 - val_loss: 1.9081 - val_acc: 0.5070\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9163 - acc: 0.4984 - val_loss: 1.8845 - val_acc: 0.5109\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9054 - acc: 0.4985 - val_loss: 1.8760 - val_acc: 0.5067\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8941 - acc: 0.5020 - val_loss: 1.8730 - val_acc: 0.5062\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8787 - acc: 0.5045 - val_loss: 1.8588 - val_acc: 0.5101\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8699 - acc: 0.5051 - val_loss: 1.8433 - val_acc: 0.5114\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8558 - acc: 0.5043 - val_loss: 1.8463 - val_acc: 0.5082\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8461 - acc: 0.5076 - val_loss: 1.8434 - val_acc: 0.5071\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8366 - acc: 0.5099 - val_loss: 1.8279 - val_acc: 0.5121\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8265 - acc: 0.5087 - val_loss: 1.8229 - val_acc: 0.5071\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8182 - acc: 0.5135 - val_loss: 1.8077 - val_acc: 0.5151\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8093 - acc: 0.5103 - val_loss: 1.7950 - val_acc: 0.5178\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7982 - acc: 0.5123 - val_loss: 1.8067 - val_acc: 0.5084\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7934 - acc: 0.5107 - val_loss: 1.7903 - val_acc: 0.5160\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7867 - acc: 0.5122 - val_loss: 1.7950 - val_acc: 0.5115\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7741 - acc: 0.5163 - val_loss: 1.7750 - val_acc: 0.5171\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7624 - acc: 0.5177 - val_loss: 1.7594 - val_acc: 0.5216\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7569 - acc: 0.5172 - val_loss: 1.7649 - val_acc: 0.5127\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7473 - acc: 0.5227 - val_loss: 1.7471 - val_acc: 0.5170\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7412 - acc: 0.5201 - val_loss: 1.7451 - val_acc: 0.5211\n",
      "Numbers of exp: 21, layer: [128, 256, 256], dropout_rate: 0.20\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 3.4729 - acc: 0.2256 - val_loss: 3.1728 - val_acc: 0.3347\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0996 - acc: 0.3136 - val_loss: 2.9122 - val_acc: 0.3659\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.9001 - acc: 0.3460 - val_loss: 2.7475 - val_acc: 0.3875\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.7582 - acc: 0.3656 - val_loss: 2.6285 - val_acc: 0.4068\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6503 - acc: 0.3787 - val_loss: 2.5406 - val_acc: 0.4090\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5644 - acc: 0.3884 - val_loss: 2.4559 - val_acc: 0.4232\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.4985 - acc: 0.3991 - val_loss: 2.4068 - val_acc: 0.4263\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 2.4448 - acc: 0.4097 - val_loss: 2.3529 - val_acc: 0.4375\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4006 - acc: 0.4141 - val_loss: 2.3152 - val_acc: 0.4419\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3534 - acc: 0.4225 - val_loss: 2.2681 - val_acc: 0.4564\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3190 - acc: 0.4292 - val_loss: 2.2282 - val_acc: 0.4585\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2811 - acc: 0.4339 - val_loss: 2.1925 - val_acc: 0.4624\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2570 - acc: 0.4343 - val_loss: 2.1895 - val_acc: 0.4598\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2284 - acc: 0.4403 - val_loss: 2.1507 - val_acc: 0.4688\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2034 - acc: 0.4434 - val_loss: 2.1136 - val_acc: 0.4784\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1724 - acc: 0.4483 - val_loss: 2.0979 - val_acc: 0.4742\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1511 - acc: 0.4531 - val_loss: 2.0712 - val_acc: 0.4819\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1286 - acc: 0.4544 - val_loss: 2.0545 - val_acc: 0.4825\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1071 - acc: 0.4596 - val_loss: 2.0519 - val_acc: 0.4813\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0860 - acc: 0.4616 - val_loss: 2.0202 - val_acc: 0.4808\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0716 - acc: 0.4616 - val_loss: 1.9977 - val_acc: 0.4912\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0488 - acc: 0.4653 - val_loss: 1.9872 - val_acc: 0.4861\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0389 - acc: 0.4662 - val_loss: 1.9652 - val_acc: 0.4882\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0165 - acc: 0.4675 - val_loss: 1.9515 - val_acc: 0.4914\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0021 - acc: 0.4706 - val_loss: 1.9500 - val_acc: 0.4877\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9891 - acc: 0.4701 - val_loss: 1.9313 - val_acc: 0.4957\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9706 - acc: 0.4725 - val_loss: 1.9031 - val_acc: 0.5003\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9591 - acc: 0.4762 - val_loss: 1.8951 - val_acc: 0.5016\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 1.9452 - acc: 0.4752 - val_loss: 1.8890 - val_acc: 0.4966\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9315 - acc: 0.4793 - val_loss: 1.8936 - val_acc: 0.4855\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.9192 - acc: 0.4814 - val_loss: 1.8673 - val_acc: 0.4943\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9051 - acc: 0.4790 - val_loss: 1.8542 - val_acc: 0.5005\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8888 - acc: 0.4854 - val_loss: 1.8528 - val_acc: 0.4966\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8835 - acc: 0.4854 - val_loss: 1.8343 - val_acc: 0.5015\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8687 - acc: 0.4849 - val_loss: 1.8194 - val_acc: 0.5068\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8615 - acc: 0.4878 - val_loss: 1.8203 - val_acc: 0.5008\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8494 - acc: 0.4916 - val_loss: 1.8109 - val_acc: 0.5062\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8476 - acc: 0.4893 - val_loss: 1.7979 - val_acc: 0.5047\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8353 - acc: 0.4915 - val_loss: 1.8002 - val_acc: 0.5046\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8229 - acc: 0.4915 - val_loss: 1.7919 - val_acc: 0.4996\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8151 - acc: 0.4934 - val_loss: 1.7827 - val_acc: 0.5051\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8118 - acc: 0.4935 - val_loss: 1.7675 - val_acc: 0.5084\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8022 - acc: 0.4964 - val_loss: 1.7782 - val_acc: 0.5030\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7923 - acc: 0.4960 - val_loss: 1.7559 - val_acc: 0.5134\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.7830 - acc: 0.4967 - val_loss: 1.7518 - val_acc: 0.5142\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7730 - acc: 0.5008 - val_loss: 1.7546 - val_acc: 0.5060\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7728 - acc: 0.4977 - val_loss: 1.7386 - val_acc: 0.5135\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7659 - acc: 0.5000 - val_loss: 1.7351 - val_acc: 0.5108\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7572 - acc: 0.5033 - val_loss: 1.7395 - val_acc: 0.5066\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.7495 - acc: 0.5017 - val_loss: 1.7379 - val_acc: 0.5073\n",
      "Numbers of exp: 22, layer: [128, 256, 256], dropout_rate: 0.25\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.5231 - acc: 0.2061 - val_loss: 3.2305 - val_acc: 0.3078\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 3.1643 - acc: 0.2935 - val_loss: 2.9814 - val_acc: 0.3537\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.9643 - acc: 0.3252 - val_loss: 2.8223 - val_acc: 0.3690\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.8179 - acc: 0.3486 - val_loss: 2.6734 - val_acc: 0.3922\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.7004 - acc: 0.3643 - val_loss: 2.5614 - val_acc: 0.4105\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.6119 - acc: 0.3723 - val_loss: 2.4944 - val_acc: 0.4044\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.5367 - acc: 0.3836 - val_loss: 2.4293 - val_acc: 0.4211\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4791 - acc: 0.3922 - val_loss: 2.3593 - val_acc: 0.4322\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.4325 - acc: 0.3972 - val_loss: 2.3137 - val_acc: 0.4368\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3871 - acc: 0.4044 - val_loss: 2.2727 - val_acc: 0.4445\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 2.3474 - acc: 0.4109 - val_loss: 2.2469 - val_acc: 0.4465\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3171 - acc: 0.4134 - val_loss: 2.2282 - val_acc: 0.4431\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.2850 - acc: 0.4197 - val_loss: 2.1823 - val_acc: 0.4539\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2529 - acc: 0.4243 - val_loss: 2.1552 - val_acc: 0.4612\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2245 - acc: 0.4285 - val_loss: 2.1270 - val_acc: 0.4634\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.2046 - acc: 0.4307 - val_loss: 2.1143 - val_acc: 0.4614\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1774 - acc: 0.4348 - val_loss: 2.0835 - val_acc: 0.4703\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1584 - acc: 0.4390 - val_loss: 2.0745 - val_acc: 0.4676\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1335 - acc: 0.4411 - val_loss: 2.0527 - val_acc: 0.4717\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.1165 - acc: 0.4435 - val_loss: 2.0255 - val_acc: 0.4760\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0949 - acc: 0.4472 - val_loss: 2.0175 - val_acc: 0.4740\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0781 - acc: 0.4465 - val_loss: 1.9907 - val_acc: 0.4802\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0627 - acc: 0.4493 - val_loss: 1.9870 - val_acc: 0.4731\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0429 - acc: 0.4539 - val_loss: 1.9623 - val_acc: 0.4769\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0246 - acc: 0.4545 - val_loss: 1.9472 - val_acc: 0.4824\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0160 - acc: 0.4562 - val_loss: 1.9339 - val_acc: 0.4887\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9978 - acc: 0.4556 - val_loss: 1.9188 - val_acc: 0.4915\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9838 - acc: 0.4597 - val_loss: 1.9210 - val_acc: 0.4852\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9690 - acc: 0.4622 - val_loss: 1.8969 - val_acc: 0.4898\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9571 - acc: 0.4611 - val_loss: 1.8831 - val_acc: 0.4864\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9454 - acc: 0.4655 - val_loss: 1.8660 - val_acc: 0.4958\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9364 - acc: 0.4615 - val_loss: 1.8695 - val_acc: 0.4932\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9258 - acc: 0.4649 - val_loss: 1.8474 - val_acc: 0.4958\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.9103 - acc: 0.4679 - val_loss: 1.8448 - val_acc: 0.4936\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.9067 - acc: 0.4660 - val_loss: 1.8365 - val_acc: 0.4944\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8875 - acc: 0.4711 - val_loss: 1.8293 - val_acc: 0.4923\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8868 - acc: 0.4686 - val_loss: 1.8171 - val_acc: 0.4998\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8682 - acc: 0.4755 - val_loss: 1.8049 - val_acc: 0.4981\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8614 - acc: 0.4760 - val_loss: 1.8016 - val_acc: 0.4945\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.8579 - acc: 0.4747 - val_loss: 1.7904 - val_acc: 0.4987\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8455 - acc: 0.4770 - val_loss: 1.7901 - val_acc: 0.4991\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8355 - acc: 0.4781 - val_loss: 1.7781 - val_acc: 0.4983\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8265 - acc: 0.4780 - val_loss: 1.7666 - val_acc: 0.4995\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8230 - acc: 0.4786 - val_loss: 1.7567 - val_acc: 0.5018\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8101 - acc: 0.4813 - val_loss: 1.7520 - val_acc: 0.5029\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8053 - acc: 0.4833 - val_loss: 1.7420 - val_acc: 0.5057\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8067 - acc: 0.4798 - val_loss: 1.7667 - val_acc: 0.4964\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7966 - acc: 0.4818 - val_loss: 1.7357 - val_acc: 0.5062\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 1.7875 - acc: 0.4837 - val_loss: 1.7361 - val_acc: 0.5043\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.7859 - acc: 0.4830 - val_loss: 1.7327 - val_acc: 0.5045\n",
      "Numbers of exp: 23, layer: [128, 256, 256], dropout_rate: 0.30\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 3.5181 - acc: 0.1968 - val_loss: 3.2298 - val_acc: 0.3051\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 3.1682 - acc: 0.2822 - val_loss: 2.9703 - val_acc: 0.3399\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.9516 - acc: 0.3132 - val_loss: 2.7861 - val_acc: 0.3585\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.7870 - acc: 0.3357 - val_loss: 2.6289 - val_acc: 0.3826\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.6700 - acc: 0.3502 - val_loss: 2.5235 - val_acc: 0.3949\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.5825 - acc: 0.3588 - val_loss: 2.4481 - val_acc: 0.4084\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.5071 - acc: 0.3682 - val_loss: 2.3912 - val_acc: 0.4145\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.4531 - acc: 0.3772 - val_loss: 2.3264 - val_acc: 0.4163\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4020 - acc: 0.3866 - val_loss: 2.2971 - val_acc: 0.4260\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3651 - acc: 0.3906 - val_loss: 2.2476 - val_acc: 0.4319\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.3312 - acc: 0.3943 - val_loss: 2.2147 - val_acc: 0.4379\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2961 - acc: 0.3998 - val_loss: 2.1766 - val_acc: 0.4438\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.2708 - acc: 0.4029 - val_loss: 2.1573 - val_acc: 0.4466\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2403 - acc: 0.4114 - val_loss: 2.1290 - val_acc: 0.4535\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 2.2113 - acc: 0.4160 - val_loss: 2.1017 - val_acc: 0.4504\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1895 - acc: 0.4158 - val_loss: 2.0890 - val_acc: 0.4533\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1702 - acc: 0.4164 - val_loss: 2.0654 - val_acc: 0.4589\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1496 - acc: 0.4190 - val_loss: 2.0342 - val_acc: 0.4648\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1274 - acc: 0.4274 - val_loss: 2.0152 - val_acc: 0.4694\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1090 - acc: 0.4285 - val_loss: 2.0175 - val_acc: 0.4686\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0906 - acc: 0.4305 - val_loss: 2.0025 - val_acc: 0.4610\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 2.0774 - acc: 0.4302 - val_loss: 1.9714 - val_acc: 0.4685\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0588 - acc: 0.4360 - val_loss: 1.9527 - val_acc: 0.4735\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0404 - acc: 0.4400 - val_loss: 1.9447 - val_acc: 0.4777\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0281 - acc: 0.4363 - val_loss: 1.9283 - val_acc: 0.4763\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0093 - acc: 0.4408 - val_loss: 1.9164 - val_acc: 0.4747\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9970 - acc: 0.4432 - val_loss: 1.9079 - val_acc: 0.4734\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9874 - acc: 0.4435 - val_loss: 1.9031 - val_acc: 0.4741\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9764 - acc: 0.4425 - val_loss: 1.8879 - val_acc: 0.4793\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9661 - acc: 0.4452 - val_loss: 1.8809 - val_acc: 0.4744\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9539 - acc: 0.4452 - val_loss: 1.8611 - val_acc: 0.4818\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9413 - acc: 0.4478 - val_loss: 1.8532 - val_acc: 0.4818\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9319 - acc: 0.4527 - val_loss: 1.8332 - val_acc: 0.4879\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9240 - acc: 0.4486 - val_loss: 1.8286 - val_acc: 0.4852\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9094 - acc: 0.4505 - val_loss: 1.8243 - val_acc: 0.4864\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9002 - acc: 0.4511 - val_loss: 1.8099 - val_acc: 0.4879\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8936 - acc: 0.4537 - val_loss: 1.8038 - val_acc: 0.4884\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8861 - acc: 0.4561 - val_loss: 1.8012 - val_acc: 0.4901\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8774 - acc: 0.4576 - val_loss: 1.7819 - val_acc: 0.4911\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 1.8699 - acc: 0.4559 - val_loss: 1.7819 - val_acc: 0.4931\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8603 - acc: 0.4573 - val_loss: 1.7718 - val_acc: 0.4926\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8568 - acc: 0.4578 - val_loss: 1.7819 - val_acc: 0.4849\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8479 - acc: 0.4625 - val_loss: 1.7681 - val_acc: 0.4950\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8434 - acc: 0.4606 - val_loss: 1.7601 - val_acc: 0.4904\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8390 - acc: 0.4596 - val_loss: 1.7457 - val_acc: 0.4990\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.8303 - acc: 0.4646 - val_loss: 1.7501 - val_acc: 0.4924\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8272 - acc: 0.4590 - val_loss: 1.7496 - val_acc: 0.4916\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8195 - acc: 0.4616 - val_loss: 1.7410 - val_acc: 0.4959\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8117 - acc: 0.4633 - val_loss: 1.7357 - val_acc: 0.4956\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8120 - acc: 0.4628 - val_loss: 1.7323 - val_acc: 0.4941\n",
      "Numbers of exp: 24, layer: [128, 256, 256], dropout_rate: 0.40\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 494,730\n",
      "Trainable params: 494,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 3s 57us/step - loss: 3.5950 - acc: 0.1617 - val_loss: 3.3310 - val_acc: 0.2684\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 3.2569 - acc: 0.2350 - val_loss: 3.0520 - val_acc: 0.3215\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 3.0316 - acc: 0.2734 - val_loss: 2.8434 - val_acc: 0.3410\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.8688 - acc: 0.2936 - val_loss: 2.7018 - val_acc: 0.3558\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.7383 - acc: 0.3103 - val_loss: 2.5794 - val_acc: 0.3645\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.6424 - acc: 0.3196 - val_loss: 2.4990 - val_acc: 0.3663\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.5560 - acc: 0.3261 - val_loss: 2.4178 - val_acc: 0.3734\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.4871 - acc: 0.3356 - val_loss: 2.3519 - val_acc: 0.3885\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.4307 - acc: 0.3425 - val_loss: 2.3065 - val_acc: 0.3934\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3799 - acc: 0.3496 - val_loss: 2.2397 - val_acc: 0.4029\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.3414 - acc: 0.3524 - val_loss: 2.2043 - val_acc: 0.4097\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.3031 - acc: 0.3636 - val_loss: 2.1861 - val_acc: 0.4107\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2733 - acc: 0.3635 - val_loss: 2.1382 - val_acc: 0.4234\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 2.2470 - acc: 0.3694 - val_loss: 2.1431 - val_acc: 0.4113\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.2247 - acc: 0.3674 - val_loss: 2.0980 - val_acc: 0.4270\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.1990 - acc: 0.3764 - val_loss: 2.0664 - val_acc: 0.4327\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1781 - acc: 0.3801 - val_loss: 2.0503 - val_acc: 0.4394\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.1598 - acc: 0.3808 - val_loss: 2.0274 - val_acc: 0.4359\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1430 - acc: 0.3808 - val_loss: 2.0231 - val_acc: 0.4326\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1198 - acc: 0.3873 - val_loss: 1.9887 - val_acc: 0.4420\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.1092 - acc: 0.3869 - val_loss: 1.9823 - val_acc: 0.4404\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 2.0940 - acc: 0.3902 - val_loss: 1.9668 - val_acc: 0.4439\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 2.0769 - acc: 0.3913 - val_loss: 1.9645 - val_acc: 0.4453\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.0680 - acc: 0.3970 - val_loss: 1.9371 - val_acc: 0.4460\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.0523 - acc: 0.3959 - val_loss: 1.9217 - val_acc: 0.4524\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0356 - acc: 0.3976 - val_loss: 1.9137 - val_acc: 0.4508\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 2.0293 - acc: 0.3975 - val_loss: 1.9065 - val_acc: 0.4509\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 2.0193 - acc: 0.3997 - val_loss: 1.9053 - val_acc: 0.4467\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 2.0047 - acc: 0.4035 - val_loss: 1.8916 - val_acc: 0.4489\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9995 - acc: 0.4010 - val_loss: 1.8698 - val_acc: 0.4541\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9848 - acc: 0.4038 - val_loss: 1.8657 - val_acc: 0.4552\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.9831 - acc: 0.4053 - val_loss: 1.8565 - val_acc: 0.4557\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9696 - acc: 0.4053 - val_loss: 1.8565 - val_acc: 0.4469\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9655 - acc: 0.4045 - val_loss: 1.8455 - val_acc: 0.4539\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9537 - acc: 0.4074 - val_loss: 1.8456 - val_acc: 0.4521\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9477 - acc: 0.4082 - val_loss: 1.8294 - val_acc: 0.4566\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9363 - acc: 0.4126 - val_loss: 1.8190 - val_acc: 0.4598\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9361 - acc: 0.4082 - val_loss: 1.8177 - val_acc: 0.4612\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9238 - acc: 0.4134 - val_loss: 1.8034 - val_acc: 0.4635\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.9197 - acc: 0.4146 - val_loss: 1.8073 - val_acc: 0.4567\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.9109 - acc: 0.4150 - val_loss: 1.8090 - val_acc: 0.4549\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.9055 - acc: 0.4155 - val_loss: 1.7849 - val_acc: 0.4644\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8976 - acc: 0.4178 - val_loss: 1.7883 - val_acc: 0.4643\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8928 - acc: 0.4181 - val_loss: 1.7789 - val_acc: 0.4642\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.8889 - acc: 0.4192 - val_loss: 1.7722 - val_acc: 0.4647\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8850 - acc: 0.4179 - val_loss: 1.7808 - val_acc: 0.4657\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.8775 - acc: 0.4190 - val_loss: 1.7812 - val_acc: 0.4606\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.8739 - acc: 0.4186 - val_loss: 1.7717 - val_acc: 0.4618\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8727 - acc: 0.4201 - val_loss: 1.7718 - val_acc: 0.4676\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.8707 - acc: 0.4157 - val_loss: 1.7553 - val_acc: 0.4655\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\"\"\"Code Here\n",
    "撰寫你的訓練流程並將結果用 dictionary 紀錄\n",
    "\"\"\"\n",
    "for i, (layer_neurons, drp_rate) in enumerate(itertools.product(LAYER_NEURONS, Dropout_EXP)):\n",
    "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
    "    print(\"Numbers of exp: %i, layer: %s, dropout_rate: %.2f\" % (i, layer_neurons, drp_rate))\n",
    "    model = build_mlp(input_shape=x_train.shape[1:], num_neurons=layer_neurons, drp_ratio=drp_rate)\n",
    "    model.summary()\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              verbose=1,\n",
    "              shuffle=True)\n",
    "\n",
    "    # Collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "\n",
    "    exp_name_tag = \"exp-dropout-%s\" % (str(drp_rate))\n",
    "    results[exp_name_tag] = {'train-loss': train_loss,\n",
    "                                 'valid-loss': valid_loss,\n",
    "                                 'train-acc': train_acc,\n",
    "                                 'valid-acc': valid_acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1575384458039,
     "user": {
      "displayName": "林建宏",
      "photoUrl": "",
      "userId": "12416564836120847438"
     },
     "user_tz": -480
    },
    "id": "PhQLdyi6ARrj",
    "outputId": "f6c24e99-e368-4151-9b0a-071594d68648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAF1CAYAAABPriuUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYmRglZERCRGCloREZEYKWhFRERi9P+UFqOTq5HkJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAF1CAYAAAAnXamsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd3QVVdeHn0khpEDooffee5Xei4CK\nICLCC4hiQ0UUGyqI2LChiKICSkeUJk1Aeu+dhB5KQkhI77n7+2PffDeBhCQQIOp51jpr7p0+c5P5\nzdlnF0tEMBgMBoPBkDNxut8nYDAYDAaDIX2MUBsMBoPBkIMxQm0wGAwGQw7GCLXBYDAYDDkYI9QG\ng8FgMORgjFAbDAaDwZCDMUJtMBgMBkMOxgi1wfAPwbKsDZZlXbcsy+1+n4vBYLh3GKE2GP4BWJZV\nFmgJCNDzHh7X5V4dy2AwpI0RaoPhn8GTwA5gBjAoeaZlWe6WZU2yLOu8ZVlhlmVtsSzL3b7sAcuy\ntlmWFWpZlr9lWYPt8zdYljUsxT4GW5a1JcV3sSzrOcuy/AA/+7yv7PsItyxrr2VZLVOs72xZ1puW\nZZ22LCvCvryUZVnfWpY1KeVFWJa11LKsl+/GDTIY/q0YoTYY/hk8Ccy2t86WZfnY538GNACaAwWA\n1wCbZVllgJXAZKAwUBc4kIXj9QaaANXt33fb91EAmAMstCwrt33ZK0B/oBuQFxgCRAMzgf6WZTkB\nWJZVCOhg395gMGQSI9QGQw7HsqwHgDLAAhHZC5wGHrcL4BBgpIhcEpEkEdkmInHA48BaEZkrIgki\nEiwiWRHqiSISIiIxACIyy76PRBGZBLgBVezrDgPeFpGTohy0r7sLCAPa29d7DNggIoF3eEsMhv8U\nRqgNhpzPIGCNiFyzf59jn1cIyI0K942USmd+ZvFP+cWyrFctyzpuN6+HAt7242d0rJnAE/bPTwC/\n3sE5GQz/SYyjiMGQg7GPN/cFnC3LCrDPdgPyAcWAWKACcPCGTf2BxunsNgrwSPG9aBrr/H9ZPft4\n9Gtoz/ioiNgsy7oOWCmOVQE4ksZ+ZgFHLMuqA1QDFqdzTgaDIR1Mj9pgyNn0BpLQseK69lYN2IyO\nW/8MfG5ZVnG7U1cze/jWbKCDZVl9LctysSyroGVZde37PAA8bFmWh2VZFYGhGZxDHiARCAJcLMsa\ni45FJ/MjMN6yrEqWUtuyrIIAInIRHd/+FViUbEo3GAyZxwi1wZCzGQRMF5ELIhKQ3IBvgAHAGOAw\nKoYhwMeAk4hcQJ27RtnnHwDq2Pf5BRAPBKKm6dkZnMNqYBXgC5xHe/EpTeOfAwuANUA48BPgnmL5\nTKAWxuxtMNwWlohkvJbBYDDcJpZltUJN4GXEPHAMhixjetQGg+GuYVmWKzAS+NGItMFwexihNhgM\ndwXLsqoBoajT25f3+XQMhn8sxvRtMBgMBkMOxvSoDQaDwWDIwRihNhgMBoMhB5PjEp4UKlRIypYt\ne79Pw2AwGAyGe8bevXuviUjhtJblOKEuW7Yse/bsud+nYTAYDAbDPcOyrPPpLTOmb4PBYDAYcjBG\nqA0Gg8FgyMEYoTYYDAaDIQdjhNpgMBgMhhyMEWqDwWAwGHIwRqgNBoPBYMjBGKE2GAwGgyEHY4Ta\nYDAYDIYcjBFqg8FgMBhyMEaoDQaDwWDIwRihNhgMBoMhB2OE2mAwGAyGZCKiYOdBiI2732fy/+S4\nohwGg8FgMNwW8fGwZhs0qwsF82V92x8WwrjvICgE3HJBi3rQrgm0awoNa4Cr69057wwwQm0wGAyG\nfzYhoSqyk2fD5atQoRRsmAkli2a8rQj8thre+AJO+0PbJjD0Ydh7DNbvgLe/Br4GLw9o1RCSkqBB\ndRg/EpzujVHaCLXBYDAY/rkEBEHFLhAVAx2bw9gRsG4HFCmQ8bab9sDoT2HXYahZCVZMhS4twbJg\nwIO6zvnL8MmPkJAIm/bCybOweivUqASP97i712bHCLXBYDAYMiYoRMdvy5e6v+chAlv2wr7jMHIg\nFC0MY5+FLg9AjYrao+7WSoX72nU47AedH0i9j2OnYMwXsOxvKOEDP38AT/YCZ2fHOkf94PsF8MtS\nCIuAhV/AD+PgUiD8vRM6NLtnl2yE2mAwGAzpExAEXZ+GAyf0++S34PkB9+dcVm+Bt7+CPUchX15I\nSIALV9Rk/dMiOHtRe7434pEbCuRT87W7Gxw8qZ8nvqxi757bse71MOj1PGzeC7lcoU8neKYfPNBA\nl5fwgSd63pvrtWOE2mAwGAxgs8H+4/DXNm01K8FXb0KRglC6GDzaGXYeghcmwPVwePsZNRFnhW/n\ngO85eGEAVCyT+e18z0H/UdqLTiY0HEZ/Bnk8oWJpqF0ZHu6gPX4nJ4iM1rHr2ctVyCvkhSpltafd\nvim8PgwK5dd9BV6DvUehW2t9AShcAD4ZBYMf0s/3GSPUBoPB8F/n3CXo85KKFUDtKtpzBBW9Jd/q\n58REGPqOCnZSErhkUkJEVNRbNYTnP1Cnr97t4ZVB0LSOYz87tkKBglC5qn4PCIIZi+HbuXAxADw9\n1NGrYU0V54qlVWxv9cLwzgh4brw6m9WuDLM/gVy5dNmuQ3ouC1Zp7zlwM3i4w6Kvsnb/7jKWiNzv\nc0hFw4YNZc+ePff7NAwGg+G/w5NjYP5KDUEa1gce65baHJwSm03Ny265tMea1yt9wRaBr36FLftg\nwefw1afw9zo47guBASAJkLcwXLyiYtu0Fpw8DgOfg7Wn4cRZ3UebxjD0Ee3Vu+VK/zqCr6nQWxaE\nhWk4lYeH7mPiD/Dz77B9Lpzxh5ET9YXDywMG91ZzfpVyd34vbxPLsvaKSMO0lpketcFgMPzTOXRS\nTdVZCRcSga37tKc5ezk4WXDsDAx5G56fAN1awsMdoXtrFeNknJxULOPiof0Q8CkIQx5Wp61jp6FY\nYahaDkr6qDPWqi3Qqx1Ex8DZ0+B/DqpXhuZNIDAM3L3VOevEWbDKg1yGGV9DvDdUbAILvoS61TK+\nnrWrYfhAGPMuDH8OfvoOPnwXmraANh2gc0d4rj9454WAa2q+//pNGNQ79fXlQEyP2mAw/Hs5cQZ+\nWaJewC3qZ31MNbOI2B/+YVC9os57+yvYdkAdnK4EQaOa6ln81KPZd9zdh+H9KfDnRvj9a3ioA2w/\nAHk9NXwoPf7eCU+8rh7S7rnVWeqVQSq6G3fD72vhj7V6TblcNezp4Q7QuLZ6TO89BvuOwbb9EB2r\n+3Ry0vjlgGvqHZ6MmyvUKAe1qkPlMprxy/e8jjv7ntMx49Q3E4omQNRpaNIclq2D3On07kEdyiaM\nhc8/guo1YeYCqFIN9u6GPxbAhrVw6ICu61MUjvurBSDZHJ9DuFWP2gi1wWD4d3H+EsQlQOWyKtTV\n7LGutavAs4/BgB7g5Xnnx0lKUi/kn36HlZshJlbHTP1W6fLHR+vYb7kSOo66YTfUrwbTP1SR+PIX\nDfGpWSnrgrHrkAr0ik1quq1TRQW3eBGNIb4cCM3rw8gnNIFHcpauddvhncmw46B+79gM5nzmcKpK\nic2mov/7Xyrc5y45lrm6QK3KUL+6CuWsZfp51Q96PhU662/wZE/YsRJO7AbX2hAQqoJetoSKdpVy\n+jtVKavTqyHqJNauKSxZBAf2wnsT078P/hdgSH/YuQ0GPwUffwXu7jevF3QVNqyDS/7w0ms6b2Af\nKF8RhjwDZcpm7f7fBYxQGwyGfzdx8bB4nYborN2uITULvtBloeHw2xr1OD5wArzzgN/KO/fmfeED\n+GaOilzfLlC9AlQqA51apL9NfLw6MvmdgyrdVbArltaecLO60Lwu+BRK/xr3H9de7NjJ+mJgsz+/\n3XKpufl6OFwMVJFNSS5XFesrQfpS4J4b5n0GD7bN3LVGRcGPP8L+QzB4KDRv6HDIAli8FvqNgh5t\n1BFr3zEoUxymT4Fxb8HAITB5mnpiu+W69ThzWhzYByeOwWNPpJ6//i94sg98MRUe7Z/5/cXFwdDH\n4c8leq86d4ennoP2ne5ZtrEbuZVQIyI5qjVo0EAMBoMh00yYKlKgqQjVREq3E3l3ssi5izevZ7OJ\nbNsv8sF3jnkffi+yaI3Ihcsi18NEEhLSPkZ0jMispSJtB4vsP6bzDhwX+W21SESEyKU0jpcRV66K\nTJ0n0vkpEddaev5/btBlKzaKlGor0vBRkQZ9RMq0F3GqoetQTaRoS5GHXxT58heRHQdE4uIc+01M\nFLkUKLJ6i0ifkSK564p0HibSf5RIuY4i9R5O+/6kx8inRXw8RPKgrVQ+kVkzbl7v750i5y85vk+e\npOsPHaDndCcMe0L39darItHRIuvWOJaFhNz+fi/6i4x/R6SCj+5/xjSdHxYm4ntCZMc2kVV/isz9\nVWTKVyJJSbr8WtDtHzMdgD2Sji7ed2G+sRmhNhj+IwRfF3lpogpKVrHZtImIjJsi0vdl3U9WBCEu\nTqRCJ4f4JbcXPtDliYkidXqLtBggkq+JLivXUWTVZsfy2TNFapbVh/zgx0SuXbu9awmLENl7VGTi\nOJGpU0QGvSji08xxTlZ1EY96Ip/8qCKcFUJCReLj9XNAkL50pEdoqMi8WSIvDnfc33deE3npGZG/\n14oc2CfSp7vI8sW6LDo67ZebhXP1njz5aPovP1khPl5k1POOFwVvJ5FTflnfj80mkmg/n5hIkT2r\nREIC9G9h4Vy9fhGRCe86XkxStuvXdfnlS2nu/k64lVAb07fBYLj3nL+k2a6On9GEFefXQn7vzG17\nxl9jeUf0g75d7+w8EhN1TPfCFTXLRkRBgxrq6RwbB/1e0fklfDSEp01jNY1euQy9O8Hxo1C1BpSs\nBId2a5hTdLymnAyL1GlohO4jjycUyqcm6EL5wcMZ9qyD6DB4/wvYsR/GDQdSPJPdvWDY8zB+goYb\nfT9Zj+/s7Ji2bg916un60dEajpRVzp6BD96BJb+peb54CR1TL1rs1tt9MFYdtsZOgJ4PO8baQ0Ph\n60/hjffurOJUfBwc3QL7VsP1QCjcUvc7oAeM/CR1ys+MsNnguxcgIgRGz4JjW+G11rqsQDEoXxcq\n1IVOQyA4CvZvB4nRuGoPdx379sgNtduCR57bv6Z0MOFZBoMhe0hI0LHg5Rthxof6YN5+ABrXyvxD\nc/8x6PYMxMbD6mmQL4+KtAhMnqXhMt5pPAhtNpgyF17/HFycITHpzq/HxeXmPNDJ5HZzJPpIxv8C\nRCVqVaXQRMhVFXYnwe6TQB54dwq4OoHHJchbAYqUA28vFejIaDh7CXYdhFA/cA4CbJDgDS2fACyo\n2R3a1oVqxSEuAk77QbMmKsrhYRpudCOfTlah3rUD+nSFES/BiJGQL5NlHndshe5tVVCHjoCH+0Gj\nJpkbq23YBJYuUsesBo2hbQcY/bYee+yEzB0/LXYuhz+/g0MbIC4aXFyhVmt4eTDUqwivt4Xoc/Da\nbMh1C4/wZGw2mPw0rPoR+ozWa6vUAD7+G04fgDP2tv8vKFEP4lzBFgTzx968rymHoFyt27+22yG9\nrnbKBnQBTgKngDFpLB8MBAEH7G1YimWDAD97G5TRsYzp22C4R4RHZn7dS4E69luslZpiy3bQcV3f\nsyIutUTaDBLxv5K5ffV6Tsdfj/imnr/jgJp4C7cQ+WFBajP2qfMirQbqsbs8pce+V5zxFxkzTqRs\nKZE8ziJU1vMo0UZkwGiRaQtFDvvqmHN0jMjJ4yJ1KqqptH9vkXNnHfvaud0xHvpoD5G160V2HxZZ\nuUnkxJlbn4fNpmOk8fEisbEiUVE6Pp48Pn3imMhjvXTfJb1FPhib/vhtVJTI/r36OT5e5P03b9+c\nm5go8uvPItVK6bHfeyPjbS6dEln2rcj0N0Q+HSjyeluRoZVELtr/JhZ/rd+/fV5kxzKR6IjU2//+\nhUgXRF5vJxIVnsH5JegxuiAy822HST8Zm03k2BGRie+LNKvpMHMXRKSsk0jjEiIPNxN5eYDIV2NF\nNv/tMJFnI9zJGDXgDJwGygO5gINAdblZqL9JY9sCwBn7NL/9c/5bHc8ItcFwl4mPFxn1sUjdhxzz\nxkwSGf2pis7G3TqWmfxA27pPxLmmimjX4SLLNzhE1GYTmf67iGcdkXwVRYYM1fHNnh1VrCZNdBwj\nWVBCw0UuBqR9bnuOiDwwQIWwTm+RDbt0/oKVIt6NRX5edPODNiOyur6Int/nM0RqdxBx99IHd15X\nkWbtRKbMFvE7d+v9xsaKfPahOmEVzi3y01Sdf/26SL+e6qR0tzi4X+Txh/Sca5RJ/cKTmCjyy08i\nVUroC0PMLcars0pMjMiyP/QlQETvj/9JkTXTRb58SmR4dZHD9vH9zb+pcHZ3EXmytMgrzUU+7OsQ\n6vR8DWJiRPbu1vv710yRbs4iLzQUuX41/fP6cpgea854xzybTcfb339LpH4V++9riXRsIfLNF/r7\nzJ8tMu5tkYF9RBrXECngmnq8+vcFd3zLUnKnQt0MWJ3i+xvAG5I5oe4PfJ/i+/dA/1sdzwi1wXAX\nuRigzlFUU3FOptVAkVy1UztV9Rmpy+LjRd7+Snu1IvqQ2/S39ginTtZ56zc5HmCFPERaNxIZ1Fe9\nam02kSEviFSoJRKQjkCnxGYTmb9CPbjLtHc4Ql27fnvX3Luztt8X6AM+PQKviXw7R++FVV3EKq/X\nU8RTZPQr6gmcVfwv6H1oWuvWjm47l4usnCZybJtI5G0cJy0OHxRZ/Jt+TkoS+WicCk4eRNo2Edm6\nKXuOI6LXdvWCSLDdqnLhuEjfQiqQXRDpk0/k7a4iR7fq8phIkeDLmXf+8z0p8uYokTIF9fyLeurL\nyEcjRXp7iuxakf62hzeJLJqkv8Vv80RefUGklv239XYS6dFO5IdvRa5kYKVJSBDx8xX5c4nIFx+L\nnM3AApJFbiXUGTqTWZbVB+giIsPs3wcCTUTk+RTrDAYm2s3fvsDLIuJvWdarQG4R+cC+3jtAjIh8\ndsMxhgPDAUqXLt3g/Pnztzwng8FwG6zdBo+/pqkcfxyv+ZxTkpSkTlXJGaNK+GgKyWQSE2Hp75qv\nef8eKFQYPvka+jym2x4+CD//CUUKa2Wl5G1GjINfpoBbMHjng2czOYYaE6uOYykzbMXHg995OHIK\njvhpuxqimbi88+h4cF4vuHwCataHUqXg3An48TMICQKvvNC4DTRuC4VL6Dh3dAys3qrJQKxIKOYB\nw1+Efl3hyC7o2BXy3KHz0NVAvV83jvvGRsE3z8K6X1LP/2Qj1GoF546A3x4oXQPK1IDct+EoBrBq\nEbzaB7wLwuip0OsR+Ho4XDiuY8Bx0Xou1ZvDG/N1m5/HQGI8eBeGvIW0la4GpapCUiJ8/j+4egGu\nnodrF8GWBI+8CsM+hfhYva5qzXSfpaplPOYtAls/hIB90OEz8CgByxfD9O9h43r1J+jRG7o8CLu3\nw8plcPkSuAL1mkHXB6F9B6jTEGKi4I/vISKXjsHv2gYX/fU4Hh7QvJXeg+699HfJAdxRwpNMCnVB\nIFJE4izLehroJyLtMivUKTFe3wbDXSAhAao/qIkvfvsSqlXI+j4e6wUrlkKFSvDCKOj/ZNpZoJJZ\nvUUTc+w6rML9cAv4ZLwmmfD2hrfGwzMvpL99XDxs2KXbH7UL88lzKv6gzmuVykDxwhARrR7WIYEQ\n4wtOkRBXEOKL2Hcm4BwFrqHgEgGJ3hBb3L7MBiU9IHcIBJ6H0mVg9/FbX1t2ERUGLzaEdk9A2wFw\n4RicPwpdh0PegjB/Isx4U9fNlRua9oS2T0CjruCcgS9wUhLsXQXLp8CelWA5QZ328OFqXf75/yDI\nH9w8HK1cbXjoJV3+YkPwP6ECnkyrfvDGPP38bB3wyAtFykCR0jqt3Bgq1ru9exHsCz/WUcFPAna6\nwcZI/T0GD9ekKT5FHeuLaGrQlctg5VI4txcqAsGFwTMY8thgP1CwFDRurulIm7aAmrXvzBP9LnGn\nQt0MeE9EOtu/vwEgImnmdbMsyxkIERFvy7L6A21E5Gn7su+BDSIyN73jGaE2GLKRa9c1LMgtl72X\nXERLBWaGoKvww7fw3Mva+123BqKjoFvPzHl4D35TU0tOeQeG93XMP3QAPh6nhRKeejb1NhFRsHIT\n/LFO02OGR+r88qWgZkWoUVFTbtaspOknXZwhLEgLO0yaCF9+rAL75njo1RciY3Sfzk7aI3NxhohQ\nFftSpcH3GDzSSUWtTFl4eQwMGAxubpm7R7eDCGxaAM16qfjGxYBbOi8FSUkQcEZ71gfWwqb5kJgA\ncwN125ArkL9o2ilIZ7ypQl+gmAp/l6egUImsn29cDIQH6312dknb4/nMafhrJWzbpN89PLV5eoKn\nl06T58XHwfUQRwsLgpBw/ZwYCP6XoTPqFZW3LozYnblymge2wge9ICpYv7d6Bga+CSVLpV4v9CxE\nBUKJpvr9+hmQJHDJDc5uOnX1AKd7GxR1p0Ltgpqz2wOXgN3A4yJyNMU6xUTkiv3zQ8DrItLUsqwC\nwF6gvn3VfUADEQlJ73hGqA3/eQKvaQhUaIQ+qG0CIweq4K7foSkyk2zg6a5FFHwKQtdWKsZJSQ4R\n3bYf+r6i6TS/fCNr57BiqVYiioiAXxaqmTCriGiRigLpmLjFXhThh+/g3DU4eEmvLT5B03v2bKup\nNVs3TDs3d8R1eLe7mlWv5YfJk+DBFprLuuotenWB5+Hgem1+B6BgR+1lJZ1RE27VplCtqZqbsxKn\nmxmiwjVMaOM8GDEZej6f8TYpSUzQXnf5Onr/nq4OCXHQ5nGo8YDut9vTanK+6KshR80f0vCm7CQ2\nFrZugjUrVKBP+Wqwb7v84F8YImI07WhUpA5XpIWrK5TNC53Dwb80xFWD/AXUYtP/SQjdCPGR0GCE\nXqvYwCmD3yP4MnwzQu9H636O+WKD06thz2Q4vRJ86sKw/brs50Zw5QbNKdYIhuzSzzu/UPEuUBkK\nVoY8JdQ6kc3cca5vy7K6AV+iHuA/i8gEy7LGoYPfSy3Lmgj0BBKBEGCEiJywbzsEsNtumCAi0291\nLCPUhv80s5bC4LdUcFPivx5KFoUJU7UYg5OTmoaTiditYvbqJ/DdfBVv/wAoXQx++wLqVc/c8W02\n+PA9NVHXawDTZkHlqnd+XQkJmoP63KUU7TIcPwXHloKVCPkaw6O94aH20LzerUUy5Aq81UnFaMw8\nqNwCNi6Hn4bq8hKVoE47TU5RvyPkKQAb5sLMt7WHCjr2WqcdjJymCSy+e1HXCb+my929oMUjMGqG\nfp87AYIv6fLwa9oTbNgVhn6iQvJaayhRGSo3UhNw2ZqpBfLUPviwLwSegyfHw6Ov31le6aQk2DAH\n1s/S3rbNpuc8YjJ0HJy1fYlojPjVQB1/TkzU/Sem+JyUqGPCa1fBpvWaXMXNDVq2hQ7tIdcfEHUW\nXryoQnbtBBSsottGRak1JipKK2HlLwCB2+H3PiqCjy6FEo3TP7/jC2Hbx9DjZ/CpnbVr810G60ZB\niB94+kD9Z6BSTyhm7z+e+Ut72ElxkBir0zwlobrdCvR1KYi46Nifizs0eQXafJC188gAU5TDYMip\nHDwBMxZD15ZazOGMv9bwfbInlCtpz0BlqenvRvNmfDwEXYfAYK1cBLB8g5YwDAyG/Hlh/IuQL2/m\nz+e9N7Rc4IDB8PmU2x+nvRigZu9VW7TM442FIpyc1AxftgQ0rASLv9WKTH9t0zHJWxFwFsa0h8AL\nEF0W/jqmBSJsNjh3WHvKB9bDkU0QHQ7vLtGx3d0rYeX3Ks512qlz1o33VASunIETO7TlLQhPvKfL\nBpWB2GjwLuRwrqrfCXqM0J7yR/3g5C7NfAVqmh7yCfR6ATbMg0mD9OVgzDyomU6SldslJAB8d0Gt\nNuCZV3uQIX5wZa86Z3kVg6ajdN1p9SAqFOKB6EQIj4Zj0bAtNnPHKlsOOnaDTt2gZRtwtWBhTzi7\nToW0zmCICYHJpSBfeWj6KtToD84pCnHs/xFWjYCCVaHfcvDO4Df3XQYrhul+m4yC4o0hf0WHaIst\ndS/32gnInQ+8ioLfn7BlPDR6Aao9mvo8MoMIRFyGEF9HK9FU95WNGKE2GHISh07C+p0wc7FWc8rl\nCh++BKP+l33HOLgfVi2H3n20Nm9muXxJTZlPDk2/9GJCvKZfdHJWz+RkoqK1hvHMJZqWUwQa1oRq\n5VWQyxa3T0tASZ/U1ZeOHlbzfWEfWLMFCqZTQSopCYZXh0un4ajA+Kla3jDNdRO1F1u6uvY07xSb\nLXOeywFnwXe3CmeTB6F2G3XK+nUsPDdFhT7l9WxcD/Nn6f1u2kJb5aqZ622HBMCetXBwBxwP0Wv2\n2Qlel8HZ7nRnc4LwknCxnmY6K30c3EW9pd2dIG9ucKoKFYZDkbxw9TcoOwRyeekLopOzTp2dIa+3\nCnXy30ZCDCzsBWfXOkQaICkejs6DHZ9C0BE1FzcaCfWGQ+gZ+Kk+lO8CD88Ht0y+SEYHw18vwZFZ\n+r36Y/CQ3d1pUn6wnMGjELh4QOB+aD4G2k7McXWn08MItcFwrxGBC5fhsJ8Ks03U8zkiAipUhGAP\nqNcQujeB4QOgRAY5lTPiwnlY9gfUbwjNHlDha2bvbdRrqGN+fR5LOxTl9wWa4/nnuembmy+fgj2r\nYO9qOPS3egLX6wgfrIJ1W2H6d7DMV9Nkli2hFoGBPaHiLXpKInDmIBQtr73A7VugV0dNiTloWNrb\nHDsCA9qrh/fURdDpDnN93y/OnoHZM2DODA0bypdPHbWC7Wb3fPmhcTNoUg+qFYGiHtBgKJzyg9Wj\nIXgjEAEuditFBPBnWcjtDlWDIFcShLlri3ADnFSsypSDmnWgVh2dliuf+oXg6FxYPAAKV4dHFqnp\n+lYc+An+fCq1SKdEBM6shh2fwbl18NRhKFIT/JZDhS6357AVGaDN1V3PTwQ2vQcx1yD6GsReh9Kt\noO5T4OWT9f3fJ4xQGwzZycgPYcdBFd+kJO1pueWCnfb401Efw4+LHB7LoD3LDdPhka6wYxt8OgWG\nDIXGNSAiHF55Q3uGuTORtxj04XTimMY1L/9De9AAr4yB9+y9iKCr8Ns8mPeL1vN1cYHj/o4Ql8RE\neP9NjYtu0hwWLIf8+R3H8OPS32EAACAASURBVN0DlRtqcYrX2oDfDshTDPJXB5cyEJEPNh4A6yhU\nCwT36jBsEnTJZE3fFT+oY5WrGzTqBi37QvHaUDmN8fSDf8Pp/fDdIjh/Fhb+6ShE8U8hOhqWLIJZ\nP8PmDSqc7TpC//7QrhUUKAOnT8P6z+HSMpBAyG33VRDgZ2+4FgYNgTIu4FUcfCpC+XpQswXUfih7\nzvPMX7DkcR2v7f4jVO+X/roial4vnnYZ5VSEnIICFbPnHP+FGKE2GLKTJetg8mw1WTs7a+hPbjeY\nN0mXT5mrcb+1KkHtKhpK5OoMfXvoA/qnOfCI/eG3eYMWWti6SSsWjXpTzc43hgeFh8OBvRASrOZs\nEahZVh2AGjeDBx+CHg9pbz0tjh2BTX874pZfeAr27dYkJUNHwEdf6Lj2YT9NJrHvR4g/A3vrw7Vo\nyBsDCc4QYzdXu7hA0UJ6jY93haR98NvHGmb0v480HOhGsY4Kg5U/QPFK0Lw3RIaqM9TFk7B5oTqI\n5faEWZfg+EmY+wt8+jXsXAYT+6mD2Kjf9BilSmfDDwmEXYBtH6mZt0IXKNvuzvcpApFXdCzzymE4\nvB4u7INFwXAxCloXgabR2htOjHZs9+xpyF8e9n2vPdXCNcGzLFwVOHEVridAgybQqKkOZ9yJI1pG\nhF+EP/rBxW3Q5Tto8IxjWUIMrBgOzd/QnrchWzBCbTDcCQkJ8N63Wp7wlcFZ3z4uDvr3hnWrYepM\n6D8w9XIRFdEJYzWL0vR5KuTL/tDMTPt2g+8JXa+ID/hd0d7Yjq1qyixWPO3jpocIDOqvLwkVm0C0\nlyYTiYiAMiFQMUgd2JwaQbkuUKIoFCusrXgRnRbKf7NQXPTV0JiD6zVE6IWpOj/oIiz5ClZ8DzER\n0H0EPD8l9bY2m457++6Gh1/RXv6s16B8OQg/D6754NeTqcd374SkeNj5uToZ2ZLUGalkMxi4UZcf\n/hUKVYei9dIOxUmMhXB/Ffqw8xB2Dqr1hcI14O/JsH2kY90EINwJEnvAY69CCdFxVrc84OrlmFZ5\nKGeZapMSNFNYg2fB0z5kkhADv/XWXnevX6HmgPt7jv8ijFAbDKBm6nFTNLtVcu83I85fgv6jtZTj\niMdgShpl7zIiNhYG9dVEIemNvYIK6JaN0Lyl9tTHvAy/zYX6jbSEYP1GOgadnqPVrUhM1GtYtkHb\nCXuIUr682iuuWR6uz1JHn7qd4KXvwads1o8jouFCZWpqhqrZ78PcD3R+y0c1xWSlBhnvx2aDJ+vD\nlYMQCdTuD1NmZl9Gqb1T1eu4XDc4WRquhIJzDCTm0VCx0gvAskGSG8T6gC0PWJXVvO8ZAbFf37BD\nJ4hsB3+ehuCzmiHLqzTU7wLtH9WUlbmy6G2ck7AlwqI+EB0EF7dDj5+gTjY6PxqMUBsMBIVA/1fV\nG/mdETDuBY1DHvwm9OsC3VvfLAKL1sCwsSrwP7x/c27sjEhM1LhRb+/b8zyNi9OH++16rIZHahrP\nZRs0y1dwqJqs2zSCB9voNZcq4qjnO2e8Jg954JHs85LdMBdO7NS0lFkVfpsN3h2jLyYjR9/5OUVc\n1qxUpVpAYhzMeh/G/6TDCaVucHrLnQg+MY6Wy6YpLbfGQS7RFE7hKVoEmoe7dXsNW+rYNeMws38S\nkYEwrwsEHvx3i7TNBpf9NLWqZz6Nv89TADy977rnuBFqw7+DM/4aJ9ysLjTKQuH2HQfh0Zc1nea3\nb8MQe5atI37QcSgEXAWfwvBkL2hfT0NWYpOg6wvQoDbM+0xTWGaFpCR4ZjAcP6LZtjLrJJYVIqM0\nqUlyu5jis/8VOHUBEhKhgDd0a6Xi3PkBLV4hAtv+gKkj4fU5ULNl9p9fTiEpQTNSbXoXPApD1zUw\n6nkdimjQGL7+Qb2gb0Xyi5aIOoVFRqjfQGSENicn3dfd+J1zColxEH4BClTKeN3sIilRC4f47dFh\nEb89EHRB49db94d6HW4/61piAvgf1xC+5HbmQOrc5sk4OYNXfodw5y0Ij72lGeCyiVsJ9b1NZmow\nZJXwSK2GBJqR65cl+rlTC3jnGXggAzNqaDh0fkrHVLfNhmO7oE1jCL0O4WEQHwqFnKFpB/h8Jkwe\nD67huq0nEHgVnh8IKzbovFV/Qky09vK88oCXl1aESlkswGaDkU9rbOzYCdn78E5Kgvkr4cMf1GHt\nRooWglLFtOhGz3bQvZW+2KTMley7B34aDYc2aCrK3Gmk5/wnEBkJaz6Gy2v0N0h2wGvxFpRoApd3\n6xh0iC8En9S43YDaGrbm7KyVv556NnNpQpN7U5Zlz1/tmfo3/y/g4nb3RTrwHBzdon+jfrvV0z8u\nRpe559Fhkzrt1MFw3a+adKblo9CmP1Rvkb6DXVyMJsM5vd8hyucOa/pV0P+B8nWh0xCoUE9DBqPD\nNXlNRLB9GuL4HnJFhf4eYXrUhpxHQoKWHfxlCSz9G3bOgzpVtUcdn6DzJs2Aq8GaAzutPNbx8Y4x\nwb+2QcMakN8bZk2HH76BilXUJJ3X3l4Zozm2v5sBbetrKsWLF9SrGuALu/NT55Ya75uS2nVhiz08\nqlsbjWG+HgKvvQNvj8uee5KYCHNXwAdTtbhGjYrweHcoU1yFuVRRzfSV0TjoN8/Cn99phqzHx0L3\nZzKuwpQTiItTD/V9u7Xt3wMhx2CwQCgQiwq1pxeUHgZthoHLJfjrZS2wULgnfDgHDh1UX4HPvrm5\nWIPh3hMerKF3B9bC/rVw5bTOd/NQwazcCCo11DDB4pUcQhwfp5XBNsyFnUtViAuXgtaPaZhffIyK\ncbIwXzimToOgPeMK9aBifUcrVjH787pnEWP6NvwzuB4G476D2ct1TLlQfujfDV4epOk0UxIdA9N+\ng9qVoW0TFe1dh3Xc9eRZeGQkjHlKk26cPK5FA7r30m0zk2EqPUJDwf+89sgjI9RT2tPTse+vPoUz\np6BWXRj6TNbHtU7t07f7klXg9AHNPHU0BiZ8r6bs2lVg7AgtVpHZawgP1oeTkxMs/QZCA+GR0Zpk\n5H4SdExzL3sUTHv5KT+YOQ02rtOXnwR7D6ZkQajeRJ3rKrpAyZawa6e+QG3for8NaI+32QNq+Zgz\nU79/OllD2f4Bmar+dYhoL9V3t4rygbX69y6iveU6baFuB6jdWrPJZfYFMiYSti+BjXM1IU9SomNZ\nfh+o2EDFOFmci5TJkb+/EWrDP4NT56HLcKhXTceLuzyQ2sHL7yQ8NRAWrYSCNzzcP5gK73wNdarA\naX9wzw1zP4WQc/DiU5qRa+/J++t5Gx8P+47D5j1w/IyGe/kUVHN1vtxwYJaa55v11nzQzzwAl3fB\n2QLg0RLGPq/m7MwIdPgluLQL9myCJT/BiG+h/cCMt7sXhJyCze/Dkdn2NI8fwtUjcO0YVOipwws/\nfQcb1qnJvnlLHf+t1xA8jsL+j+HxNVAqjXzZNpu+mG3fAts2w44tahUZOgLe/VCtKIaMSUyA6wFa\nTezaJZ0mFyTB0r9By+nmKahwRoVqiwzV+Pko+zS5V+viClWb6RhzvQ7ac84Oy07YNa29naegRh0U\nuMOMf/cQI9SGnI3Npm+4lqUm3rRqzx45pOkl8+SF3cdUwMe+DpWqQJ/+us2sZfDxT1A4P8ycAJM/\n1gd+0xYwY74mFLmXhEdqSNSWfbB5L+w8pFm+QAU6LFJDt4pGQNVAcEuEC/nhtI+aa2OioGUiuJ+G\nsrXgtdlp1wJOyaUdsHE8nF0F2NNLiisMPABlqmsijeggLWiQv6KW7XPNZH3qrCKiFghvb32Qh13Q\nMeOD07UwQqMXoOlozc/8+xA4Ph1CnWFLEkSUgkFPa/IXn6Ka53n5/8BvGVTsAQ9O1+0yQ8phEENq\nkhLh7CE4vl3bxZMqyqGB+vulxNVNh0xA485tNscUcXx3zwNe+dRr2tNbp175wMNbv5epqTnisyP/\n+r8I40xmyLmIwEsTISYOvn8vbZHeswse7gIeHrBwuYp0XJwWj/jyE3jrVRg4RHtN//tTvXK7ttZx\nzBdf1Z5UdsXf3oqkRFg7F6ZPgOCLsLEUJFjgkQjVq8Iz/eCB+tp8Cum1L/4WfngBilaDxs9AUkHN\nEBYSpo5gXVtp1acvh8CLDeGj9VCjRerjxoaB5QJunnBlH5xeA5dt4F0LWvcCn5Iq0gAnFmk93v/H\ngnLt4fG/9KvfcnDz1qpGyUkussrlS+pIN2em9m6dnVVsO0ZDkTCIrQ5eXeBCRbi2Bn6fD6uXQSWg\nkwf0iIB8rtCqjG53YRMsflxfMDp9BQ1fyJrp8t8g0qf2wbbFjp5pdDhEh9l7q/bv8bFQqCQULafO\nUCmnPuV0qCPsmlYFO75NhfnkLoizZ0crUAzK1VYHw0IloWAJbYXtn/MUyJEm4/8CRqgN95dJ0zUd\n5yuD0jbpbtsMfbpBoSKwdK1W7gF1HNp2UFNvTvsWvvsKJk+CH36Fx56A9p1g9FvQo/fdv4aIEE2g\nsnQKJIZCjKvmwn7jWfVK3/oJ7FwCIQfhWAOIbwCNuup4Wdchmn6005D0nVkad4Mph2DRJKhir9lr\ns8G1o7D+fTi9FAK9YOJZqDsE3KpBvmJQKo060o+tgLgIuH4arvtpOcBcKXo2K56GyMv62b2g9rqr\nPgzNXtN5FzZDnuKQtzQ4p3j5iY6GZb/Db9PgwGaNNa5dBR55AOKrw5V4uHoajlyFcwEQ/Jlj28JF\n4KUxMHg4lC6tLwtbxmvVJdAEG64eMGi7o4bwf4WEeJg7HuZP1N6qR17tlXrk1R5q/qLqz+CRV3u8\nQf5avevoFhXvlHh6q6iDmpnL14XOQ6Facw0zKlLaCHEOxZi+DfePeSs0CUnfLjD3s7SF+uB+GPUc\n/LLw1qbrK5dhxjQY8vSdhc3Ex8PidVCkINSteutazokJ4HsePnhfs3qFeEKZTvDWBKiTorTk8e1w\ndCuc2qvtkh8UrwjfH8t6DKgIHF8KcwdA7ihIAoKdwKcDDJumD9s7Iew8XDvuaKFntBJRy7Eaj/xx\nbkft3zwlIdEJAnxg1jGwRcDzN+7Qgi7fQoMRqWfHx0NggCYbqVbj5l6viKb5dHHT4yVEp36h+C9w\n9hB89qRWGOswCJ7+Uk3ImUEEIq9rbe3AszoNugCFS6soV2qoCVoMOQYzRm24NyT/LWXmrXzDLo1v\nbloHVk/TXmVKTh531FG+V/VkI6Lg4Rc1QUky5UpC/epQr6o6udWqCGe2wOyP4FIirEgCdzd4sjWM\nGa0lHjMiKkx74UXLZf0cRWBaPQg8DtE+0OR5aD9MzZJ3m5go2DgDjmwE/4OagzspDs65aW3gxx6H\nvGe1J+5eAHLn1zrEnkXu/rn9m0hKhIWfwOz39Hd98Qdo2vN+n5XhLmPGqA13l4QEmDpfE5J4ukP/\n7joeeyvRio3THuviyTeL9LxZMGIwfPMTDBiUvSIdH6exmiWrpDY1B17TYhEHTsDUd/Xc9x2D/ce1\nrVgOJUO1uSVBtCtcLQHvjoLnB2goWWbxtDvVZAYROLsWdnwKvWbruHHfP8CrGLjcZiIVEdi3B7Zt\n0p6xpyd4eGryFg/7Z09PrW3se1yLf+zYqmP+cXZnuHIVoGk/aNNBw528/mO93cwSEwnLvoGV08Cn\nDNRqA7XbQJUmkMvt5vX9T8CkQTp23KovPPtt9hUiMfxjMT1qw50RFw8N+miWrHZNVHRXb4V1P0Pr\nRprWEqCk3RydkOBw7Eornnn6D/DSM9CyDcxbmr0CcD0Q3u0OfnvVhFi7rYZClWujvfvLQbDwC43F\nBs0C5uSkLwqfDYH1M8GnJni6QukK0KYN1B+i5tkYe+xu7nzZ82IhAmdWw+ZxcGm7mpkfXqAVnm6H\n+Hit0PXnEli5VB2+MourK9RtoN7zTVto7eoiOajKU04kNhqWT4GFH2tIU522Gqp05oD+trlyqwk6\nWbgrN4IVU2HGm5rs47kp0PoWdaAN/zpMj9qQ/Vy4DKWLg1suGNADalVWgbMsTT6S3MP89Gd1Fmvd\nSDNpTZkLz/aHpx5NLdIi8MXH8N4b0Lm7jkm7u2fvOf89W/MGD/1Ew1D2/wWhETBrmoaFvd0AvALg\n/DHYughWTYM3FkC1ptBnBFTNDSfmQFwYJOyFvxZAg6G6701jYc83YDlr2FCJZtB16u2VLYyLgLmd\nVaDzltJ6wHX+py8EWSE0FNasgBVL1EM+OTlL+87QvTd06AyuuSA6SouHREdBVKTjc3Q0lCmr8cvZ\n/Vv8UxDRv5nocA2Nyyjdanys1tyeP1HjkOt1hIHj9G8IIOK6Onod3qApXGe/B7NE/xdsNjVxv/A9\nFPiPpSc13BLTozZkjcBr8M5k+GkRbPoFWmTghXvqvGYam70c/M5r+NWKqdCxeer1Du6HVg3gkcdg\n6ozsDamJj1Mzo4g61RSvoPP/2gp9nod8BWDxJJjQxuEVC5r4f+A4qNoETq+G+d2gah9o+qrWKo65\nBt72CkkXNsOVPRB9DaIC4OgcKFwT/rfr9nrYa0dBgcoq0M63uBehoXD+bIp2zvH5lK++gBTx0bSZ\n3XpBm/b/7sIR2UF8nArpzuWwa7nmnwb9HUtU1gxX5evqtEI9yFdYt1nzM8yboIlBarfRv52Mip0k\nC/fRLVChrqbANJ7X949kf5jECNjbDvI2hLyNwLsxeFbTF/G7hHEmM9w5sXHwxUwtBhEbD88/rkUx\nCmTBC3XPEZ02rp16fvKDafsWNavebnrPtNj6O3z/ksYfF6/omD9vBTw5BqqUg1U/QAkfdeLx26sx\nq8Xygt8cKFwD2n+i5xl+wSHMGXH1MCREQYmmWsvXSpG5KS0SolWcGzwLRWpB8DXNqBV0VfOOBwXq\nNOXnS/4q1Cnx9oYy5bRVqQade0CjJtl7T3MSoVch+LLG/t6JwIUEwO4VKsz71mgFJTd3TWnZuIem\nojxzQHNHnz4AV887ti1YXKfBl7UwxJPj1dRtyB4SrkPseYi9DPFXwBYLPv0gVzaO3SeEwplxEHcJ\nas+HmHNwbBiE74ZEe5ibsyfUmAE+fbLvuCkwpm/DnWGzQbP+6mjVsy18Ohoql83aPizr5tKU16/D\nwEfgpdfVDNssjZSQd8KSyfD9SHXc8UzxQvHVr5pkpVVDWPKNIwTL2QXcY+DqdDi4W0siluvgOP/M\nijSo2Caz4S2t49vz17STiAQdgz/66vRsOKy4Aps33JwZyt1de8eFfVSIm7XUuPIyKVr+LDi1/ZPx\n2wtLvoaN8yAxXutodxkG7QZqDzczXL0AmxdqO7lT5xUuBe2fVHGu01bFOplmvRyfI0JUsE/vVwGP\nDIWXp0P9jv/eHnF8MAQtBknSnmWh7uBWFGLOQugOnWc52XudTpC/Dbh6Q6w/RJ1wzLfFQVIkFOoK\nzh4QshGCV0NShM5PCIG4K9BwEzjnhtPvgf/Xqc/l1LvQ6qIuvxMkCS5Og9PvQEIwlBimL9buZaHB\nWg0NjPaDsF0q2p5p5Ca4B5getSF9jp+GquX1wTNrKRQrDO2zqf6q/wV4pKsWsPjhV3i4b/rrimgP\n5soZqNtO5wWeA68CjsIS167D3D9h5hI4fQFKX4DiARCcD/zKQ5KTY1+R0VrUYs6nqT3Ot38C619X\nQW7+JtQaCK7ZMDa77wdY86KGLT00D0q3dJzLvh9h9fOawWypDfwSoEIlePRxrZFcuIgKcxEfdaz7\nt4pAZkhM0BraS76GY1s1BWWHwVC2JqydqfHqLq7Q/CHoPAzqtr/ZkhB0Ebb8BpsX6PqgiWeaPwxN\nemhmrn/SPU6KhvA9ELoVYi9A6ZF3R0xi/WFvB4j2dcxruBnyPwCXf4Gjg27epukByFMH/L+FEzcF\n2EOLU+BRAc5+DKffBuc84OIFLvnBrRjUnA25CkL4Xri8Do4ugTPbwBnwcoFKr0KzMXBtPvg8Cq5Z\nfEmNPA6HH4PIQ5CvFZSfALG5IPg4RFzWJDu5vMDVU6c3fvYofPuRF2lgTN+GrHE1WAtcTPsNZn+i\n4Va3IjFRnZUqVYWKlTSEJ/ha+glKjh5WkY6MgDmLoVUaZsLTBzS5/okd2kKvag7hhdc1rGpMO3XG\n8S4JEe5wIhyC3aBCI6gWBddWgncT8HnQ/pafop5wmeJqund2hoQYiI/Unm7wSTg2X/NP34lAJyZC\nWKhaIpJb0CHYMAIi/aH2K5CvGyz/APKsg/PA1gLQbQD0ewIaNPpnicXtEBECRzZrT9bNAwoUVxNy\nweL6OW9Bxz0IDVLHvuVTdPy3WAXo+QJ0HJw6zO3cEVj9E6z7RffvU1YzbzXtCYc2qjgftZcoLV9X\nw59a9nX4LPwTsCWCkwvEXoSDj0DEPhB7tSin3FDnDyjUJfuPub2mmp1rL9KxWkmCXEW0R5sQBlGn\n4fhC8Fuq1dAKVgafdlCsqRbIiTkD2HQ7JzcVZc/K+llsgJX23/ylXVrA5fQKjc1v/ApU6Q3bPoIj\ns9SCVOk6uHhD6Zeh9Evai08LsUFcAMRcgcgYuLoHQiZCeFG4HALhF7N2Xx75Hao+lNW7mS5GqA2Z\nIz5ePbTHfQfRsSpmY0doHee0OOUHs37WnM4BV+Dl1+H9j2D9X9C7E1SsDK3bQ+t2Gm5VsBCcPQMt\n62nd4EUroWaK8eojmx3F338Zq6kTS1ZR03XVptrK19Ge/uRPYftqcA6B/PGQKx4qNYevt2pozJbf\ntFpUeoInAid/13Fhn3rw6B+3d8+io/XF4/ABOLRfneKOHYaYmJvXzQV0BSoCPwPxuaFfLeg6Ftp1\nvjf5yO8X4cFweJM6aR3eqFm3RMDJ2VFRKSUuuTT3dH4fzcyVEKce1L1ehIZdb107OD5We96rfoSD\n6x3zy9ZyiHPJytl+idlO8F9qdo0+pS3yEBR5CKp8AbYE2N9VHZ3ytQDvpip6Lnl028szIN8D4FHx\nlofINNdWqTDnvcF5NDpYox32fA0xIVCsISTGaGpasf+ubnnBp67+nxWtBwUqgVdxey6AdCIZLu20\nC/RKFegmo6Dh87qvZC7vgXWjIGgTlPMCr0hwyQfl3oAyo/V//+xEPfeoU5AQANggGrBnp8UltzqG\nFqpmn9o/5y0FibH6Eh8fqf4mKafxkVC+Y9aGwzLACLUhc3QZDqu3QNeW8PnravZOCxHo+yCs/lMf\nmJ26aZWjTt1UbC76w+KFsHE9bN0IkZG63fZDUL0mfDQOnvgflLKnu4yLgZ9Gw7JvYeQ06Pg/8DsB\nV4IhKAr8A+DCFZ36nYcjfuo93qM1/O8hPd+o6zpOmJkH8NUj8NdIOLdex5I7TYYyrTN3j+LiYO4v\nsGWjCrPvCXv1ICBfPqhdT2tRlymn98bJKXWzLEgM0KIXbTpAnjyZO+4/DRE1LW+cp5aPc4d1vpu7\n5pau1doeP9wYEA1lCr6sLeSGackq0OM5R2GRrHD5FBxYr97XpatlvP69RmwQtgOClkDEAXAvD9W+\n02WbSkHcRXByV8H1rApFHoGiGcRXJ0bAlvJqFq/0MZR69taOjOkRtlPHlounYdYOuwA7P4cD09QR\nslJPaP46lLRHcyTEaK72gP0QeAAC98PVQ7puStwLau74ZOH2Kg4B++DMKl32/wKdzv+JCPguhfWv\nQawvVCoI7nFQY5MmCgr6BpIuQpwN4oHcZaFgUyj9BBSurkJ7O/fmLmCE2pA2IrBknYZKeXrAmq2a\n5KNrK8c6NhucPgV7d2m4z1vvq9h8+J6G+fR/EooVT/8YCQmwfy9s2QAjR9/cEzp/FD56TM2WZTvD\nX0lwPlDNxynx8oDSxbR1fkBjtwvfRtrME4vg9376Zt76A6g/XE2JGZGQALNnwCfj9UWkREkV5dr1\noI5dnEvnzIL095SocPh7Fvw5VcXZzUOtJLXbqDhXbqSx2wY4M0HHb+OvgOUKXrWhQDuo/IkujzgE\nuQpDrqJZ/7uKvQjHnoLgVZC/LdT4WR2kMkvI33DgQXArAc0OaW8d1OFxxydaSxygxuNasKVwjYz3\naUuCED8IO6djwJGXb55GBmjSoKavQoPn0hfoG0lKgH1TtRduCwZ7Aj0KVYOy7aFsB30Zz53JKJX7\ngBFqQ2pEYNnf8O436sn9zdvw3OOpc3WvWQnfTNK0kWH22GJPT9h5VAUpO/h7Dnw5VJ02LlWHHcGa\nGKV5XShlF+VSRXXqnef2RTAhRmOb85VTU93WD6DF2zqWlhFJSbBwLkx8D86ehoZNYOwEjUc2ODi1\nT8V5wxwNbapQT1Oytulv6g4DJEWpCfbaSu0xO7nC6fch8oiaswt1T39s9UZEtORniK8KX4gvBPtC\n5BX9G/9/E251iN0Ivq/q8R44Cy4pTMdiU4G8fspeTe0URAaCzRfct0NibggspyVo4yMhwW7ydfWA\nuk9Bk1fA+w6LwNxI8jCI023GK8eGwsGf1dGrTDvIe49r0N8BJjzLoIjAik0q0HuPQoVSMHOiZgxb\nsVRTd85ZDA0bQ0w0hF7XBCT1G0GDxhqXm1a96NvFoxC4lIblQH4L5k+CR7tkX69UbHBkDmx4U/9x\nh+xRce74Rcbb2myw7A+YMBZOHFMP7PnLoEv3f2+vOSlJ86CfP6IWjgvHAAGv/I6Wp0Dq76f3awIb\n391q1m7dH7o9A5Ub3r/7lBihsbdJkRrygzPkbXB/zif6FFz4Ei5NB1s0uBaAMq+AV3Wo8G7G2yfF\nq/nYf4uahJPFOS5FYh4nF8hfQU3H/ls02U4yzrmgSDkonB+2ToL4cEiYBTGJEBoJUYk6ZpuEinmx\nPFA8BOI9IaweeOaH/F7gavd0zlMCag/S/927we0KdDK58+kLxL8MI9T/BWJi1fPSsuCz6RASBj9/\nAAN7qvBe9IdnBkGJUo6sVb0e0ZbdHNqoReutuvDSRxDgAs/1hw9Gaq85uzi/Ada+CgF7oWgD6PBZ\nxg9qES27uHOb9qAP7ofKVWHmAr0X//SkIQnxEBWqY/mR1yEsCPyPqyifO6yf42N1XcsCn3Ia7hQR\nousnJaa939LV4ZmvI35bzwAAIABJREFU1Xkvs2UYs4ItXuNq3e2WnKBlELJOzbtxFyH+KuAED5zS\n5ceGQuDC1PsoPhSq/x97Zxkd1dWF4edGiJOgCQR3d3cpVqxIgeIUKbRIoUCxtlCKtRRKgRbX4tJi\nxSmlWHB3J4EQEogTm7nfjz35JgkxSIKeZ6277szVM0OY955z9n73/Fcr1oGnwKMCaFbg1lHmel1q\nJj7VEh4k1rH3D8ridVSCs0DmUzMVhhKdJao6Q0FZO+eOfc3wIPC7Ar6XZHl8UR66/L+Xh6lCFuAY\nAelj/Hu6D4Uik+HuFPDbBWW2mAPTFK8dJdTvKg98ZHh78z+wzwNu7BD3rRU/ytN1zMIYn/eQOdg/\nNkL+VIoSjY8/p8P8r0BPD7vdoHQJ2DwbKpSQ/SGPwdY5ccvM5HD1L1jfSiI3W/4BxT8xB4zoOty5\nLcPYt2/CnVvm9Z1bEGhyIcqTF+YshfadEo8wfhMJ8JVUplO7RGBDTMIcHk8kOkAmd8lFLl1P1rlL\niPjGrFes6zKkHfxUlqAnYn+ZwVWKS6S2AOq6BFjd+h6CTsu2eiEmg4z94LUAbHOCTQ5wLhA7h9a9\nN2RsKEJj6QT+ByU/92XaaAiFW+Mg6Dw4lgCnUjKX7FBUeqAxMUbCo/UQ5Q85+4FTWSg0Fdw+kbzg\n+AgLgHsH4O4+WT86Y6737VoWyvaRvPsc1cExmf7fNk6QvaIsMYkKl/9bmibfb/hDiSQPPm96gLCE\nPKaI6bifTfFaSdYctaZpjYEZSKr5Al3XJydwXBtgPVBR1/UTmqblAS4DV02HHNV1vW9i91Jz1Cnk\n9CX4bBwcN0XZ5ssJLevBkG7mClYx+W0GjPgSfp0H3XunXbtW/wxLh8JjZ7iZD8YNlnnxaBG8+hf8\n3QfK9YPa48D3MpxdDIVaig1nQkNiYQFwZw/c+Fu8tSsPluHCU/OgTE9zPnRUFPy5DqZPhgvnzOen\nSycR2nnzQ558si5QCOo1ePvSpR7ckIeh3YtFlItWlZxkRxcZpnZwif3a3hJsL0HGcuBYFKyzvBnD\n+sEX4WJ3MfKwLyi9UdtcIniWdiZLVsuXa6v/UbDNIYuuw/WtcPI3cMkD5fqCa+nYxz89CCdqS8R1\n6A3QI2R7pWPgXFGMRp4eBHTw/E2MQdJXgkpH429fZCjcPyTCfGef+MPrRkkTcq8COWvK4l4l+YFU\nineCFM1Ra5pmCcwGGgCewHFN0zbrun4pznFOwCDAI84lbuq6XualWq5IGoNBfKvdMotrmGtmsNBg\nwiAR6GIFEv9B838KTVtCt15p074bd2HcEHi8ER47QsEesGmE9O5Bgj92DYLzy6QHUfRj2f7wJBz7\nReow22eBAs1EtAs0kV7B8VlwZT14HpIfbhtncDA9iFimg4omJ6SwMMnz/uVH6TEXLgo/zYTiJUWY\ns7u//UPaVzxg/U9weCNYWkO9ztD6q/jTmXQDRPiCjSv47YFTQ+C2aZ9VBukpFvoZXKrIPG/QOf5v\nVBG9dqkhvdXQm+KDnL5s6nyOqCC5rnUm6ckWWwTZujw/VJycKP34MEbC+U9AjwSXwXDsD+nBOuWA\ne/slati9qmQCZHODLI3Feav6VUmPMkaKM1ewqXcNEh198xt57VIbco8H23Lyd/nsKYT7Q9hTCdLy\nPAieR+Q6FlYixtXHQJ568vpFq6Mp3huS7FFrmlYVGKvreiPT+5EAuq5PinPcL8BuYBgwNEaPequu\n6yWS2yDVo04m0ZHbo2dIXnGHD2HV1Je7Vnx1oVPKyYswZQFs2A05AqGsJfywHUrEsDe8fwj+7CDR\nqtVHQY0xsYe9wwIkn/LaJukxR4XBEF8Jaln3EfjfEeHO/6H80FnG6AEHBsKiOTB7OjzylmC4r0ZK\nFam3XZhB/s2ObYUNU8UoxsFZoqxbDhSjkPgIviApO8YIqOQhw6thnhB6BUIuS85syGUoPBOcSopp\nxsUez1+nyjnZf28mXB0ophs5P5dCCZYv4ejmf1QsJI1hYksZPTSb2r17owHO/gTe34AWBd7ZodxE\nSTGKCIJzS+H8r5DpDjgAWncoP0oMOmIS/Ah8zop/u89Z8D0NATfgWUTC99YswK2ciHLuepCzuvwd\nKxQmUhr17Q7cj/HeE6gc5wblgJy6rm/TNG1YnPPzapp2GggExui6/l88DewD9AHIlSuVw/3fRQ6d\ngqE/wdGzUCCXCHS7F7QNnDVNcoBr1U094dJ12H1YBHqfB7jYwfBPYVAXyJrp+ftYO4BdBmizAdwr\nPX89W2co1l4WQ6QExUT/uLVeG/9ctu9jGc5fMFsqS9X9ABaskM/5JgzrpoSgJ3Bmr8w9n9wJj+9D\n1lzQZ7pYZdonMFRqCIPbE+DOFLFaLDzdPHRsl0uWTA2fPy9DXSi/F4hTbMHeZLnp1h7QZMj3Yg+4\n9hW494ECE5P+rkNvwOPNUozBb5cMu+cdCRhN90vFfytDpOT9Hp4oEdOuBSF/OOTwgkwGecCzzQDu\n9hDsAzhCcHG49AccWyKFWbKWFsMOn7MQ4mO+tpO77MvdRCKhbTOYFpcYrzPI3/LLjgQo3ntS/Jej\naZoFMA3oHs/uh0AuXdf9NE0rD/ylaVpxXdcDYx6k6/o8YB5IjzqlbXpnie5lXLopLl3zxkH3j158\nLvXIQRgzTNzB4vPZflEMBvhzj5TAPH1Zind80xauzYG2xcEtRjUjr6NwcwfUGgtuZaDXmeQ5A1la\ny/H/fx9HpL0fwq9TYeHvMtzdorVU5SofJ6DmTSD8mVRcsnUwpznZOjwvTlGR4nN+apcs147L34B9\neik48ekUqNFWIrMTIvSWWE2GXoNsXWVYO7nlAe1ymyOt4yNdVsjVH3J+AU/3i3lH6HXz5wjwkHq+\nz27L64CjkHOAeDz7HxZht80jwp5zgAR8pRZh/hKcdWcfXPsLAu6KoLZZD4VbSerWuY/lQSF7DzjX\nFnw2QsYGYg5imwPqPYSzC+H0PKk3nqW4jN64lpZrZS2VvFx8hSKFpHjoW9M0Z+AmYPKJxA14ArTQ\ndf1EnGvtxzQsntD91NB3PPg+hb7joE5F6N9JAqOiDLErPyWXoCCobgqYOXQ2cQtLXZde2/WTUiSh\npMmxLNBP3kdGwoqtMHkBXL0tpS+H94Q6BWFUfYkannoQ7Kzh1i7x7b28VqKxe56W3nRK8fKU+ecl\n8+R7+bgjDB0laVVvEkajDFHvWy5lFUMDY++3tJIALyeTcKezExORZ0EyElG4MpRrCOUbibuXZZxn\nbF0H4zMpERj5RAw2XKpKScGzbaSqUqYGaf85oyOWQ2/AoYLiaGU02URZOkDJVZClucxtG57JXHlq\nEBEi88J3TEFa3ifNQVo5a0KFAVCwWeyHIWOkzLlb2sowPshDR9wHR12Xa6U0x1ehSISUDn0fBwpq\nmpYX8AI6AB2jd+q6HgD8/xE9phhrmpYFeKLrukHTtHxAQeDWS3+S9xGDATp8BQdPQa3yss3K6uWN\nR0YOhnt3YceBhEV65XgpI3j9JAT6yrYabUWon3hD15zgnB8u6nBZh6LFxaykTUPw9YRh1cFSg4l7\n4OlpmGeq4+uQFcp/DnUmxDbXfxnu3ZUI7uWLRAQ7doMhIyHfG1YJyfMq7F0O+/6QUp12jvJdVm4h\nc6Yx06eCYrwODYS6HUWcS9UB6xAJ3gq9CLc2w7ObUHK1iMrVr+D+LHNEMoCFPdQPEaEsu/XVfd5o\nkbPNDSXXgP8BSWdyrgKOxU3D54hDllUifwORz8DzMNz9R2IYdB0JZtMBk3BGC2iQl4zUvGiQloU1\nYBqNyDUgkc+kmdutULwGkvy113U9StO0/sBOJD1rka7rFzVN+x44oev65kROrwV8r2laJDL51FfX\n9Sep0fD3hm9nwt6jYlDSo3XKrnXoACxbKEFVVaqbt4cESmBSXdPz14ntEB4q5QELlof85aROb1AI\n/LYa7meXQK5s4ZBNg1LuUM4VTsyBzcOhQChUHAbuBSHEBepOgnyNZMgwpQb4N2+IQK9cKj3NLp/K\nEHfuPCm7bmri6wVH/pJyi1ePSTvLNYTuE6HqR7Hzk+Oi61Kg3mej5LRaO8ONb+H2ePMxmjXY5ZUe\ntKUDuFQTgbLKKM5X1hlknRYBWc+11yj5uYZwCfYzhJvfG/NB+rwStW/rmrjYGSIlVenOXukRex6W\na1hYgX1W0xy5qaiJZoGURTS9t80gaXkqSEvxjqK8vt9kNu+Dlv2hV1uY/33Kr2c0wuo/oG0HyR8G\nuH0eJrQB79sw/ypky/d8FHhYOMxaIXPQTwOhYXUY1Qdy20k5SY8lUNwB/C6BpTPkqge1h0tvJjXa\nfOYUbN8CO7aIW5itLXTrDV8Ol+IYrxNdl/zlC//BhQOy9jYNGuUrDfW7it91QpHY0QRfAO9V4L1a\navdq1lDllKQBBRyHwJOSImSfH2xyvp7AJN0oRRnu/2d2zgq8l/zzbZzFtMPBzby2cZZh6nv/io80\nSEnEPPWlR5yzpsonVrwXKK/vtxVvX6hSGmaOfvlrREbCpg0izC1aQ8eu5n27l8LsfjI3OmmviDSY\nRdpolBztUb/A3QdSTnJcf6hY0nyNPCVAOyjRtK3XQZE2Ke/FPXsG+/eKMO/YCg8fSJsqVZV61590\nBbckhC+tiIoUy82LB+HifyLMT71lX/rMUKoqtHMA65sizuVMvsMBHuKSZV8ALEwPSbpBepkBx+FY\nJcACMn0AecdIoQZrkx2nc0VZXjVR4dLLvX9QxNnzkARpgfhK56wJpXvIPLCVLVjayGJlY34NUkAi\nxFsqI0WvvU/L6/BAyFgISnQxpS7VSTsfaYXiLUX1qN90DIaXs7D085MAq/mz4YEXNGgCG/427/9t\nAGyZBaXrwterxAoyJvuPwbCpcOIClCkCU4eJoQrIEOfRn6FUVwkMC/SSwDDrRIZ0k8OxozBtEvyz\nW8Ta0RHqN4LGzaHRh5A5S9LXSE0MUeKRfO0E3DgJ10/ArbMQaQqOypITStQyLTXBzQ3OthC3qmxd\nIJ2ruWTh4RIQclGE2S6/WGG61IQiv0pP1WsRZG0hkdSpQcA9KSeYo9qL974DveDEr3Bqrrn4Q6bC\nJtesGrJ2yZs6w+qGiJRbxioU7wCqR/02oeswaCLUrQytPng5kZ41HcaPFrGr+wFMnyNCF5P8ZaDD\naOg8LvY9Lt+E4T/D1v1SYnLZZKn9bGEhbbv6F+z9CvxvQzoHqPRlykvJ+T6G70ZIYFhWV+jSE5o0\nhxq1wSYV3ZpiztkGnpScYvs43uY3TsGeZXDtmKRQRftj2znJfH2LAVCgvNhzusZIXTKEwvHqYn9Z\ncjW4tYt93RLLRaijTUUi/SS4CmSuNUcqOMMZIuD6Fji9AG7tBHRwcBVDj5JdZEg5MXF9dBY8foaL\nq+ThoXBrOTdnDXBIo4ckJdIKRZIooX7TmL8OZq6ADOlFqJPLP3ugRCnIkhXy5oP2naHvQCgWwxTu\n0EapoFSngxhkxMTTG36YAws2gIMdTB4CAztL1S0Q7+1dA+H2Hskn7bRXhipTgsEAC+fAD2MgOFjm\nnId/Iz3p1CbgGFzqAyWWgFMZyeF9+q/k+WZuC3fsYNtquHwE0tlKCtSHfaFgBRHo7AUTN4axtIcs\nLaDAJMgcj/lM+rKpZ7UZF98rku97bqkMMzvlgBrfyL/TpdVwYhYcmy5e6CW7QPFO5ocrXZe8do+f\nJZDL2kEi8yt9Kb1mhULx2lFD328SJy5A9U7Sm972e/J706uWw2ddZf528NfP7/e6Dmsnw65FMtQ9\naa+5Z3XyIkxbAmt3yvt+7eHbzyFznBznbb3h8jqoPR7K90t5MJPHERj6hQSH1akv/tuFi6bsmvFh\nCINb38GdqWCTHUquhAw1xTrz5ly4sxDSPQQduOEM2cZCg+7JL9cYclWsL51KJ31sahIZKv8ep+fL\n3LGFFRRsAWV6Qb6GsXN+Q/0kf/38MkljQoO89SF3Xald/PgiOGaHigOlWlNq5LcrFIoXIrGhbyXU\nbwp+/lC+rfRwTm2ATMkUij07oV0zqFYT1m0Duxhey3cvwvJv4fCf4uzVoj90nyRmGVv3w7Sl8O9x\ncHKQyPKBnSFPjJ7WxVWQqQhkKyc/9rpBcqFTwmMfGeb+Y7EUxJg0HT5qmzZpRP5H4VIPGW527wUF\np0ru7vl/YfMsSaHSjVCrDtTIIQKXvTNE+kvxhqytIOtHCc8bB56CU43kAaDK6ZSnniUHowHOLYH9\noyHkkQRilekFJbuCYzLMQ55ch/N/wIXlMn2RtTRU+UpsWtUwtELx2lBz1G8DK7fCw8dw8I/ki/TJ\n49CljRiOrPhTRNpolBxoO0cI9oez/0C7kTK3apMeFmyE6Uvh+l3IlQ1+Hg4924BzjBSYR+dg1wCx\nYCzTC5rOT9oqUdclT/v2TQgNhWehMkcecx0SAnt3yDD34K9h2Ji0GeaOxvdvcegqtxNsKsHfS2Hb\nHLh/GZwyQushUsjCLc4Q77Pb4qx1+TO43A8y1AbXNlJuMbru8dMDcKa5VJwqtf7ViPS9/2D3l+B9\nSoLEWq2GXLVf7CEnY0EpI1prLATel2DAt90DXaF4x1E96jeJq7ehcDLnBQ0GqFwCwsOkEEbGDOKA\ntfFnKFUXBvwux4WFisHGzD9g7Gx4EiDpVV91hzYNYjuchfnDge/gxGwpIlBnktR0Tsw60WCQ9K/p\nk2UYOy42NmBnLw8RdvZQpJgM0afFMDdIbWDdABlri2HGtUOw/Q84sFoCwwpXkrnn2h3AJpFKT7ou\n5QwfrQefddIrr3ZVfKp9/oTzHcEuD5TbLb7QaUnAXdg7XIavnXJA/R+hWAclsArFO4TqUb+pGAzw\n9c/QtSWUKpx8kQaZv16xUUTUGAq9Kosvd4FyUDZGEJqtvQSJffOrGJV82w+qlY3/R/70fDg+E8r1\nlbnoxHrR4eGwapn4bN+6AQULw2+LoFY9sLcXUba1fbmo9Zfh2R24PgIerYH0VeFJd/h7rkRx2zpA\nvS7w4Wfy/SQHTQOnUrIU+B5CroODqdyh91pwKA7ltkO6NEwZiwiBw5PBYyqgQc3voOrwlKfBKRSK\ntwol1K+LyEjoPgpWboOMziLUySEwEDashu69pVf69BF8VR3CQmDCLhHpmCIcLdJdWsDiCc8Lp9dR\ncYTK+wFUHAB5G8SuUhXf/RfPlTrP3g+hbAX4YwM0bfnqRDkmUYFw8we4N0N6wY9KwfzzEPQZ5CkJ\nX/wGdTuBQwq9xaNFGkwVqLK+eEBdZKikQD08CU+uSdyApY3ZMCTaNMTKVoxAjkwRH+vin0C9KTJM\nrVAo3juUUL8OwsKh3RDY8g9MGgwjeifvvPBw6Nwa/tsPlatJ6lVEmJQ9HPYHFI1j2TkhEZEOegj7\nR0pKj3sVsWy0sk1YpIODYfoUmD/LXOd53nKoXe/VDsHqutidXvWAKx4Q9DeUuQ6XgIOAlTdUaiXD\n20Wrpk3bbLMnfUxMUfY2LY8vybA8iB+1rps8saPiv0a2ClJ3O0e11Gu7QqF461BC/aoJDhH/7n0e\nMPsb+PyT5J1nNELf7mKtOWcpFCwk21xzw68nns/xnTAHxvwKnZvHFmlDBBybAQfHi0hUGwHVRiUu\naPv3woBeUrHqddV59vWSoexdC8H+ATgCt+xkKNurCpRtBh0qQ9Zcaf/gYIiEIE/p7Sa0BNwzi7JD\nVnArDwVbQrbyIsBO7uZ2Gg3mohbRhS2MUZAh/6sJUlMoFG80SqhfNdbWUkd6+WTo3CJ55+g6jBgs\nQ97jJkP7TjC5A1jbwLDliYv0komxe9LXt8K+4VCgGTSYJlHACREYCN8Mg8XzIH9BKY1ZtcaLf+aX\nRdfh3H7YMltSqTQDdHKVoqrpCkO1s/IdvEq8PGB9awh+EHu7lZ2Ir5M7uFcVU5H4RDk+LCylLKWa\ne1YoFPGghPpV8chXIqwzucDW35Pu9UVFibWmWzZ44gdzZ0K/QTBoGMwZKFWrev/8/HUmzhWR7tTM\nLNKGCPA5L8JRuBV0/U9sIRNjz04Y2Ft8wgcOhdHfx87RTktCg2DfchHoe5fMqVQVosBnOuQfD7mH\nmos+vCourIStn4rwNl0gEdjR4mzroqKwFQpFmqCE+lVw7wF80BPcXWHf4oR/0IOCYO9O2LYJdm6D\nMuVg8x7IlBnWbIGGTWD1BBGwtsNEvGIycS6MniEivXSSiHRUGGz4WOwh+10X68jERNrfH0Z/Jb7b\nhYrArkNQKRXKVSYHz2uw+Vfx2n4WJNadgxdJKlXkHThaBlw7QL4xr6Y90ehG+Pc7OPQD5KoFbTao\nCk8KheKVoYQ6rQkKgXo9wNdfergJifQ3w+H3GRARARkySlGKFq3N+xs3he3zxWnsg27w6RTzPl03\n96Q7NjWLdEQIrP8Ibu+FJr8nXTxj+1b48jPweQRDRsCI7yTFKq25eAg2TIWjmyQSunZ7aPaF5DxH\nf1++R8VspMiMtG9PTCJCYEs3uLIBSveEJr8pBy+FQvFKUUKd1gyeDLe9YP8SyV9OiMJFoU9/SXOq\nXC22EUk07gWhzicwaH6MKlDB0Oc7WLNdRHrZZBHp8EBY00x8oJsvkZKUCREVBSOHyPB6sRKwahOU\nizfv/uUJOgO2ecHSDnx3QKamIswbpkohDKeMUs2ref/nS24CuPeQilSWDqnbrsQI9IS1LeDRGfhg\nmhSqUMPbCoXiFaOEOi3Z8g8s3AAjekHNeIRP12HXdhnS7twj4esYosSfu1QdWaI5eRHaD4E7DyTN\na3hPc2DZqTngdURsJot+nPC1/f2hWzupAf3FYBg7KXVLS4IUwDjVCNJXAsfKcOcbOJQVPHzALR98\nPksKYdjGI8JhnhB8QSpSvUqR9joG61pCZAi02wIFm766eysUiteHrgP6G5VxoYQ6LSlZCD5rB2P7\nx79//m8wtD+s2SxD3fFhMMBXNaBqS2g/UrbpOsxaAUN/gqyZpLdeo3zs86oMhdz1IHsiPeMb16F9\nc7hzC2YvhC6fvvBHTBJjOJxrK/Wa7xSBtdOhIVDlMdSdCDWGJ2yUouvitf1kH9S8k7YuYDHveWm1\nBI05uEHH3ZC1RNLnKRSKl0c3QIQPhHmBHg62ucEmG2gvaKJkjILIx4CFPNhb2iV8DUMYPLshFfBC\nr8Ze65GQvQfkHBjb7Og1oYQ6LYj2T8/jDnPGxn/M5YswZig0/BAaN0v4WtvnibnHR4Pk/dMA6PkN\n/LkHmtWRee/oIh5BD2FbT2gyB5xzJS7S/+6DLm2lB755D1Sv9aKfMnlcGQABHuCRBw5NhQpNoFw/\nCBwIxpkQ2RUsE5g7f7QWfLdCoWlpJ9K6LlWk7v4Dd/bJOvihBNy12QgOr+DhQKF4F9B1CPcSt0Bj\nuJR/NYbFeR0GEY/luDAvCPeU1+EPQY9j/KNZg21OEW27POZ1uqwi6v+/RvTyAMK9AWPs61jYmkTb\nXtYW9hDlL7bDMY+1yQ72hcHtEzAEgec8uD8LMjeFXF9CxvqvbepLFeVIC5b+JXPGq6bGrkoVTXg4\n1K0kFpxHz0PWBMoT+vtA78KQv6zUkD52Dtp/BV4+MGUIDO5m/sMJuAcr6ovIfLIj8cjuRXOlJ1+g\nkEST582X8s8cH7fnwI1+cAy4nB36zYLqrWRf8AU4VhXSV4QK+54/N8IPDheV/5iVjrz4k3VC6LoY\nksQU5oC7ss/BVWo05/0ASnQGq1ec/qVQvE0YQiHwBPgfgYDDso58nLxzLR2lmI2NO9jkAFt382sL\nawi7J0Iadte8Dn/w/HWsXOS8/5/vLj1xkMp5hlDTOkRqIkS/tnQEhyIizA6Fwb4QWMX5rQ73Bs85\n4Pm7PBg4FIdcgyBbZ+mppzKqKMer5K4XDJwIpQuDYwIGFuNGwYVzsHZrwiINsGgEPAuGfjNh2hIY\nMR1yuMJ/S6CKqbjExdVwYQV4HRaHq057xBI0PqKiYNRXMOdXaNAEFq+G9Cn0wE6Io5th0feQF8jQ\nD+ZOAgdn837HElB6g/zHio9rX0HUUyi2xyzS3mfg9DwID5DocAvTYmktkdjR7w3hUgks3F/WYU9j\nvzdEyPXsMkKuOlBlmAh05qIqWEyhAHmgNYZJzzIqSHrJhiDpwQYcFWEOOmPuBdsXhMwfgnMlsM4M\nFjbSk7WwldeWtqDZyOt0maUu/ItiDIew+yKa6bJKD9gyDU2CbNwg/1jIOxK8V8O9X+ByH7gxEtz7\nQM7+ybMTTgWUUKcmRiP0GCPr6BSp+KhdH+wdJOUqIXy9YP9KaDEIxvwOt9bBgIxQ+BHsrwalPCWX\nN+COLPkaS2Ul19LxXy8gALq3lzztLwbDDz+lTRENvwcwry/8twVyl4CPNj7vQR5Npoay1nUZHneJ\ncZxLDXnidSwJt/fAkR/h9m6wdgBHN7HxNJoWQ5y1hTXYZQAbFzEisc0ALnnN7x2zSR1n11JvVMCI\nQpGq6EYZUg67C8/uSi81yt/Uqww29y5jLSZhNgSZLXDjYmEvgpx7GLhUA+cqIr5pjYUN2BeQ5VVi\nYQPZu0G2ruD/nwj2ncngUvWVCbUa+k5Npi+FIVNgwXjo2eb5/br+Yj2288dh9Ago8S84GMAxO2Qt\nCVlLQeWvwDGR3ng0j7xh4RxYNEcczqb/Dt16Jb8NycFggDvn4eQOWDcRmgZDlmLQ6DRYWSd9/oPl\ncLErlFwFbh1kmzEKLq8TgX50RgK7Kn0J5T4TsU2I6L9n1TNWvE88uwsPl8OzmyZRviu9Tz0y9nGa\ntWm+NubiGPu1lRNYOsnaKr35taWTxIo4FH/xynHvGs9uy5x5Kj7oq6HvV0FEBMxeCc3rwqetn9+v\n69CjA5QuB4O/Tvxajz3haRi0/Q68vaFeRfhkTsK95fg4fVIMVDaukbY1agrDxqSOy1hUJFw/CRcO\nyHLxIIQEyL52OcE9CEqMTp5Ig+RHe82Hiz3g0SYI0uGMh4wUZCoCTRdCiU7JmzNWAq14m9AN8Hgz\neM4HaxfIOUAgpd2zAAAgAElEQVR6qMn9Ow7zhNsTwWuBDEPbZBMBca4Mru1MAVi5ZW2bC6wc0/bz\nvC/Y5X2lt1NCnVqkSwfH1oDB+Px/srBQWLUcNq6FcklUnXp4BSaVAisN/MrCtmXPp14lRFQUbP1L\nBPrIQXB0hO594LMBUm0rJYQEwpZZcHafGJSEh8r2nEWgVnsoXhRcfeHhBMg1BLIlsyoYyNBS6Q1w\nsAT4rIZHgFN1aDgDCjZTw9OKd4/IJ+C1EO7Plt6vbU6ZB/ZeBekrSFqQWzv5vxEfYQ9k+NVzLqCD\ne0/IO0quo3jnUEKdGuw9ArUqQMZ4hmTDQqFnIXj4AGpVhP5Dnj8GpMd9YQX82ROyRsI1N/hvMRQt\nmvT9dR3mzIRZP8P9e5AnL0ycJnnRzs5Jn58Ux7bB733A/QHkyQyVsoGTJViHQs2zYJEOrvSXH52M\n9aDglKSvGbf9x+bCaR/I6wo1l0Huhilvt0LxphF8Ee7NhIfLwPgMMtSBwtMhc3MJ3nq4HO7/KlNB\n14dBjr6Q4zNzJHO4N9yZItHIepTk+uYdLb1mxTuLEuqUYDTKvPTwn+G7z+Hbz2V7VJT0qi0tYXx3\neOIFRkuYtkjyluPOVT+5Adt6wb1/IRS4lA8WHocsGZNug67D6KEwaxrUqA0//ip52akRKBbgC3O/\nhH9WQKMsUBywMoCNrfxw2GSTHxeLdJDjCymY4Vz5xeavosJga0+4uFKGt5suAKtX4C+uUKQ1hjBJ\nV4p4DKE3wGsePNkrkdDZOsswt1Mp8/EWjpCzn4jzkz1w71e49b0Mbbu2g3SukipkDIfsXSHvN2Cf\nRqmVijcKJdQvi58/dBsJ2/6FptWhiIsU1jjhAadPwLb9kD83nN8K1m7wzVooVEICr0bUhYofQqsh\nYJ1O5o2uPIabVhBoCyuPg0syRBpg4lgR6c8GwI8zUmeOVtfhwFqY3x8s/KHTd9B2MIRflWjP+HBM\nRs8/LsHesO4jeOABdSZAtZFqjlnxdhH2APy2Sw5xhI9JmH1EnA1BsY+1yQEFJoF7b0iXKeFrahpk\naiBL6A0x3fBaJFHZ2TqJQL8BblmKV4cS6pfh+HloPQh8/GBkZ4nyPrBA5qlLlYVuvSG9M6yeKDm7\nc/dDjsJy7rMgcMoEi0fCniXQ5xf4aRf8/Qxqp4PRy5Iv0r/8CFO+lyHuKb+kjsj5PYBZ/eD+Zmhr\nAy7ZoNZoSXmyTUCkX4ZHZ2Ftc3jmJ2Uji8QTgKdQvGkYoyDgCPhuB9+/IfisbLfOLJ4A6bKCcz6J\njk6XFayzmF67irnPi0ZL2xeAwr9IDXZDiOT2Kt47lFC/DBnSQ9aMsGkWFMgBjhEy7Fy5WuyCFllG\nQ/HqZpEGcHSBb/+Ued8V3WFdE7jlBCMmwJfdwD4eJ7P4mDcbvv0a2nSAX+eZi3G8LLoOuxbBoiFQ\nIQTaaWCXHYovFpFOTa5thr86SppVl/8gW7nUvb7i/cQYKYFZodelJxp6Q7ycw+7JfO7/U1HjrjGl\nHmUWwbXObHqdyfw67J6Is98uyUXWrMClusRjZGoiBj5pORpk5fS8c5bivUHlUSeXR76waCOM6C3/\nIZPKiU5q/6OzML8CeGngnRnW3pFh8OSwYgn06wEftoDl68E6hUL6xBumfwqXt0NnO3B4Bjk+lx+h\n1Ezn0HU4+hPsGwHZKsDHm8ApW+pdX/F2EflUhM8uDziVf7Hepm6AgOMy7BzgIaIcdie2SYelo/RI\nbfNIHAUAWux19P9lQyBE+kGEL0T6QlTA8/dMl03ctzI3gYwfgHUqBGoqFCZSnEetaVpjYAZgCSzQ\ndX1yAse1AdYDFXVdP2HaNhLoCRiAgbqu73zxj/Ca2XcUOg0H/yBoWQ+KmZxxBvSG5q2lTGVMLvwH\nC4bCiNXgFk++XXgQLG4CQcCzz2DttOSL7ca18EVPqNsAlqxJuUh7bBWRfhYEvX6FPEclkjTTBym7\nbkye3oRrm+Dyeim9Waw9NFsM1qnvl6t4wzFGwZPd8GAJPN4kgVEgPdoMNSFDXchYF5zKPO/vHuED\nvjtFnP12ibBiAU6lZVjZ7ROzc5VdARl6ftlerjFSUqgifWW+2TqjuOSpGArFayBJodY0zRKYDTQA\nPIHjmqZt1nX9UpzjnIBBgEeMbcWADki8cHZgj6ZphXQ9IW+6N5At/0DL/lA4L+xaYBbpfbth6QIo\nWSb28boOC4aJBahLPM5hug7zW0LkQ7jTDP54AZHevgV6dZIh9pV/gm0KoqPDQuVhYtvv0CA7tNwB\n+WsDA17+mtHoOnifgqt/wbW/4PEF2Z61NDT4BSoOVD947xvBl+DhUnGhi3gow8rufcCtvfhHP/0H\nnvwj874gxRYy1JL0pSh/GXYOPAHoMu+b+UMZcs7UMPHArJfFwhpsXGVRKF4zyelRVwJu6Lp+C0DT\ntNVAS+BSnOPGA1OAYTG2tQRW67oeDtzWNO2G6XpHUtrwV8bMFZA7OxxfA44Osk3X4ftRkDsPdO8d\n+/j/1klZysGLwDYew/j9++DWYfApAQtXJ1+k/9kDXT+WB4N128DB4eU/043T8GNHuH8FepWD9KeA\nf4HaL39NgPuH4OIq6T0HeYpRSa5a0GA6FGopftuK9wNdh2e3wG8nPFgKgcekh5y5KWTvLmuLGFM9\nbu1kHfYAnu43C/fjzYCFpP3lHyfinL6cMsFRvFckR6jdgfsx3nsClWMeoGlaOSCnruvbNE0bFufc\no3HOfa5ckqZpfYA+ALly5Upey18Foc/gzBXo87FZpAG2/Gmy6Fwskd7RRIRLNHeeklC/6/PXu3Ad\nWo8G99qwb0XsaybG0UPwSUspS/nnzpeveGU0wsafYelocMkMIxtB+E6ptZrvm5e7Jsh8+74RcGsH\nWNlBvkZQ5wco0FQKhyjefQwhEHBCIqIDjkiFpQgf2edYUmqKu3VMuodqmx2ydZQFpLdtaSdDzwrF\ne0qKo741TbMApgHdX/Yauq7PA+aBBJOltE2phr0deO6DsAjzNoMBfvgGChWB9p1jH797MXjfgh92\nPG84cvM6fNMAHHPAtkWQNZkCdu4MfNwUsrnDpt2Q8SV/sB7fh5+7iwVojVbQ2ABPNkO+72R5maFo\n/zvw7zfiqGbrAvWnQvl+YJ2GpecUyedFi8CA1O8NuSz1wkMugeGZ9IQ1q3jWVuI1HXAEgs+ZA7ns\nC0KmxuBcVeadHYq9/FSHbQJlUBWK94jkCLUXENNANodpWzROQAlgvyb/Gd2AzZqmtUjGuW82ui49\n5nRxorEHDYcsWcEqztfXoAc4ZYTyjWJv930KE+tCOS8YNAlyJ/PH5/o1aNUInNLD5j2J165OiMee\nsP5H2DEfLCzhy4VQ9yM4WVt6ObkHv/g1Q33h0AQ4+ZsMQVb7Gqp+nXhVK8WrI/gCXBsqLljRdXtt\n3OOss4N1Bgi9CSEXxNoy+IIMV0enLWnppKKSHiUirEeZ6g8bzfeydBQTnDwjRJidq6TNnLFC8R6T\nZHqWpmlWwDWgPiKyx4GOuq5fTOD4/cBQXddPaJpWHFiJzEtnB/YCBRMLJntj0rNu3YeGvWDxBKgZ\nb8R8bIzG+HOZjUboVB1KHIVcfaDL3OTd3/M+NKwOYWGw8z8oWDjpc2Licw/WToadC6Uubf2u0G4Q\nZCssRv+GMCnm/iJEhMCxX+DojxARDKV6QK2xkD7Hi11HkTZE+MCNb6USmZWz2ExGBUG4F4Q/kHXk\nk+fP0yzBvjA4FgeHEpIT7FgC7PLFnzKl62bhtrB+PjpboVC8MClKz9J1PUrTtP7ATiQ9a5Gu6xc1\nTfseOKHr+uZEzr2oadpaJPAsCvjirYn4XrUNbt6XQLL/b1sOfr7Qb2Dsoe1Hd2BUAxi8GErUiH2d\nhT9DYQ9IVwI6/Za8ez/2gZYNIDBArEhfRKS9b8OaSeJ6BtDwU2g3AjI4wqnG4FcASq1+MZH2uwrn\nlsLZRRDySALD6kyUmtOK148hTAo53J4gQ9c5+0P+7+Kf1zWEmUT7gaQ32eUDh0IJV2mKD02TYW/l\nl6RQvBKU4Ul86DoUbw6ZM8CB5bItNBTKFIC8+WHHgdhzblM6weGNsOA6ZInRuwwMgrFuYAeMvAWO\nyRi6DgiAZnXh2hX4axdUrZH0OQAPbohl6d5lMsTdpDd8/DVkyCCVdu5OFROHUushS9OkrxcWAJfW\nwLklkvusWUC+xlB9FOSsnrw2KdIWXQef9XD9aylkn7k5FPoJHF5w9EWhULx2Umx48t5x7ipcvgW/\nf2veNn82eD+Exatji/Tt87B/JbQfGVukASbMg2VusObH5Il0aCi0awaXLsDqzckX6f2rYFoPaVfz\n/tB2GGR2hyf74WAbGe7M2AAKTATnRIbxjQa4s1fE+eqfUtkqczGo/xMU76RcxF43ulHMN8IfiAvX\n3WngfxAcS0G53alrUqNQKN4YlFDHx8ptEijW1hQUFhgI0yZD/UZQvVbsYzfNABs7aDPUvE3X4cBi\nmL4EOrWH+m2SvmdEBHRuAx6H5WGgQeOkz9F16UUvGwMlasLINeCYDiIeAe4yz5ihLuQeCi5VEr6O\n/x04s1AEOsgTbDNA6Z5QqjtkK6/MSV41z27D4y0S2BXmZZpj9oLwh6BHmo9LlxWKzgP3T9U8sULx\nDqOEOj7qVwEXJxn6Bikj+fQJfDsh9nEBvlKr+YNuEu0djcc0ODgUSuaDicmIqjYYoE8X2LMDZs6H\nVh8nfU5kBMz8DHYvgXqdod8P8GA6nPkNHIpDZQ8pJlB6fQL3jITrm+H0PLi1W7blbyzmJAWbg9UL\nzFkqUk7oLXi0Tpagk7LN0lGitG3dIUPt2JHbtqYHMcsUGN8oFIq3AiXU8dGwuizR1Korvdey5WMf\n55QRRq2DnEXM227ugL3D4YKTBHFly5L4vQwG8e7euBYmTIVuvZJuX9BTmNAGzv4DncdCq67gUQai\nAsGtA+QdlfC5T27AmQVwbjGE+IBTDqj5LZT+FJzfILOZtxldB2OYBGgl5qAVeiOGOJ+WbekrQsEf\nwbUt2CknN4VCoYT6ef7+FwrlgQK5zdtq1JYlLhYWULmZ+b3fVfizAzxxhDMVYUm3xO8VGQm9u8DG\nNTBqHAz4Kun2PbwF3zWFhzdh2HKo2wlONZBUmapnpZcVH7f3wuGJcGefDJMWbAZleksv2kINm6Ya\nQWfgUm+TLzVgYQeW9qbFASxMr6MCIPi8HONcGQpONYlz7oSvrVAo3kuUUMckIgK6jIBGNWDlTxKB\nPX0y9BsErnEKth/ZJJ7eHcaIp7cxCta3hnAjLHKFZaPBJpGyleHh0KMDbP0Lvp8CXw5Pun2Xj8C4\nlhL0NXEPlKwl1YfsCoBru4RF+txS2Pqp9J5rj5f85/TK8SlVMYTBre/h7o9ScCLfWMAo6VKGUDCG\nxn6dzhUKdYesbcFOjWQoFIqEUUIdk91H4EkAdDSlLy2eK0FkLds+L9RrJkHwE+j6g7y3sIIKY6Hb\nt1C5JjSrk/B9QkMlcGzPDvhpJnzWP+m2HVgLU7tClpwwbhvkKGS6rw0Um5PwecdmwO4vIU99+Pgv\nSJeK9aUVwtMD0osOvSYFJwr9rLypFQpFqqFK0MRk5VbI6AwNq4kj2OzpUPeD5+emLx+V3nSLgTL8\n7XdVts+9AFetYfrXCUdKBweLd/fenTBrQfJEetdimNQeClaAaUdEpHUjXO4HAcfjP0fX4cBYEenC\nraD9NiXSqU1UoPwbnKgNxggotwuKL1YirVAoUhXVo44mJBT+2gddmou395L58Mgb5v/x/LGbZoB9\neon29vKApdWgwjxYsB4GdYGi+eO/R0AAtGkCJ4/Jddt1TLpdx7bBjN5QtgGM3QzpTI5innPFyMSp\nHDhXjH2OboTdg+H4r5Ji1XR+/FaQipfn8RYR6fCHkGswFBivIrAVCkWaoH69ozlxUYK7OjaTSOxf\nfpSedO16sY/z9YKD66HFALB3gt0zwdoBJu6SdK7vPo//+n5+0LoRXDgHS9ZAy2TkVl89BhPbQb7S\nMGaDWaSf3YHrw8TExD1OlLgxCrb2hPPLoNKX8MHPqnZvTIwRUoAi0s+0PDEtMV5H+SOFKSxMIyMx\n1xYQFQwBhyUmoPRGKUqhUCgUaYQS6mhqVwTvA+CSXgxOylcSMY07hB0ZDtVaiQNYiA9cXgcODeHf\n8zBvnJwfF59H4t194xqs+BMaJ8PC88EN+LYpZHCVOWl7J9muG+FST8ACii2I3b6oMIk6v7ZJgsaq\nj1ZmJSDTAAEe8HA5PFodf2EKSwcZsrbKCNYuyKyQUc4l0lSIQuf/laPy/wB5hoFFIgGDCoVCkQoo\noQZz3d6MpjKNLi6wcEX8x2bLB6PWyutDE8EQAfN8oGxR+LT188cHB0Pz+nD3NqzbBnXqJ92ep49g\ntMkVbfwOyBgjkM17DTzZJ45UMaOFw4Ng/UeSftVwJlRMxtz3u86z2/DwDxHo0OuSKpX1I8jyEdhk\nE2G2ziTlHl+kKIVCoVC8QpRQA8xfB8s2w5bZ4HVXRLt4yeePu3BQTE5yFxNxP7sYLIrChSDYPyJ2\nRS2QY/r3gquXYeOO5In0s2DJk376ECb/Y47ujsa1reRMZ+ts3hbyGNY0Be9T0GI5lOzMe0tUoDzM\nPFwO/v/Jtgx1Ic9IcG0DVvGMeCgUCsUbjBJqgBVbwc9fhq27D4XLF+DSPfH7jkbXYXY/CcqadUrE\n/OPdUKM1fFBWhs7jMneWmJl8NxHqNUi6HVGRMKEt3DwD3/4FRSrHuL9RRMjaBbJ3MW9/ehNWNRaP\n7rZ/QqHmL/89vO0EX4LTjSHsPjgUkSIkbp1UnrJCoXirUVFG9x/CgROSO332NPyzGz7/MrZIg9h1\n3rkALQea530X74VrYTAunmFmjyMwagg0aQ6Dv066Hbou0d0nd8KAObEdzwA8f4fDReHZPfO2B8dh\nSVUIewqd9r3fIu1/CI7XAGMkVDgAVS9B3pFKpBUKxVuP6lGv2S7rTz6EH4ZD+vTQ47Pnj9s0A5yz\nQJ1PJG9686ewMAQa14BqZWMf+9gHun0MOXLCnKWSa50US8fAnqXi3d04TiR3wDGpOexSA2xzyrYb\nf8PGj8E+K3yyAzK9xzWIfTbB+Q7y3ZTbqTyyFQrFO4XqUW/YDRVKABHw13ro9Tk4O8c+5sFN8NgC\nH34mKVInf4MHHuAV+nxv2mCAnh3BzxeWb4AMGRK/v67D2imwZiI07g0dY9TA1g1wawIcry6BT8Xm\nS2/+zCJY2wIyFobuR95vkfacB2dbS03mioeUSCsUincO1aPu3BwypIcLZ8Elg/h6x+XmKXBwhqb9\nICIYzi6Byy4SHFapVOxjJ3wH+/fC7IVQuuzz14pJWCjM6AX7V0GtdtD/t9jpVPdnw80x4Noeiv4O\nVi7w3/dw4DvI1xBarwcbpxR/BW8lui7e2rfGQuYPodRaZTiiUCjeSTRd1193G2JRoUIF/cSJE6/n\n5uHhYJNAmk74M7Cxg1NzYXtfmJsb/twG5Yubj9m+Fdo3hy6filAnxsNbML4V3DkP3SZISUxNEwGK\n9IV0WaTQg992SSfSDbD9czgzH0p2haYLwNI69T77m0BUoKSe2ReUYDAtgapexii48gV4zRNv7aLz\nwOId+y4UCsV7haZpJ3VdrxDfvve7R33qEmTPAs+CIE/e+EX67kXIVUxEWtfh2Ex4ZA9lP4wt0ndu\nw2ddoFQZmDor8fue3AWTOwA6fP83VGgs2yOfwOW+4t9d9RxYOUHWVhAZChvbw42tYmJSe/y7ZWSi\nG+HBErgxEiJ8ZJuFPTiVgfQVIH15WRyKiLPY+U/g8Sapu53/h3fru1AoFIo4vN9C3X0UZHSEG1ug\nZz8YNzn2/nuXoX856DJOery6AXyKwR5/WBFjbjosDLq2ldfLN4CdXfz303VYNwWWjILcJeCbPyG7\nyRfcby9c7CZClX+81CwGCPOHNc3A6wg0/h3K903d7+B1438IrgyCoJPgXA2KL5PvIPCEbPNaAPd/\nlWMt7GWuPtwLCs+EXMrURaFQvPu8v0Lt4wfnr0GLomIZ2qpd7P0GA0z/FOwcoUEP2RYQAj/fgfqt\noWwx87HDB8KZU7BmM+TNF//9QoPkegfXQ5220HcSOOeX3uSpxvBkt/QYy2yG9OXknOBHsKoR+F6C\nVmugaNtU/xpeG2GecH04eK8CG3cosRLcOph7x9G54roBQq5A4ElZQq9C4RngGo8LnEKhULyDvL9C\n/c8xQIdrJ6BmHShTLvb+TTPgylEY9of4bQc9hBlfwrNAGPtFjOvskUpbQ0ZIznR83FgPWz4Hl8cw\nKAtYboSbgZJKpFmIiUmekZBvjLknHXAXVnwAwQ+kRGW+ZBimvA0YnsHdqXB7MmCAvN9A3q8TDgTT\nLMGxuCzZu77SpioUCsWbwPsr1PuOgpMleN2DYaNi7/O6DktHQ+XmUNdUivLwr2C9Ftp1h1KmdChd\nh/FjJF965Nj47xPoB8c+gRJRYJEdMlcGx5LgXMV8TKm1sc/xvQwrG0BkCHTcAzmqpsYnTlt0XUpv\nBp81bdBir6N7yo+3QthdyNoWCv0EdnlecUMVCoXi7eI9FmoPyGMPdzVo9lHsfX4PwDWvOIRpGhgi\n4dhvcMMBRsYQ9R3b4IQHzJz/fCCabgBjOCwYBkeM8MM+KFw36XY9OAGrG4tVaed/wbVU0ue8CdyZ\nBDdGyxwy0dHaumnBvLbLD8WXQMY6r7qFCoVC8Vby/gr17oXg90TsN7O6xt5XqjbMuWB2FDvxB1gE\ngs1HULygbDMaYcI3kDc/dOwW+3xDCJzvCP6PYLeHBKIlR6Tv7hcjE7tM0HE3ZCyQ4o/5SvBaKCKd\nrTMUX6rqXysUCkUq8v4KdR53WWLy8Bb8uxraDgOrGHm5O36AQGsY/KN52+aNcO4MzFsO1jGODX8I\np5tD0Gk4kQHcC8Z2G0uIa1vEEjRDfvhkF6R3T/qcN4HHW+BSH8jUCIotUiKtUCgUqcz7+av62yoY\nOAQmfy89YzAVxegFaydLPehovO7BI2+IrAZFTb1pgwEmfAuFi8LHn5iPDb4Ax6pAyGV41BwO+MGg\nBZKDnRjnlsH6VpC1FHQ58PaItP9hONdOcpxLrVemIwqFQpEGvH89al2HH+aA5VUokBtGmHq72+dJ\nhawBcyFLDvPxS/6GOXngcgynsXWrpMb0snXmGtS6QTynjZGQdQ5M6y7e4CVrJd6WA9/BwfGQp76U\nqXxbLEGDL8HpZlIIo+w2sHJ83S1SKBSKd5L3T6iv3gbvB+D4CFoMlm0+9yToq0x9aNLbfOzTm7Bz\nB1QqCYVNxiSRkTBprDiQtYiRy6tZQsnVoDnD8FaQwQ0+nZJwO6LCYOuncHEVlP4UmvwOlulS/eOm\nCWH34VQjsLCRFLN0WV53ixQKheKd5f0T6r1HwSpIXrdoI+tZ/cR4ZNB8cxqRrsNfn0LFI+Ay3Xz+\nyqVw+yas2SLBZo+3QOApsbNMXw5W/SD+3d9tkkIe8RHyGNZ/BJ6Hoe5kqDr87bHBjHwiBi2GQKn7\nrKpVKRQKRZry/gn1Pg9wDIMipSC/Kaq601jwvgVuMUTn1i54cAAOu8Jvpojt8HCY8j1UqAyNm0KE\nL1zqDencIO9IsRxdOV4qYVVpEf/9fa/AmqZiZNJ63dvlNmYIlUC50BvSk3Yq/bpbpFAoFO88yQom\n0zStsaZpVzVNu6Fp2oh49vfVNO28pmlnNE07qGlaMdP2PJqmPTNtP6Np2pzU/gAvjO8TcHWDjzua\ntxWuCLXbm9/rRtj3NYSnB89CUKaobF8yHzzvwzc/yPsrn0sPs8QywApm9AZbB+j7a/z3vrMPllaF\nyGDovP/tEWldFxvPc+0g4AiUXKHyoBUKheIVkWSPWtM0S2A20ADwBI5rmrZZ1/VLMQ5bqev6HNPx\nLYBpgKkkFDd1XS+Tus1OAf8ul6htS0upB71wGDTqBQVi1I6+sBJ8zsLOvNCkrgxLh4bC1AlQo7bU\noX60Bh6tgwITwakUbJkNlw7BV0vFcjQuZxbB9s8gYyGxBHXJ88o+8ksRXXLSdwf47RA3MTQoMhtc\n35IHDIVCoXgHSM7QdyXghq7rtwA0TVsNtAT+L9S6rgfGON4Bsx3Vm4efL2TKLK9vnIStv0GFJrGF\n2ucsOBSF4zqMri3bFvwmaVpL14IxDK4OEhvQ3MMkGG3xCCjXEOp3ef6eB8bCf+MgbwMZ7rZNYO76\ndaLrEHzOLMz+B0GPAktHyPiBDO1nagx2uV93SxUKheK9IjlC7Q7cj/HeE6gc9yBN074AhgDpgHox\nduXVNO00EAiM0XX9v3jO7QP0AciVK1eyG//CfNQX/pkPU36BvgPg8hHZXjjOx6n/E2xzhnTroX4V\nCAqCaZOhfiOoVlOOKfs3WKaXaO+ZfUXoBs59Pijs+lYR6VLd4MP5YPkG5hqHe8OlnuD7t7x3LA25\nh4owu1QFi7ckGl2hUCjeQVItmEzX9dnAbE3TOgJjgG7AQyCXrut+mqaVB/7SNK14nB44uq7PA+YB\nVKhQIW164xERsGcHWBihcjXZduUoZMsPLqb0ojB/CLwPWUvC1kNQtxI42MOPP8ATPxgzXpzHbLKJ\nyQfAmX1wYjv0mQaueWLfM+QxbOspRiZN5r6ZIu2zCS71AkMwFPwJ3DqCbfbX3SqFQqFQmEhOMJkX\nkDPG+xymbQmxGvgIQNf1cF3X/UyvTwI3gUIv19QU4nEOjH6Q2VVKWuq69KiLxqhMdXgyLCgLZ47A\n9bvQrA48fQozp0LTllAsCxwqDPdiBIttmgHOWaBpv9j303X4u7eIf8s/wCpO0Y7XjSFErD/PfiSm\nJZVPQZ6hSqQVCoXiDSM5Qn0cKKhpWl5N09IBHYDNMQ/QNK1gjLdNgeum7VlMwWhompYPKAjcSo2G\nvzDb94NlCLRsI8PTwU/Fz7uIqdxkoCccnwHFP4F/rsm2prVh6XwICIBRY+FiD9mepaWsH94Cjy3i\nQJbONvTKk5UAACAASURBVPb9zi6Ga5ug7iTpob9JBByDo2XBawHkGQGVjoJj0dfdKoVCoVDEQ5JD\n37quR2ma1h/YidQvXKTr+kVN074HTui6vhnor2naB0Ak8BQZ9gaoBXyvaVokYAT66rr+JC0+SJJs\n+Qs0Hdqb0rKcMsKyexIBDmLlqRuh9nj4dRwULyBFO7b8CWUrgPO/cHU/FFtoDqjaOhssLOHDvrHv\n9fQW7B4EuetCpS9f2UdMEmOUlKO8NQ5s3KH8P5Cx9utulUKhUCgSIVlz1Lqu/w38HWfbtzFeD0rg\nvA3AhpQ0MNVo2hweF4dKVWNvt7SExxfh3BIRVS0T/2vvzuOrqu69j38WSZghBpBBwihjGAwQBkVl\nKHpFUUS4Khe8eEGtqE8VtWKHR6vV1tbh6q36VFu1Du3FirUiUicQBxQISFQEUQIBEhASAgTIQIbf\n88c+kJP5BE5OknO+79crr5yzh7NXNsZf1t5rry8fr4c7roUf9kDyanjgVvj+buhwCZzh61XnHYF3\nn4NzZ0AHvxCNkmJYco03yOzSvzScNKncbbDxGjj0GXSeBQOehJjT6rtVIiJSg8iZmeyhhWXfPzAd\neg2FWffC3hRo1QnO+Tks+wyKimDKOPjXW962Y8+E3E6Q4DfF6PKX4eghmPqTsp/7+e+8qUGnvgKx\ndTiCPVBmkPEsfHcHuGgY/DfoMrPm/UREpEFoIN29Ovb66/DHP0B+vvf+WAGsWQr5R733g2fBzdug\nZXtY+hG0i4UxZ8HSf0KvM2H4LTD2e2+0N3jFb8n/QN+k0nvcAHu+8C6hJ1wFg/6DepefARsmw+Yb\nIfZsOPtrFWkRkUYmMgr1T26Hn/8Uon0XEFI3QNExr8hmb/UKb3RzL5t62cdw0bmQlwerP4D/6geU\nlM1a3vAB7PrW600f72EX5sGS2V7P/KKn6zdkwwz2vAKfD4YDn8CAp2H4e97obhERaVTCv1D/sM8b\n0T1geGmh/na19733YHgmwZs5DCD5a8jM9h7L+uAduLAQBv0LDqeU/cw3/wdO6wjnXVm67MO7IWsz\nTHkBWrSr65+qasf2wVfTvfvRrQfB2V9Ct/mNJ51LRETKCP9C/cc/gyuBf/cL3fh2NXTsDjmboKQQ\nevrSsZZ+5A0u+7ex8N7f4XIHHaaUTm4CsHsrJL/tjfRu6ns2etv7kPw/MPIn0PuC0P1s5e39B3w2\nCDLf9iYvSfoIWvapv/aIiMgpC//BZG/9A6wJXDe3dFn3BG9Gsq3LoFlbiB/rLV+6Es5JhDatIOYt\naGnQ54Fyn+d7JOsS3yNZBTnw1rXQYaCXLV0fivNh8w2w52VoM9xL82o9qH7aIiIiQRX+hXrXTujY\nG9q0KV026x7vPu4fukGvC72pPTP2Qsq38Ls74JM3YVIBFI8tm7mcexjee9675N3ON7Bsw7NetvSM\n1yGmRWh/NoCiI97sYtnLofc90OuXZe+ni0ijUFhYSHp6OvnHB71KWGrevDnx8fHExAT+/+nwL9RL\nP4CiwtL3uYehWQvI2gSHM6DPxd7ytz/yvk8ZB6/9Eno4mFIuV3r5S5CbA5f7HhsvLvQuefcYD13H\nEHKFB2HDxXBoDQx6Ec74z9C3QUSCIj09nTZt2tCzZ0+cxpSEJTNj//79pKen06tXr4D3C/971KOG\nwjl+95j/9wGY2Qlie8GVb0HfS73lSz/yZiIb0Bte/Qzeuww6DC/dr6QElvzBS9rqP8pb9u1iL8Rj\n1O2h+3mOO7YP1o2HnHUw9O8q0iKNXH5+Pu3bt1eRDmPOOdq3b1/rqybhX6jL+3Y1nNEXmreBvlOg\nZQfIy4cPPvd60+uehqxdcMnlZffb8D6kbymd4MQMVj8K7ftD30tC+zPkp0Py+ZD7HSS+BZ2mh/b4\nIlInVKTD38n8G0dWoS4ugu+ToV8ifHwfHNrpLV+51ivWU/rDgVu92JGLppTd983/gbjO3pShADs/\nhh/Ww6gFoZ0mNDcVks+Fgt0w/F3o8G+hO7aIyCm69tprWbx4cb0c++DBgzz99NNVri8oKOCqq66i\nT58+jB49mrS0tEq3mzt3Lh07dmTw4MFllv/qV7+ia9euJCYmkpiYyLJlyyrdv7Yiq1Bv/woK8uD0\nZvDJr7xBYOBd9m7VAroshWKDPaOhfYfS/TK+h+RlXpRlTFNv2ZpHvd74kBBecj7yDSSf52VHj1gB\nceeF7tgiInWo+HhAUh2qqVA/99xzxMXFsXXrVhYsWMDChQsr3e7aa6/lnXfeqXTdggULSElJISUl\nhYsvvjgo7Y6sQn18opOSXdCiPXQZ6V3CXroSpg+Eva/AOwbjriq731tPepGYF//Ye79/C3z/Fgy/\nKXQjvQ+tg3XnA+Y9Hx2bFJrjikhEeeWVVxg1ahSJiYn8+Mc/ZseOHfTt25esrCxKSko477zzeO+9\n90hLS2PAgAHMmjWLgQMHMmPGDHJzcyt8nplxyy230L9/fyZNmsS+fftOrOvZsycLFy5k+PDhvPba\na6SkpDBmzBiGDh3KtGnTOHDgAADjx4/n1ltvJTExkcGDB7N27VoAsrOzufzyyxk6dChjxozhq6++\nArye7SOPPHLiOIMHDyYtLY27776b1NRUEhMT+elPf1qhrW+++SZz5njhjzNmzGD58uWYWYXtzj//\nfNq1C93EVuE/6ttfwliY8wBkPA5nXuQ9D73xe9i5B6YVQEk0/OMYfDK1dJ+jOfDeCzDuaojr5C1b\n+zhENYOkm+u+zWaw91XYdAPEtIMRH2gSE5Fwd9tvvcdFgylxADz+s2o32bx5M6+++iqrVq0iJiaG\nm266iY8++oiFCxcyf/58Ro0aRUJCAhdeeCFpaWls2bKF5557jrFjxzJ37lyefvpp7rzzzjKf+cYb\nb7BlyxY2bdrE3r17SUhIYO7c0nkt2rdvzxdffAHA0KFD+cMf/sC4ceO45557uO+++3j88ccByM3N\nJSUlhY8//pi5c+eyceNG7r33XoYNG8Y///lPVqxYwX/+53+SklJuJkk/Dz30EBs3bqxym4yMDLp1\n86Zajo6OJjY2lv3799OhQ4dKt6/Mk08+yUsvvURSUhKPPvoocXFxAe9blcjqUfc+C867APKy4Ezf\nJYmlKyHKoGMUrOsE3YZAr96l+3z4V8g7DJf+H+99bpYXiTnkGmjVsW7bm7cTUi6Fr2dCq/4w8lMV\naRGpM8uXL2f9+vWMHDmSxMREli9fzrZt27juuuvIycnhj3/8Y5meardu3Rg71pswavbs2Xz66acV\nPvPjjz9m5syZREVFccYZZzBx4sQy66+6yruCeejQIQ4ePMi4ceMAmDNnDh9//PGJ7WbO9AKFzj//\nfHJycjh48CCffvop11xzDQATJ05k//795OTkBPGM1M78+fNJTU0lJSWFLl26cMcddwTlcyOnR300\nB75LBkuDmFbQ2zcI68O1MHgA9P8TTOkIC+aU3e/T16DbQOg/0nu//v9BUb43iKyuWDHsehK2/sLr\nUfd7DLr/xMu4FpHwV0PPt66YGXPmzOG3v/1tmeW5ubmkp6cDcOTIEdr4JpAqP4LZOceaNWv48Y+9\n24T3339/jcds1apVQG2r7FhViY6OpqSk5MT7qh6H+sUvfsHbb78NQEpKCl27dmXXrl3Ex8dTVFTE\noUOHaN++fUDtA+jUqdOJ19dffz1TpkypZuvARU6P+ptP4OeTILoP3JHtRVoCHP0eEtvBO29DkcEU\nv8eyDmXB1x/D2Cu890X5sP5JOHMynJ5QN+08/DWsPQe23AannQfnfAM9FqhIi0id+9GPfsTixYtP\n3EfOzs5mx44dLFy4kFmzZnH//fdz/fXXn9h+586dfP755wD87W9/49xzz2X06NEnBlNddtllnH/+\n+bz66qsUFxezZ88ePvzww0qPHRsbS1xcHJ988gkAL7/88oneNcCrr74KwKeffkpsbCyxsbGcd955\n/PWvfwVg5cqVdOjQgbZt29KzZ88Tl9O/+OILtm/fDkCbNm04fPjwic988MEHT7QV4LLLLuPFF18E\nYPHixUycOLFWj1Pt2bPnxOs33nijwqjwkxU5PepvV0OTJtAvCaJ8I7fN4IIvYdgGeDULusZDot8k\nJ2vegpLi0kK98a9wdB+MDs7ljDKK82H7ryHt9xAdB4P/Cp1nKvVKREImISGBBx54gAsvvJCSkhJi\nYmJ47LHHSE5OZtWqVURFRfH666/zwgsvMGHCBPr3789TTz3F3LlzSUhIYP78+RU+c9q0aaxYsYKE\nhAS6d+/O2WefXeXxX3zxRW688UZyc3Pp3bs3L7zwwol1zZs3Z9iwYRQWFvL8888D3qCxuXPnMnTo\nUFq2bHmiyE6fPp2XXnqJQYMGMXr0aPr16wd498PHjh3L4MGDmTx5Mg8//HCZ48+bN49rrrmGPn36\n0K5dOxYtWgTA7t27ue666048bjVz5kxWrlxJVlYW8fHx3HfffcybN4+77rqLlJQUnHP07NmTZ555\n5hT+NUq5yka01aekpCRbt25d8D/45xdA4Xcw4HS4epl3f/nQYVjWDhgOt30Ns+fCo0+W7nPvpZD2\nNfzF+2uMZwd782hftyG4BfTQGi+WMvd76DIH+j0KTQO/3CIijd/mzZsZOHBgfTcjYGlpaUyZMoWN\nGzfW+bHGjx/PI488QlJSeDztUtm/tXNuvZlV+gNGxqXv4mL4dg2cHuM9O93ydG/5jm+gQxFkx0Je\nXtnL3rmHvdnIzpnmFeXUd7z5wcfcEdwinfUvWDfBi9sc/j4M/ouKtIiInBAZl753bfZGbpcUw5lX\nlxbaHz4HB3x7BGJj4dzS+yGs+xcUFniFGmDtY9D6DEi4qsLHn7QfXoWNs6H1EBj+DjSt41HkIiJB\n0rNnz5D0psG7/xzJIqNHHd8f/u+foDi3NC0L4NB67/vyzXDhJeAfO/bZGxB7uvfs9d4vYfsHMPIn\npfe3T1X6M95jV7Fnw4gPVaRFRKRSkdGjjo6BglRoEg09J5UuTx0Eb50O6Zlwid8kJ8fyYe1SGDcT\noqJgzWPeI13DbghOe7Y/BFt/Bh0u8ZKvoloG53NFRCTsREaPetFv4FgrGHMXNI8tXb49HzbFQNOm\nMOmi0uUpyyHviHfZ+/Bu+OZ/IXEetDjFGWbM4Lu7vCLd+T/grDdUpEVEpFrhX6iPHIQXfwHZwIQH\nS5cXH4X2K6DLUUgaDW3blq777A1o2RbOmggpz0FJEYy89dTaYcWw6XrY8TDE3wSDX/ZGkIuIiFQj\n/Av1d8nQAuhZburNw1/BhE+hWz706FW6vLgIPn8TRl0CTZvBt4uh21iI681JKymAr66G3c9Br1/C\ngCdDG40pItJAKOay9sK/Wmz+HHoCyXd5l56PO7zB+/59AXTtVrr8m1WQkwXnXAHZW2HfVzBg+skf\n3wy+/HfYt9ibCrTPrzWJiYhIOYq5rFr4F+pvV0FcE+g7pWyBPLgOcppAFtCte+nyz/4BTZtD0kXw\n7evesv5XnPzxf/gbZL3lTWLSow7nBxcRCQLFXCrmMvRytkDHkrKPZQEcWA+pTYH80h61mXd/eviF\n0KK1V6i7jITY7hU+NiCFh+C7O6DtSOh+ive4RSSyjJ9TcdmVF8FNMyE3Dy6+seL6ay+Ha6dB1gGY\ncVvZdStfrPGQirlUzGX9uORyLzu6p1+0mhVDwRZI9QVdxPsK9ffrIXOXd9n70E7Yk3xql71T74Fj\n+2DA0wrVEJEGTzGXp0Yxlycr9V/QYwLE+D0G5aIg82VYfBNwFOJ9PebP/gFNomDMpbD5JW/ZyRbq\nwyleVGX8jRAbHvPTikgIVdcDbtmi+vUd4gLqQZenmEvFXNaPWcth0iMVl2fkwNFiaNO29NGsz96A\noeOhTTvYvBg6DoV2fSruWxMrgc03QUx76PNgzduLiDQAirlsmDGXmFmNX8BFwBZgK3B3JetvBL4G\nUoBPgQS/dT/z7bcF+LeajjVixAirc7v+ZPaHH5m1Oc1s1CBv2Y5NZhdh9tZTZjm7zR5wZh/fd3Kf\nn/6c2XuYZbwQtCaLSHjbtGlTfTfBzMwWLVpkZ511lg0ZMsSGDx9uK1eutNGjR1tRUZGZmU2bNs2e\nf/552759u/Xv399mzZplAwYMsCuuuMKOHj1a4fNKSkrs5ptvtn79+tmkSZNs8uTJ9tprr5mZWY8e\nPSwzM/PEths2bLDRo0fbkCFDbOrUqZadnW1mZuPGjbNbb73VEhMTbdCgQbZmzRozM9u/f79NnTrV\nhgwZYqNHj7Yvv/zSzMxyc3PtggsusISEBPuv//ovGzBggG3fvt3MzGbOnGmDBg2yO++8s0Jb8/Ly\nbMaMGXbmmWfayJEjLTU11czMMjIybPLkySe2u/rqq61z584WHR1tXbt2tT//+c9mZjZ79mwbPHiw\nDRkyxC699FLbvXt3pee4sn9rYJ1VVYOrWmGlhTYKSAV6A02BL/0LsW+btn6vLwPe8b1O8G3fDOjl\n+5yo6o4XkkKdPN7slU5m7dqaXeE7+f/7oFeoM9PNkp8yewCzfRtr/9kFWWYftjdbO9aspDi47RaR\nsNVQCnWgtm/fboMGDQrJscaNG2fJyckhOVYo1LZQB3LpexSw1cy2mdkxYBEw1X8DM/O/e98KOD6e\nfSqwyMwKzGy7r2c9KoBj1h0z7/7xjlZgx0oHkn32DxgwBjp0hS2vQ/v+0CGh9p+/9edQdNA3gCz8\n7yyIiEjdCmQwWVdgl9/7dGB0+Y2cczcDt+P1uo8P6+sKrC63b9dK9r0BuAGge/eTfBQqUPk7vEK6\nqQMU+R7N2rvDG/E97/eQmwU7PoKzF9Z+YpJDayHjT96jWG2G1k37RUQaAMVchk7Qunxm9pSZnQks\nBH5Zy32fNbMkM0s6/fTTg9Wkyh32PT+XUuB9j+8On//Te33ONPjuTe/xrdqO9rZi2DwfmnaGM+8L\nXntFRCSiBVKoMwC/OTaJ9y2ryiLg8pPct+4VZkNUHGz3XZ2P7war/gE9h8AZfbxJTmJ7Qudhtfvc\n9Gfg8BfQ/zGIblvz9iIiIgEIpFAnA32dc72cc02Bq4El/hs45/r6vb0E+N73eglwtXOumXOuF9AX\nWHvqzT4FXedCqw/hmG9e2bYtYdOnXm86/yBs/wAGzqjdZe9j+2DrL6DdROh0Vd20W0REIlKN96jN\nrMg5dwvwLt4I8OfN7Bvn3P14o9SWALc45yYBhcABYI5v32+cc38HNgFFwM1mVvczr9dkdyY0KfRe\n7/oCSkpg7BXw/VIoKaz9Ze/v7vJiMwc8pcANEREJqoDuUZvZMjPrZ2ZnmtmDvmX3+Io0ZnarmQ0y\ns0Qzm2Bm3/jt+6Bvv/5m9q+6+TECdGw/fJ4IB94FVwTtT4cN70LnXtBrqHfZu01XOKMWA9MPJcOe\nF6HHHdBqQN21XUQkDDT2mMtdu3YxYcIEEhISGDRoEE888UQdttgTWc8PHU6BI19CZg5EF3mpWbu3\nQu9EKDwK297xkrJq81jVtvsgph30+lndtVtEJMw1lpjL6OhoHn30UTZt2sTq1at56qmn2LRpU102\nO9IKtS+D+rsYiDFvINm+nXB6N29O8KL82l32PrQOst6G7rdrAJmIhAXFXFYfc9mlSxeGDx8OeFOS\nDhw4kIyMuh0jHf6hHP4Ob4Bm8bA1B6wAunSCTYfh9O7eZe9WHaHbuYF/3rb7IDoOuv+fumuziESe\nhbfB11XHNZ6UIYnwu8er3UQxl7WLuUxLS2PDhg2MHl1hapGgirwedZtESM+AkiJo50ttad8Jtr4N\n/S730rMCcWgdZC317k2rNy0iYUAxl4E7cuQI06dP5/HHH6dt27qtAZHTozaDNsMhdgzsfhJigJa+\nH794Dxw7UrvL3upNi0hdqaHnW1dMMZcBxVwWFhYyffp0Zs2axRVXXBFQ+09F5PSonYMhr0Dr2VDo\nu48S47v3kL0Wmsd5udWByFnv603r3rSIhA/FXNYcc2lmzJs3j4EDB3L77befzGmutcjpUZccgyZN\nIX1v6TPUlgvRUbDjA+g/FaJiAvusVF9vupt60yISPhISEnjggQe48MILKSkpISYmhscee4zk5GRW\nrVpFVFQUr7/+Oi+88AITJkygf//+PPXUU8ydO5eEhATmz59f4TOnTZvGihUrSEhIoHv37px99tlV\nHv/FF1/kxhtvJDc3l969e/PCCy+cWNe8eXOGDRtGYWEhzz//POANGps7dy5Dhw6lZcuWJ4rs9OnT\neemllxg0aBCjR4+mX79+gHc/fOzYsQwePJjJkyfz8MMPlzn+vHnzuOaaa+jTpw/t2rVj0aJFAOze\nvZvrrruOZcuWsWrVKl5++WWGDBlCYmIiAL/5zW+4+OKLT+HMV8+VH9FW35KSkmzdunXB/+DN8+HA\nSjj0DFx+BbQ4CD+/GtLeh6774N+XQL9La/6cnPWwJgnO/DX0rtWU5iIiVdq8eTMDBw6s72YELC0t\njSlTpoQkmGP8+PE88sgjJCUl1fmxQqGyf2vn3Hozq/QHjJxL34c3eIEZ6XuhSRF06gyZu6BDE2ja\nGnpfENjnbLsfok9Tb1pEREIiMi59WzEc/grifwzpP3iXvrv3gKxd0PcYxJ8D0c1r/pycLyBzCZx5\nP8TE1n27RUQaKMVchk5k9KiPfgcledBmGGTsg+gSL4c6Kx1cvpeWFYht9/l60z+p0+aKiIgcFxmF\n+viMZG2Gwa4fgGPQub0XwFGSC2271/wZORu83nT3BepNi4hIyERGoW7V35vms9UA2LUTrARiW0BT\n3/rYAAr18d50d/WmRUQkdCLjHnXbEd4XwG7fnKwto6DZ8fU1FOqcDZD5JvS+D2JOq7NmioiIlBf+\nPWozr9CWFMDRXDh60FseVQTHx4/V1KPedh9Ex6o3LSJyihRzWXvhX6gL0mHNcMj4szeQzBV5y4uP\nQqsYwEGb+Kr3P5zi9aa7L1BvWkSkjijmsmrhX6gP+1JS2gyDDN+sZE2bQU4mtG0Jbc6ofkay3S+D\nawrdbw1Ne0VE6pFiLhVzGXo5GwAHrYdC+gpwhdD5DO8Z6vZRNd+fzv4AThur3rSIhNbF4ysum3Yl\nXH8T5ObCjEqmrJx1rfe1PwuumVF23bKVNR5SMZeKuawfhzdAy74Q3bq0R92jB2TuhOii6u9PH9sH\nR76Cdj8KXXtFROqJYi4Dp5jLYDqcArG+v3bS90JUMXTrBjtWAtEQ26PqfbN9KS/tJ9V1K0VEyqqu\nB9yyZfXr23cIqAddnmIuFXNZPxL+BD0WeK93ZgCF0CHWy6O2ouovfWcv92Is24wIRUtFROqVYi4V\nc1k//HvDO3d639s2C+wZ6uwPIG48NAn/0yQiophLxVwGpM5iLgE6ngX5X8Gv74TkR2AgcF0KdDqr\n4rZ52+HT3tD/CT0/LSJ1TjGXVVPMZaQoLISD+73XTY6V9qiruke9f7n3vZ3uT4uISP2JnGu6ezLB\nHfNeHzsMbVtA02hoVkXARvZyaNoFWjWev3BFREJFMZehEzk96uOzkrVqDQf3QOtm3qNZlY0ctBKv\nULebWPl6ERGREImcQp3+g/cMdeeukLnLu/Rd1UCyIxuhMFPPT4uISL2LnEKdsc+blaxHD9i3E5rk\nV31/Ovv4/WkVahERqV+RU6jTf4AmRdDtDCg4DJZfdY86e7k3m1mLAHKqRURE6lDkFOq0dHDF0L6N\n34jvSgpxSSEc+Ei9aRGROhDOMZe/+tWv6Nq1K4mJiSQmJrJs2bKgtDtyCvUOb2YaWjetfrKTnLVQ\nfESFWkQkhMIl5nLBggUnZjsL1iQokVOod/tiyGJKqu9R718OOIibEKqWiYg0GIq5bKQxl865i4An\ngCjgz2b2ULn1twPXAUVAJjDXzHb41hUDX/s23WlmlwWp7YErKYHsTN/83vnQwnmPXbXpWnHb7OVe\ndnXTihOxi4iExB9vg21VxzWelN6JcOPj1W6imMtTj7l88skneemll0hKSuLRRx8lLi6u2nMeiBp7\n1M65KOApYDKQAMx0ziWU22wDkGRmQ4HFwO/91uWZWaLvK/RFGiDrAJTke8U5/yC0aekV6fJzeBcf\nhUOf67K3iEQkxVwGrrKYy/nz55OamkpKSgpdunThjjvuCMqxAulRjwK2mtk2AOfcImAqcOKivJn5\nx6GsBmYHpXXBkrHXezSrbRzsz4BW0ZXfnz7wCVihCrWI1K8aer51RTGXpxZz2alTpxOvr7/+eqZM\nmRLQz1aTQO5RdwV2+b1P9y2ryjzgX37vmzvn1jnnVjvnLq9sB+fcDb5t1mVmZgbQpFpK3+s9mtW5\nC2Ttgpjiyu9PZy8HFwNx5wa/DSIiDZxiLk8t5nLPnj0nXr/xxhsMHjy45pMegKDO9e2cmw0kAeP8\nFvcwswznXG9ghXPuazNL9d/PzJ4FngUvPSuYbQJKe9Tde0LWu9CnpPLJTrKXQ+zZEBXYX3giIuFE\nMZenFnN51113kZKSgnOOnj178swzz5zCv4YfM6v2CzgbeNfv/c+An1Wy3SRgM9Cxms/6CzCjuuON\nGDHCgu7n/23W2pktmGd2GWYPYLbu6bLbFGSZvefMUu8P/vFFRGqwadOm+m5CrWzfvt0GDRoUkmON\nGzfOkpOTQ3KsUKjs3xpYZ1XUxUAufScDfZ1zvZxzTYGrgSX+GzjnhgHPAJeZ2T6/5XHOuWa+1x2A\nsfjd2w6ZtB3gDOJaVf0M9YEPAdP9aRERaVBqvPRtZkXOuVuAd/Eez3rezL5xzt2P9xfAEuBhoDXw\nmu96/vHHsAYCzzjnSvDuhz9kZqEv1L77E7SKrvoZ6v0fQFRraDsypE0TEWmMFHMZOgHdozazZcCy\ncsvu8Xs9qYr9PgOGnEoDg+L4ZCdNivwKdbl71NnLIW4cNIkJadNERESqExkzk2X7rsaX5EKrGGgW\nC83alq7P2wl5W6FdpX9viIiI1JvwL9Q5R6DgKDSJgqP7oXWzipe9FWspIiINVPgX6oy90KQQ4tpD\n4opFwwAADplJREFUVjo0dxUHkmV/AE07QuvgPPMmIiISLOFfqNN9z1B37gqZO6HJsbL3p80gewXE\nTfSmGBURkTqjmMvaC/9CneGblax7POTsBQrK9qiPboJjP+iyt4hIPVLMZdXCv1Dv3OOblawTNPUt\n879Hffz+dHsNJBMRUcxlI425bNRSU8FR9WQnB1ZCi17Qomfo2yYiUpW7xldcdv6VMOUmyM+Feyrp\nrV1wrfd1KAsenFF23e9X1nhIxVw20pjLRu/4PYbmTaC5b5l/j/roZi9/WkQkwinmMnANLeaycdvt\nRbPR5JjXo3ZR0PoMb5kVQ+426HBpvTVPRKRS1fWAm7esfn1sh4B60OWZYi4bbcxl47bfdz+k8Ai0\naQZt471nqgHyM8COQcsz6699IiINhGIuIyDmssEpOAa5OdCqGRzaCy1jyt6fzvOlbbZQoRYRUcxl\nw4y5dOVHtNW3pKQkW7duXXA+rLgYLp8Mu7bDmGYQvx2GTIOpr3jr0/8Mm6+Hc7d5A8pEROrJ5s2b\nGThwYH03I2BpaWlMmTIlJMEc48eP55FHHiEpKanOjxUKlf1bO+fWm1mlP2B4X/qOioLDB6FHL9i3\nA1wetPWb7CQvFVwMNOtWf20UERGpRnhf+gbI2AUDB8DuI9772HKXvlv0hCbhfxpERIJJMZehE949\n6oIC2PsDtG9d+TPUuVt1f1pERBq08C7UUVHw/mcwJskvh9pXqM28HrVGfIuISAMW3oU6OhpGnw3u\nWMUedeF+KMpRj1pERBq0yLg5m7kLWjhofho08x7UP/FoVss+9dcuERGRGoR3j/q4zJ3Qunm5+9N6\nhlpEJNQae8xlfn4+o0aN4qyzzmLQoEHce++9ddhiT2QU6n07vR51bLmBZKDnp0VEGoDGEnPZrFkz\nVqxYwZdffklKSgrvvPMOq1evrstmR0ihztoFUYUQW+4Z6mZdIapF/bVLRKSBUcxl9TGXzjlat24N\neHN+FxYWVjvveDCE/z3qkhI4sAsoqjh9qC57i0hD9N5tsLfquMaT0ikRLny82k0UcxlYzGVxcTEj\nRoxg69at3HzzzWViLutC+PeoD+6FqCLvdWy5e9QaSCYicoJiLgMTFRVFSkoK6enprF27ts4nfgn/\nHvW+nRUfzSo6Asd+0DPUItIw1dDzrSuKuQws5vK40047jQkTJvDOO+8ELSmrMuHfo87c5TfZie8e\ndd4277sufYuInKCYy5pjLjMzMzl48CAAeXl5vP/++wwYMKDW57o2wr9HnenrUTeJgdadvWWKtxQR\nqUAxlzXHXO7Zs4c5c+ZQXFxMSUkJV155JVOmTDm1E1+D8I65BHhmAWx5Es7sBjf7etJpj8D3P4Xx\n2RATF7xjiYicJMVcVk0xl+Eucye0iqk44jumnYq0iIg0eBFw6XsXdLCKk53osreIyElTzGXoRECP\negc0KYC25SY7UaEWEZFGILwL9bECyN0H+PWoSwohf6cezRIRkUYhvAt1VDQsfM57ffwedf4OsGL1\nqEVEpFEIqFA75y5yzm1xzm11zt1dyfrbnXObnHNfOeeWO+d6+K2b45z73vc1J5iNr1FUFLTxPUR9\nvEedq3hLERFpPGos1M65KOApYDKQAMx0ziWU22wDkGRmQ4HFwO99+7YD7gVGA6OAe51zoR1qfWin\n9/14jzrveGqWetQiIqGmmMvaC6RHPQrYambbzOwYsAiY6r+BmX1oZsdjU1YD8b7X/wa8b2bZZnYA\neB+4KDhND1DOTmjRHpr6pqnLTYUmLaBZl5A2Q0REqqaYy6oFUqi7Arv83qf7llVlHvCv2uzrnLvB\nObfOObcuMzMzgCbVwqEdlaRm9YY6jiUTEWmMFHMZ5jGXzrnZQBIwrqZt/ZnZs8Cz4M1MFsw2kbMT\n4vzuR+emasS3iDR8L4+vuGzglZB0ExTmwqKLK64fei2cdS3kZsHrM8quu2ZljYdUzGXjjbnMALr5\nvY/3LSvDOTcJ+AVwmZkV1GbfOmPm9aiPh3FYiRfI0UIDyUREylPMZWAaYsxlMtDXOdcLr8heDfyH\n/wbOuWHAM8BFZrbPb9W7wG/8BpBdCPzslFsdqKI8aBMPcX299wV7oCRPPWoRafiq6wHHtKx+fcsO\nAfWgy1PMZSONuTSzIuAWvKK7Gfi7mX3jnLvfOXeZb7OHgdbAa865FOfcEt++2cCv8Yp9MnC/b1lo\nxLSEGzfDyFu890rNEhGpkmIuG3HMpZktA5aVW3aP3+tJ1ez7PPD8yTYwqE48Q61CLSJSnmIuFXMZ\nkKDHXPrb+ktIewgm5nn51CIiDYRiLqummMtIkrsVmvdQkRYRkUYj/GMu/Sk1S0QkKBRzGToR1qPW\nM9QiItK4RE6hLjwARQfUoxYRkUYlcgq1UrNERKQRipxCrdQsERFphCKnUJ/oUfeu33aIiESwxh5z\neVxxcTHDhg2r82eoIZIKdV4qNO0MUYFNVyciIqHTWGIuj3viiSdC9tx75BRqjfgWEamRYi6rj7kE\nSE9P5+233+a66647hTMduMh5jjovFdpVOdOpiEjDseU2OFx1XONJaZMI/R+vdhPFXAYWc3nbbbfx\n+9//vsy84XUpMnrUxXlQkKEetYhINRRzWbOlS5fSsWNHRowYUafH8RcZPeq8bd53jfgWkcaghp5v\nXVHMZc0xl6tWrWLJkiUsW7aM/Px8cnJymD17Nq+88kpAP8fJiIwetVKzRERqpJjLmmMuf/vb35Ke\nnk5aWhqLFi1i4sSJdVqkIWJ61MqhFhGpiWIua465rA+REXP57S2w5xUYfwCquVwiIlJfFHNZNcVc\nRoLcrV5vWkVaREQamci49J2bCm2H1XcrRETChmIuQyf8e9QlRZCfpvvTIiLSKIV/oS7YBVakQi0i\nIo1S+BfqXF9qluItRUSkEYqAQq1nqEVEpPEK/0KdlwpNmkGzrvXdEhGRiKeYy9qLjELdohe48P9R\nRUQaK8VcVi38q1duqgaSiYgESDGXirkMLTNfvOWE+m6JiEjtrBtfcVmnK6HbTVCcCxsurrj+jGu9\nr2NZ8NWMsuuSVtZ4SMVcKuYy9EpyIW4CtB1V3y0REWnwFHNZM8VcBltUKxj2Vn23QkSk9qrrAUe1\nrH590w4B9aDLU8ylYi5FRKQBU8ylYi5FRKQBU8ylYi4DUicxlyIiDZxiLqummEsRERFpsAIq1M65\ni5xzW5xzW51zd1ey/nzn3BfOuSLn3Ixy64qdcym+ryXBariIiNSfUMdchktv+mTUeI/aORcFPAVc\nAKQDyc65JWa2yW+zncC1wJ0VP4E8M0sMQltFREQiTiCDyUYBW81sG4BzbhEwFThRqM0szbeupLIP\nEBGRmplZtY8dSeN3MuPCArn03RXY5fc+3bcsUM2dc+ucc6udc5fXqnUiIhGiefPm7N+//6T+Ry6N\ng5mxf/9+mjdvXqv9QvF4Vg8zy3DO9QZWOOe+NrNU/w2cczcANwB07949BE0SEWlY4uPjSU9PJzMz\ns76bInWoefPmxMfH12qfQAp1BtDN7328b1lAzCzD932bc24lMAxILbfNs8Cz4D2eFehni4iEi5iY\nGHr16lXfzZAGKJBL38lAX+dcL+dcU+BqIKDR2865OOdcM9/rDsBY/O5ti4iISPVqLNRmVgTcArwL\nbAb+bmbfOOfud85dBuCcG+mcSwf+HXjGOfeNb/eBwDrn3JfAh8BD5UaLi4iISDU0M5mIiEg9q25m\nsgZXqJ1zmcCOIH9sByAryJ8ZqXQug0PnMXh0LoNH5zJ4ansue5jZ6ZWtaHCFui4459ZV9ZeK1I7O\nZXDoPAaPzmXw6FwGTzDPpeb6FhERacBUqEVERBqwSCnUz9Z3A8KIzmVw6DwGj85l8OhcBk/QzmVE\n3KMWERFprCKlRy0iItIohXWhrilHW6rmnHveObfPObfRb1k759z7zrnvfd/j6rONjYVzrptz7kPn\n3Cbn3DfOuVt9y3U+a8k519w5t9Y596XvXN7nW97LObfG97v+qm8WRamBcy7KObfBObfU917n8SQ4\n59Kcc18751Kcc+t8y4L2+x22hdovR3sykADMdM4l1G+rGpW/ABeVW3Y3sNzM+gLLfe+lZkXAHWaW\nAIwBbvb9t6jzWXsFwEQzOwtIBC5yzo0Bfgf8t5n1AQ4A8+qxjY3JrXgzTh6n83jyJphZot8jWUH7\n/Q7bQo1fjraZHQOO52hLAMzsYyC73OKpwIu+1y8Cii0NgJntMbMvfK8P4/2PsSs6n7VmniO+tzG+\nLwMmAot9y3UuA+CciwcuAf7se+/QeQymoP1+h3OhPtUcbamok5nt8b3+AehUn41pjJxzPfES5Nag\n83lSfJdrU4B9wPt4aXwHfbkEoN/1QD0O3AWU+N63R+fxZBnwnnNuvS+2GYL4+x2KPGoJQ2Zmzjk9\nMlALzrnWwOvAbWaW43VgPDqfgTOzYiDROXca8AYwoJ6b1Og456YA+8xsvXNufH23Jwyca2YZzrmO\nwPvOuW/9V57q73c496hPKUdbKrXXOdcFwPd9Xz23p9FwzsXgFem/mtk/fIt1Pk+BmR3ES+U7GzjN\nOXe846Hf9ZqNBS5zzqXh3RacCDyBzuNJMbMM3/d9eH88jiKIv9/hXKhPOkdbqrQEmON7PQd4sx7b\n0mj47v09B2w2s8f8Vul81pJz7nRfTxrnXAvgArx7/h8CM3yb6VzWwMx+ZmbxZtYT7/+NK8xsFjqP\nteaca+Wca3P8NXAhsJEg/n6H9YQnzrmL8e7DRAHPm9mD9dykRsM597/AeLwEmL3AvcA/gb8D3fES\nzq40s/IDzqQc59y5wCfA15TeD/w53n1qnc9acM4NxRuYE4XX0fi7md3vnOuN1zNsB2wAZptZQf21\ntPHwXfq+08ym6DzWnu+cveF7Gw38zcwedM61J0i/32FdqEVERBq7cL70LSIi0uipUIuIiDRgKtQi\nIiINmAq1iIhIA6ZCLSIi0oCpUIuIiDRgKtQiIiINmAq1iIhIA/b/AQms3L/hZWHUAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "NUM_COLORS = 25\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\n",
    "scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "color_bar = [scalarMap.to_rgba(i) for i in range(NUM_COLORS)]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Loss\")\n",
    "plt.ylim([0, 5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day082_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
